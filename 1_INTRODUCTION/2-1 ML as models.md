idealization, simulation, abstraction, similarity:
- “The semantic conception provides a straightforward answer to the question of how models give us knowledge of the world: they specify structures that are posited as possible representations of either the observable phenomena or, even more ambitiously, the underlying structures of the real target systems.” (Knuuttila and Merz, 2009, p. 4) ==semantic approach to modelling==
- “Thus, according to the semantic view, the structure specified by a model represents its target system if it is either structurally isomorphic or somehow similar to it.” (Knuuttila and Merz, 2009, p. 4)


**de Regt: models are explanatory, i.e. they can provide some understanding, because they represent their target and are intelligible (i.e. have qualities that facilitate their use)**
“De Regt takes understanding on the basis of models to be possible if they are explanatory qua representational (also Giere, 2006, Chapter 4). A similarly important role for representation is reserved by Morrison (1999, p. 63): The reason that models are explanatory is that in representing [their target] systems they exhibit certain kinds of structural dependencies. The model shows us how particular bits of the system are integrated and fit together in such a way that the system’s behaviour can be explained.” (Boge, 2022, p. 53)

“Hence, establishing ways in which to represent a certain target by means of a model allows us to map the relations established in the model onto relations pertaining, for all we know, to the target, and so, if the model’s behavior matches that of the target in relevant respects (e.g., segregation patterns emerge), we may infer an explanation of the observed target-behavior from the model (e.g., in terms of moving behavior being in part determined by preferences for neighborhood-composition).” (Boge, 2022, p. 54)

**de Regt: theories and models are explanatory if they have qualities that faciliate their use**
“In de Regt’s account, for representational models to explain, they must also be constructed under the principles of an intelligible theory, where a theory is intelligible if it has certain qualities that “provide conceptual tools for achieving understanding” (de Regt, 2017, p. 118; emph. added). ==Among these tools, de Regt (2017, p. 115) lists “visualization, mathematical abstraction, and causality [as] prime examples.”==” (Boge, 2022, p. 54)

“However, de Regt spells out the intelligibility of a theory in terms of “qualities [...] that facilitate the use of the theory” (de Regt, 2017, p. 40; emph. added), and Reutlinger et al. (2018, pp. 1084–1085) equally offer a use-oriented, empirically accessible explication of Strevens’ notion of grasping.” (Boge, 2022, p. 54)

i.e. the representations of the model need to be adquate to make target mechanisms intelligible

---



**Three distinct senses of DNN as "models" identified by Boge**
“In sum, at least three distinct senses of ‘model’ should be distinguished here, which, so far as I can see, exhaust the use of ‘model’ in the DL literature: (a) DNNs as (crude) models of actual brains, (b) the algorithms employed in DL as abstract, selective models of human learning, and (c) the input–output mappings approximated through training as models of features pertaining to the data, such as their statistical distribution.” (Boge, 2022, p. 46)

**DNN as models or techniques? DNN as crude models of the brain**
“Napoletani et al. (2011, p. 13) actually refrain from calling DNNs ‘models’ altogether and solely use ‘technique’. Humphreys (2013,  p. 580), on the other hand, acknowledges the possibility of “simulating neural dynamics” with DNNs, but also urges to “keep separate uses of neural nets as simulation models from their use as techniques in computational science”, and additionally finds most neural nets to be “extremely crude models of real brains[...].” The latter verdict is frequent in the literature (e.g. Chirimuuta, 2020; Goodfellow et  al., 2016; Sullivan, 2019), not least ==because feed-forward processing and gradient descent are biologically implausible==;” (Boge, 2022, p. 45)

**Algorithms employed in DNN as abstract models for human learning**
“Note that the learning algorithm involved in deriving this model may itself count as yet another model: Buckner (2018) points to the possibility of understanding concept abstraction on the basis of deep convolutional nets, without drawing too close a parallel to either brain processes or most details of human cognition. Similarly, it may be possible to understand certain errors made by DNNs in analogy to errors made by humans under similar conditions (Buckner, 2021, for discussion).” (Boge, 2022, p. 46) 

**Objectual approach by Knuuttila and Merz**
Knuuttila and Merz (2009) presents an objectual approach to modelling according to which a model is an 'concrete constructed object'. Its function is not representive, but productive. The way a model provides understanding depends on the way we interact with it and how it is implemented. Thus, a single model may provide a multiplicity of understanding depending on its practical use. The specific implementation of a model is key to understand how the model affords understanding. The objectual approach allows for models to be highly unrealistic while still allowing for understanding to be gained. 


A classical approach to scientific modelling is that relevant model features are measurable and specified, parameter values remain largely consistent across domains of application and model assumptions are few. On the other hand, the idea of DNN models is that their relevant model features are initially unspecified and are acquired through training. However, due to their high level of opacity, it proves to be difficult to determine which features a DNN model tracks and what the parameter values are. When DNN models are applied to new domains, their parameterization usually changes drastically (Scorzato_2024).