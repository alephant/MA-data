## Cichy presents three reasons why DNN have explanatory power despite their opacity:
- how can I re-work this to say something about "understanding"??
Comparing DNN-based modelling approaches to other modelling approaches can illustrate the difference in transparency. Mathematical-theoretical modelling is used to investigate visual representations in primates. Here, parameterization, interaction between parameters and their mapping to parts of the target phenomenon are all transparent to the modeler, as model parameters such as geometrical primitives are identified in advance and are a priori mapped to cortical activations. 
The detail parameterization of DNN is aquired through training and not set in advance. Thus, it is not directly interpretable how the usually high number of model parameters interact and how they map onto elements of the target phenomenon. 
However, Cichy argues against the problem of opacity of DNN threefold:
DNN provide an explanation that is different in nature compared to other model classes. DNN explanations are teleological and are not based in a specific kind of isomorphic or similarity mapping. 
DNN are actually quite similar to mathematical-theoretical models. The DNN is defined by a set of parameters that are defined by the modeler in advance, i.e. model architecture, training material, training procedure and objective are all known prior the DNN is trained. Cichy argues that the difference in model parameters does not imply a difference in the capacity to explain, but is due to a difference in the way they represents the target system. 
In a model-of-model approach they characterize DNN as one type of model among a diverse set of models. DNN are not less capable of explanation, but add to the toolbox of available models due to their specific profile of desiderata.
Lastly, they argue that DNN will be made more transparent in the future through advanced XAI methods. As these methods increase their level of transparency, DNN become better suited to explain and enable understanding. It might be even the case that their high level of complexity might be feature and not a bug as DNN could serve to model highly complex phenomena such as brain activity. 

