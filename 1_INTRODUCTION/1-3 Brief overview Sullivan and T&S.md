- Teaser Understanding of vs understanding with ML models
- Teaser for narrow and wide view of understanding from ML models

Sullivan argues that whether or not ML models enable understanding of phenomena is not a question of model opacity, but depends on the degree of link uncertainty between the model and the phenomenon. For a link to be established between model and phenomenon, 

Opacity is not an in-principle problem for understanding from ML models, i.e. the reason why opaque do not enable understanding is not opacity, but link uncertainty.

**Sullivan's approach to dismiss model opacity as an in-principle problem for understanding suits those researchers who want to give up on transparency as an epistemic aim for complex systems**
“The pessimism about transparency expressed by philosophers interested in modeling is understandable. Complex computation will retain aspects of ineliminable opacity. However, we need not, and ought not, give up transparency. The types of transparency for which Humphreys, Lenhard, and Winsberg hope rely too heavily on aspects of computation that are often either difficult to salvage, like transitory variables, or insufficiently explanatory, like neurons. They conclude, therefore, that we must abandon transparency as an epistemic aim for complex systems.” (Creel, 2020, p. 33)

Some reseachers want to be content with black boxes. They are pessimistic about transparency in modelling.



