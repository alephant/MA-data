There are epistemological concerns in the use of ML models due to the opacity of neural networks.

**Neural network transparency as epistemically essential to gain knowledge from ML applications**
“What all of these reasons have in common is a commitment to the idea that neural network transparency is epistemically essential to effectively use and gain knowledge from powerful artificial intelligence applications in scientific and societal settings (Gunning et al. 2019).” (Duede, 2023, p. 1093)

This commitment invokes the idea that without transparency, “scientists are unable to understand the outputs of their models, are powerless to explain why the models perform the way they do, cannot provide justification for the decisions they make on the basis of the model output, are uncertain whether and to what extent the models reflect our values—on and on.” (Duede, 2023, p. 1093)

**Lack of interpretability is problematic for high-stakes decision-making settings**
“While lack of interpretability is of particular concern in highstakes decision-making settings where accountability and value alignment are salient (e.g., medical diagnosis and criminal justice) (Falco et al. 2021; Hoffman 2017)” (Duede, 2023, p. 1090) 

**Opacity can be seen as problematic for research settings**
“opacity of deep learning models may also be of concern in basic research settings where explanations and understanding represent central epistemic virtues and often serve as justificatory credentials (Khalifa 2017)” (Duede, 2023, p. 1090)

**Opacity is problematic in the "context of justification"**
Concern arise “when network outputs are treated as scientific claims that stand in need of justification (e.g., treated as candidates for scientific knowledge, or treated as the basis for high-stakes decisions)” (Duede, 2023, p. 1090)

**Epistemic opacity is not a problem when DNN are used to guide science in the "context of discovery"**
“What I hope to have shown in this paper is that, despite their epistemic opacity, deep learning models can be used quite effectively in science, not just for pragmatic ends but also for genuine discovery and deeper theoretical understanding. This can be accomplished when DLMs are used as guides for exploring promising avenues of pursuit in the context of discovery.” (Duede, 2023, p. 1097)

Epistemic concerns should be different when neural network outputs are treated as scientific findings in their own right (context of justification) versus when outputs serve to guides in the process of scientific discovery (context of discovery)


**Computational opacity due to different kinds of data that influence machine vs human decision-making**
“Part of the ineliminable opacity of complex computational systems comes from the fact that the factors that influence machine classifications or predictions are of different kinds from those described as reason-giving by humans. Even when information that is sufficient for machine classifications or predictions is available, its difference in kind and scale means that it explains little to us.” (Creel, 2020, p. 31)


**Epistemic opacity is a context-dependent feature of computational science**
“Here a process is epistemically opaque relative to a cognitive agent X at time t just in case X does not know at t all of the epistemically relevant elements of the process.” (Humphreys, 2009, p. 618)

A cognitive agent is not able to know and understand all epistemically relevant elements of a computational process.
Key question based on Humphreys: can I make justifiable claims on the basis of a process of which it is impossible to know all its epistemically relevant factors?

**Functional opacity due to genetic programming**
“A program generated by machine learning may fail to be functionally transparent if its algorithm was developed autonomously, without being programmed by a person, as in genetic programming.” (Creel, 2020, p. 10) 


**Structural opacity is not correlated with functional opacity similar to how acc. to Sullivan knowledge of model implementation is irrelevant for understanding phenomena from the model**
“It is possible to know the algorithm that the code realizes but not know how the code realizes it, i.e. to have functional but not structural transparency.” (Creel, 2020, p. 13)


**Computational opacity of algorithms that involve emergent properties, complexity effects etc. like in DNN is not easily resolved by process decomposition**

“The difficulty in determining which emergent properties, complexity effects, or unexpected errors the code might possess cannot always be resolved by being able to trace the code line-by-line or function-by-function. Only at a higher level does structural opacity emerge.” (Creel, 2020, p. 15)

Higher-level emergent properties are not decomposable (see Humphrey, 2004)

“Thus, a second source of opacity would remain even if hand checking were possible: the program would still not be fully transparent because we could not understand how all the steps interact to bring about the algorithm or to what extent each individual step contributed to the final result.” (Creel, 2020, p. 15)


**The degree of structural opacity is a function of model complexity**
“The structural opacity of a large cosmology or climate simulation is an accidental function of size and accretion; a similar but smaller program would be easier to understand.” (Creel, 2020, p. 19) 




