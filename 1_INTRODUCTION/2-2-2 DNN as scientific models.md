If we want to understand how DNN can be used as scientific models, we first need to consider which properties models much have to provide understanding and how these criteria relate to DNN.

Models are intelligible in virtue of modellers that can understand and use them to gain understanding of phenomena in the world. In this sense, intelligibility is a context-dependent property of a model as it relates to the ability of the modeller or other scientists' to use the model to make inferences on the target phenomenon. An intelligible model doesn't need to be realistic to provide understanding (DeRegt_2005, 2015). 

**Boge: What is needed for DNN to provide scientific understanding?**
“==Without a doubt, it is possible to understand something about underlying mechanisms on the basis of the outputs of a DNN though.== Hence, what is required to facilitate that understanding? I believe that the following three steps are crucial in that respect: (I) The conceptualization of input and output, prior to training; (II) establishing what the (trained) DL model represents, on account of (I); and (III) connecting that represented something to underlying mechanisms on the basis of further background knowledge.” (Boge, 2022, p. 55)

“I take it that this analysis in terms of steps (I)–(III) is an ==assessment of understanding from DL==, or the possibility of a want thereof, compatible with Sullivan’s recent appraisal of what she calls ‘link uncertainty’, i.e., the “lack of scientific and empirical evidence supporting the link that connects the model to the target phenomenon” (Sullivan, 2019,  p. 8). Because of the dependence on (I) and, especially, (III), the DL model on its own is conceptually too poor to provide an understanding of underlying mechanisms: ==Only if, via (I) and (II), a connection can be made to those mechanisms in a final step, (III), will a DL-success promote an understanding of them.==” (Boge, 2022, p. 56 ff.)

Boge assesses the possibility for understanding.
Boge's analysis is compatible with Sullivan's account for link uncertainty.
Boge's step I and II are necessary for linking the model to the target in step III.