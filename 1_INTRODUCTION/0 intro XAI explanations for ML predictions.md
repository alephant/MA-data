**XAI has three aims for explaining ML decisions**
“The growing field of “Explainable AI” (XAI) has emerged in direct response to this problem. XAI can be seen to espouse three overriding aims for explanations of ML decisions: (1) completeness or depth; (2) realism/fidelity; and (3) interpretability.” (Zerilli, 2022, p. 2) 

**XAI to explain how a ML model truly comes to its prediction**
“Real explanations are those which are faithful to the way a system computes its predictions (Rudin 2019; Guidotti 2018).” (Zerilli, 2022, p. 2) 

**Lack of fathomability limits XAI to give interpretable explanations for most advanced ML models**
“In fact, however, complete explanations are only possible for certain types of ML systems, namely, those whose operations are fathomable or in some other way intelligible to a trained expert. Because the operations of the most advanced ML systems are not fathomable, nor even readily intelligible, in many cases XAI prioritises interpretability at the expense of completeness, and sometimes even realism, so that explanations are frequently interpretable without being underpinned by more comprehensive explanations that are faithful to the logic of the system.” (Zerilli, 2022, p. 3) 