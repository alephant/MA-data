#### 1-1 DNN as predictive powerhouses

ML models in general and DNN in particular are predictive powerhouses and their predictive power will never be any worse than today. Among many more examples, ML algorithms have achieved human-level accuracy in visual recognition tasks (Russakovsky_2015), disease classification (Esteva_2017), similary judgements (Peterson_2016), high performance on complex tasks (Mnih_2015), natural-sounding audio generation (Oord_2016) or the prediction and modelling of neural activity (Cadena_2018, Yamins_2014, Yamins_2016, Khaligh-Razavi_2014).


#### 1-2 DNN opacity is a problem


However, the use of DNN as scientific models is controversial. DNN are highly complex computational archictectures containing billions of parameters, their functioning is difficult to understand and control for the modeler. As DNN models lack transparency and simplicity, one lacks the insight into the computational process that enables their unique characteristics such as their high predictive power, and the driving factors that determine DNN model outcomes. Thus, the problem of opacity in DNN is discussed as a lack of epistemic access to all elements of the computational process that are epistemically relevant in understanding DNN functioning and outcomes (Humphreys, 2009).
The epistemic opacity of DNN thus becomes a problem when DNN models are used for scientific explanation and understanding. Duede (2023) identifies the "context of justification" in science as critical in this matter: it becomes unreasonable to treat DNN output as scientific claims which are in need for justication as DNN lack transparency to all epistemically relevant elements that determine their output. e case of DNN. This is especially the case when DNN outputs directly inform high-stakes decision-making, e.g. in medical intervention or criminal justice.

Thus, DNN are labelled black boxes. 

Black boxes do what they are supposed to do, but one does not fully understand how they do. As DNN lack transparency and simplicity, critics dismiss their capacity to enable scientific understanding. At the same time, the use of DNN as scientific models is continously expanding. Especially data-driven sciences such as genomics are transformed by the use of DNN as they are highly capable to learn complex patterns (Eraslan_2019). But worries remain - do scientists sacrifice scientific understanding by ML models compared to traditional scientific models?
“One might argue that a machine-learning approach to determine the non-linear response from varying parameter settings is a rather black-box approach that goes against the traditional approach to spectra: based on scientific understanding and physics.” (Agarwal et al., 2012, p. 1410)


This should suffice as a general introduction to why DNN opacity is discussed as a problem. In section 4, I will elaborate further on the black box nature of DNN after more information on their structure and modelling process was conveyed.



- “astonishing capacity of deep neural networks to facilitate discovery (DeVries et al. 2018) and overcome the complexity of otherwise intractable scientific problems (Senior et al. 2020), as well as to both emulate and outperform experts on routine (Chen et al. 2014), complex, or even humanly impossible (Degrave et al. 2022) tasks. In fact, nearly every empirical discipline has already undergone some form of transformation as a result of developments in and implementation of deep learning and artificial intelligence (Stevens et al. 2020).” (Duede, 2023, p. 1089)






