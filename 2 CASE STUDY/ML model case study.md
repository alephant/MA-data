what is understood by understanding?

For T&S, understanding with ML models require the identification of the appropriate target of understanding. For them, this the Target of ML models, i.e. TML target. 
T&S define the TML target, i.e. the appropriate target of understanding with ML models, as the "relationships of features represented by the data". What do they mean by that?
The model data of a DNN can be subsumed as x and y-data. x-data is the input data, i.e. the data on which data transformations are executed given the respective neural network architecture and parameterization. Y-data is the data which determines the prediction or classification to be made. Through training, the parameterization of DNN model change in such a way that the prediction of y-data is optimized based on given x-data. 

Throughout their work they rely on probability distributions to illustrate
By drawing inferences on T, a model is used to explain a phenomenon T and provide understanding of T. For T&S, by drawing intra-model inferences on the TML target, one can gain understanding of T, i.e. the actual target phenomenon.


What is irrelevant for understanding? Understanding of the ML model!
Sullivan: implementation irrelevance
T&S: functional approximate irrelevance
conclusion: epistemic/ structural opacity is no problem

What is relevant for understanding? Necessary for understanding from ML models
Sullivan: link between M and T
T&S: “similarly trained models irreconcilably diverge in their feature importance implications, approximate irrelevance comes into question” (Shech and Tamir, 2022, p. 12)

What in the model is the basis for understanding of T??
Sullivan: “abstract patient representation where only the most important derived dimensions remain” (Sullivan, 2022, p. 16)
T&S: 

What is the target of understanding?
Sullivan: real-world target phenomenon (e.g. disease development in the deep patient DNN)
T&S: feature relationships represented by the data (TML target)

What kind of understanding can we gain?
Sullivan: “there are several explanatory questions that might be interesting to ask of the model, each with varying levels of answerability. Some of these questions are howpossibly questions and some of these questions are more pointed why- and how-actually questions.” (Sullivan, 2022, p. 23)
T&S: “we emphasize that a predictable relationship between the represented features is not necessarily causal. Background scientific theory is vital for inferring causal claims from information-theoretic claims about features.” (Shech and Tamir, 2022, p. 18)
“Origin theories about causal links such as PHT are not merely suspect; they fall outside the scope of target features to be understood by ML models according to the TML hypothesis.” (Shech and Tamir, 2022, p. 19)
One cannot gain causal understanding based on the feature relationships represented by data





