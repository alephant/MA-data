Objection 1: 
R&B challenge the arguments from analogy that Sullivan makes. We do not know what function a DNN implements in the same way we know what function an algorithm implements that computes factorials. In the case of the factorials we can give a description of the function that governs the input-output relation. For each input number the function computes its factorial. This description does not change when the computational implementation of the function changes.
Knowing the classifier function that governs the input-output relation of a DNN is not possible. 

R&B challenge the analogy between the Schelling model and DNN with which Sullivan argues that in both cases understanding model implementation is irrelevant for understanding phenomena using the models. R&B argues that the Schelling model implements "rules of transition" that are predetermined by the modeler, and thus, known, whereas the "rules of transition" in the case of a DNN are not known by the modeler. The overall "story" that the Schelling model implements makes the model behavior accessible to reason, whereas a DNN cannot be reasoned with.
R&B argue that the fundamental problem is that “we know little about what function is computed by an DNN.” (Räz and Beisbart, 2022, p. 8)

Objection 2: 
R&B challenges the assumption that an understanding of the higher-level functioning of DNN is enabled through indirect means such as saliency maps. Sullivan introduces these indirect means as a way “to satisfy our need to know the high level details of how the model works to open the door to understanding the phenomenon the model bears on” (Sullivan, 2022, p. 18) However, if these means do not enable epistemic access to the higher-level functioning of DNN, then understanding of phenomena - in the sense that Sullivan is argueing for, i.e. explanatory understanding - cannot be gained from black-boxed DNN. R&B directly attack the claim that 
- “Simply having a highly predictive model, and knowing the high-level emerging properties of the model, uncovers that it is possible to use a machine learning representation for disease prediction.” (Sullivan, 2022, p. 20)

Objection 3: 


Objection against Sullivan's notion of understanding:
- Sullivan employs a strong notion of understanding (explanatory understanding) which she cannot reasonably back up
- however, employing a weak notion would not do her a favour as it has nothing to do with true understanding for which she wants to argue (objectional understanding; heuristics; exploration)
- Sullivan argues that ML can provide explanatory understanding; her reasons are not sufficient; in fact there are fundamental reasons why DNN cannot provide explanatory understanding

- thus, DNN lack "functional transparency" (Creel, 2020) which they argue is in fact a problem for understanding from ML models
- DNN do not "implement" classifier function, rather classifier is a property of DNN and represents the input-output relation of the DNN (but for the sake of argument, R&B give her that)
- DNN do not enable explanatory understanding
	- predicitive power is not explanatory power
	- how-possibly explanations do not provide explanatory understanding
	- DNN do not even provide how-possibly explanations