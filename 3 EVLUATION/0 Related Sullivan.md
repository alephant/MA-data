**Boge agrees with Sullivan: h-opacity does not necessarily impair understanding**
“Regardless of which side we take in this debate in detail, the tenor which is common to all these positions clearly carries over to DL’s h-opacity: In order to gain understanding of underlying mechanisms from DL, we need not understand the training or the learning algorithm in full detail (Sullivan, 2019). However, whatever potentially prevents understanding in DL, in the sense of a disconnect with underlying, datagenerating mechanisms, must therefore be something else.” (Boge, 2022, p. 60)

**Sullivan's implementation irrelevance fits views on CS that dismiss their opacity to be an in-principle problem for understanding from them**
“This assessment resonates well with various proposals on the opacity of CSs. For instance, Durán (2018, p. 108; emph. added) argues that “researchers are only interested in a limited amount of information that counts for the justification of results.” For Durán, this allows disputing the epistemic relevance of the unknown elements, and so whether CSs are even interestingly opaque at all. Similarly (Lenhard, 2006, pp. 611–613), who embraces CSs’ opacity and considers it “a major obstacle to explanatory potential” (Lenhard, 2019, p. 224), still acknowledges that CSs promote understanding: a researcher can acquire a kind of orientation within the model [...] based on experience of the model’s behavior [....] mediated by the calculating machine[...], whereas the model itself remains epistemically opaque. (Lenhard, 2006, p. 613)” (Boge, 2022, p. 60) 