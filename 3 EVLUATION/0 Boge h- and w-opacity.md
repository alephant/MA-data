**Boge: What is needed for DNN to provide scientific understanding?**
“==Without a doubt, it is possible to understand something about underlying mechanisms on the basis of the outputs of a DNN though.== Hence, what is required to facilitate that understanding? I believe that the following three steps are crucial in that respect: (I) The conceptualization of input and output, prior to training; (II) establishing what the (trained) DL model represents, on account of (I); and (III) connecting that represented something to underlying mechanisms on the basis of further background knowledge.” (Boge, 2022, p. 55)

“I take it that this analysis in terms of steps (I)–(III) is an ==assessment of understanding from DL==, or the possibility of a want thereof, compatible with Sullivan’s recent appraisal of what she calls ‘link uncertainty’, i.e., the “lack of scientific and empirical evidence supporting the link that connects the model to the target phenomenon” (Sullivan, 2019,  p. 8). Because of the dependence on (I) and, especially, (III), the DL model on its own is conceptually too poor to provide an understanding of underlying mechanisms: ==Only if, via (I) and (II), a connection can be made to those mechanisms in a final step, (III), will a DL-success promote an understanding of them.==” (Boge, 2022, p. 56 ff.)

Boge assesses the possibility for understanding.
Boge's analysis is compatible with Sullivan's account for link uncertainty.
Boge's step I and II are necessary for linking the model to the target in step III.


**The two dimensions to the opacity-problem of DNN: how does it learn to estimate a desired function and what in the data drives this process?**
“As a matter of fact, it is easy to recognize the very same process involved in h-opacity as involved also in w-opacity. This is what it means that there are two dimensions to ‘the’ opacity-problem in DL, instead of two problems. When a DNN learns to approximate a desired function, it is hence not only opaque how, precisely, it achieves this goal: It is also opaque what it is about the data that drives this process.” (Boge, 2022, p. 62)

**H-opacity refers to the opacity of how the machine learns; it concerns mainly functional transparency**
“The point hence is that, despite some abstract analogies between human and Deep Learning, it is in important respects opaque how the machine learns. Call that h-opacity. ==H-opacity concerns the way in which a DNN automatically alters the instantiated function in response to data==.” (Boge, 2022, p. 59)

**“W-opacity constitutes an independent dimension to the opacity of DL, and may therefore create an independent epistemic problem (as I believe it does).”** (Boge, 2022, p. 65ff.)

**W-opacity refers to the opacity of what was learned by the model**
“The second dimension concerns the question of what was learned by the machine. Call that w-opacity. As I shall show, w-opacity is, ultimately, the distinctive factor which sets DL apart from all traditional models and, eventually, impairs our ability to acquire scientific understanding in a special way.” (Boge, 2022, p. 61)

**w-opacity is non-reducible to h-opacity**
“The purpose is to show that w-opacity is non-reducible to h-opacity (which is continuous with CSs’ opacity), and so that there is a novel challenge.” (Boge, 2022, p. 61)

**The combination of DNN formal model elements not carrying any meaning and the opacity of what they learn is problematic for understanding**
“Subsequently, I will argue that this unique combination of c-instrumentality and w-opacity is likely to lead to an unprecedented gap between scientific discovery and understanding, at least when DL is used under certain conditions of interest in several sciences.” (Boge, 2022, p. 61)

**Automatically discovered insights drive DNN prediction and refer to the model elements that are unknown but epistemically relevant to understand the DNN**
“In fact, the unknowns are what makes w-opacity (and DL, accordingly) special: ==They correspond to automatically discovered insights; complex, non-obvious features that can be abstracted from the data and allow the machine to discriminate.== Their existence is an empirical matter, so I will provide examples below. It is these very features that drive predictive success but, as the examples will show, at the same time yield the greatest prospects for understanding. They are hence epistemically relevant.” (Boge, 2022, p. 61)

=="complex, non-obvious features that can be abstracted from the data" sounds a lot like T&S approach.==

**Example of unknown elements that are learned by the DNN = automatically discovered high-level features abstracted from data**
“The surprising result of Baldi et  al. (2014) was that the DNN always outperformed the other algorithms when given access only to the low-level features, and had a modest additional increase when given access also to the high-level features. The other algorithms instead exhibited major differences in performance between these situations. From this, Baldi et al. (2014, p. 7; emph. added) concluded “that [DNNs] are automatically discovering the insight contained in the high-level features.” ==These automatically discovered high-level features are a clear instance of the unknown ‘whats’==, but their existence is by no means restricted to particle physics.” (Boge, 2022, p. 62)

i.e. during training, the DNN automatically abstracts high-level features from data that make it predictively successful and are epistemically relevant for understanding the functioning of the DNN.  

Key question based on Humphreys: can I make justifiable claims on the basis of a process of which it is impossible to know all its epistemically relevant factors?

**W-opacity makes DNN different from all other models as there is a key difference in what makes DNN predictively successful**
“‘But’, you may insist, ‘is it not equally opaque what features of initializations to a CS drive its success?’ I believe this to be a confusion: Due to the interplay between target-conceptualization and coding, all information about what makes initial values play out in terms of specific simulation outputs is contained in the algorithm, not the data used for initialization. As far as I can tell, there really is no pendant for w-opacity in CSs or other scientific models.” (Boge, 2022, p. 63)

