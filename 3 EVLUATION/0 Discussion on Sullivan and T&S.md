Problematic assumptions of Sullivan:

**"We know the algorithm that governs the input-output relation"**

Creel: no, we don't know what the learned algorithm of a DNN is.
We might get some indication what the algorithm is that governs the input-output relation for a particular model decision through post-hoc explanations, but PHE do not give uns any insight how the ML model actually comes to its decision; PHE is just an accurate characterization

**"It is sufficient to know the high-level emergent properties of a ML model as these determine model outcome"**

Creel: no, we don't know aspects of how the algorithm is implemented in code that bring about the model outcome;
We lack structural transparency to understand "how steps interact to bring about the algorithm or to what extent each individual step contributed to the final result"
i.e. there are model elements that are unknown which are epistemically relevant for knowing which algorithm is being implemented by the DNN (Humphreys definition)
We might have understanding of some aspects of how the algorithm is implemented to bring about the ML outcome (learning algorithm), but is this truly sufficient to determine the model aspects that drive the model outcome?

R&B: we don't know how high-level properties make DNN predictively successful
how can we use ML prediction for understanding phenomena, if we don't know what determines the prediction?





How do we know which aspects of implementation are irrelevant?

Making judgments like “this path from A leads to B.” is analogous to ML model's estimations of learned p(Y given X)?

