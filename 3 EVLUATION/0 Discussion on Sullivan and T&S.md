

**Sullivan: we know the algorithm that governs the input-output relation**

Creel: no, we have some indication of it, but we don't know what the learned algorithm is.
PHE provide some local understanding of relevant features and functioning of the algorithm, but no global understanding of the algorithm.
PHE do not give uns any insight how the ML model actually comes to its decision.

**Sullivan: it is sufficient to know high-level emergent properties as these determine model outcome (really?)**

Creel: DNN lack structural transparency, i.e. we don't know how the algorithm is implemented in code that bring about the model outcome.
We lack structural transparency to understand "how steps interact to bring about the algorithm or to what extent each individual step contributed to the final result"
i.e. there are model elements that are unknown which are epistemically relevant for knowing which algorithm is being implemented by the DNN (Humphreys definition)
We might have understanding of some aspects of how the algorithm is implemented to bring about the ML outcome (learning algorithm), but is this truly sufficient to determine the model aspects that drive the model outcome?

R&B: we don't know how high-level properties make DNN predictively successful
how can we use ML prediction for understanding phenomena, if we don't know what determines the prediction?





How do we know which aspects of implementation are irrelevant?

Making judgments like “this path from A leads to B.” is analogous to ML model's estimations of learned p(Y given X)?

