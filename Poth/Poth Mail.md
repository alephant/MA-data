Teile der Begründung von Functionally Approximate Irrelevance hängen von Differenzen der Parameterisierung zwischen zwei ML-Modellen ab. Anhand dieses Vergleichens sind T&S in der Lage FAI einzuführen und damit die Grundlage für ihre modes of understanding zu schaffen. Auch erst anhand dieses Vergleichens sind sie in der Lage zu argumentieren, dass selbst highest-level opacity kompatibel mit ihren modes of understanding ist. ==Sullivan führt ihre Argumentation auf Basis von Betrachtungen einzelner ML Modelle.== Ich weiß nicht, ob das ein Problem ist. ==Ich weiß auch nicht, wozu T&S überhaupt für eine Kompatibilität mit highest-level opacity argumentieren.== Ich verstehe nicht wie highest-level opacity für ihre Argumentation arbeitet. Sullivan stellt ja klar, dass DNN keine highest-level black boxes sind. Ich sehe aber auch nicht wie das die Argumentation von T&S negativ beeinflussen würde, wenn ich ihren highest-level opacity claim abschieße. 


Ich sehe Punkte zur misrepresentation link uncertainty von T&S, die ich angreifen könnte. Ich muss das auf Englisch schreiben. I dont think Sullivans link is concerned with which features of the data are intended or not, but with which features of the data are empirically validated as being actual features of the external target. Aren't confounding variables in the data just features that cannot be empirically validated as being actual features of the external target, and thus, the differentiation in data-misrepresentation and link uncertainty is irrelevant? What if link uncertainty already captures the aspects that misrepresentation link uncertainty wants to capture?
==- FAI vs misrepresentation LU?==


Grundsätzlich bin ich mir immer noch nicht sicher, worauf sich T&S modes of understanding beziehen. Welches phenomenon meinen sie, wenn sie schreiben "FI understanding of the phenomenon"? Meinen sie "FI understanding of TML-targets to enable understanding of external target phenomenon that the model bears on"? Hier sehe ich das Problem, dass das "appropriate target of understanding from ML models" von T&S ja als TML-targets festgelegt wurde, d.h. "appropriate target of understanding from ML models" = "phenomenon that the model bears on". Ist das nicht ein Zirkelschluss? ==Das ML model ist dafür da das ML model zu erklären?== Darüber bin ich auch während dem Q&A zu meiner Präsentation ins Stocken gekommen. Vielleicht verstehe ich was nicht richtig.


Daneben, was meinen T&S, wenn sie schreiben "In order to study trained ML models, especially complex DL models, for insight into external targets, we must first clarify how a link from ML models to TML targets (horizontal bottom side of a C-schema) may work."

Ich dachte die Horizontalverbindungen im C-Schema betreffen die Repräsentation- und Inferenzverbindungen zwischen dem Model und seinem external Target, und dass das TML-target intra-model verortet ist, d.h. sich allein auf intra-model inferences bezieht?

Ich komme da ehrlich gesagt nicht weiter im Verständnis.


Sie haben recht, ich denke, Sullivans link noch nicht richtig verstanden zu haben. So wie ich das verstehe: ein Modell identifiziert possible causes of the target phenomenon, indem es ein mapping zwischen Modellelementen und realen Elementen herstellt. Wird diese Identifikation empirisch belegt, so spricht Sullivan davon, dass ein link zwischen Model und Target hergestellt wird. Was passiert, wenn der empirische Beleg sich im Nachhinein als falsch herausstellt? Was ist der Zustand des link zu dem Zeitpunkt, als das noch nicht klar ist? Wie kann Sullivan davon sprechen, dass link uncertainty entfernt wird? Reduziert ja, aber entfernt? Ich habe die Bedeutung von Begriffen wie mapping, identification, connection between model and target nicht trennscharf parat.


