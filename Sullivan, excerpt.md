Whether or not ML models enable understanding of phenomena is not a question of model opacity, but depends on the degree of link uncertainty between the model and the phenomenon. Opacity is not an in-principle problem for understanding from ML models, i.e. the reason why opaque do not enable understanding is not opacity, but link uncertainty.
Opacity is not the deciding factor for understanding from ML models. The implementation of ML models can be black-boxed on various levels without necessarily impeding understanding. 
Sullivan argues that understanding phenomena requires understanding higher-level emergent properties and not understanding detail implementations of the ML model. As these higher-level properties of the ML model are abstract representations of the relevant features of the target phenomenon she assumes that these properties drive the decision of the ML model. Thus, the higher-level properties are the relevant basis for understanding on the part of the ML model, and not its detail implementation. The detail model implementation is irrelevant for understanding.

As long as the ML model is not a highest-level black box, it can in-principle enable understanding. She discusses several factors that help to rule out that the level of opacity is lower than for a highest-level black box. The modeller has background knowledge of the model and the target which enables understanding of higher-level properties of the ML model. Indirect means such as saliency maps could help to check whether or not the model decides as intended, thus, higher-level understanding of the ML model's input-output-relations could be enabled. This would not be the case for a highest-level black box as one would only know the inputs and outputs, but not the relation that governs the overall model behavior. 

The model maps model elements to real-world elements in order to represent target features as abstract representations. The target features that are abstractly represented by the model are assumed to be the causally relevant features for the target phenomenon. For understanding to be possible it seems that it needs to be intelligible that the higher-level abstractions of the model are the cause that drives model behavior. The "link" between higher-level abstractions and model decision needs to intelligible.

For the Schelling model the decision parameter is an abstraction of the feature of the target phenomenon, in this case racial segregation, that is believed to be a causally relevant feature. The decision parameter is implemented as a simple algorithm in the model and determines the behavior of the model elements. Based on this modelling of possible causes the higher-level feature abstractions of the Schelling model emerge that drive the overall model behavior and its decision. Thus, understanding of the phenomenon is enabled based on the intelligible relation between higher-level abstractions and the decision of the Schelling model. Understanding whether or not the model was implemented with red vs. green or blue vs. orange model elements is irrelevant, i.e. understanding detail implementations of the Schelling model is irrelevant for understanding the phenomenon that the model bears on. Lower-level implementation details do not need to be transparent, whereas opacity of higher-level abstractions of the model would impede understanding. 

When the higher-level abstraction of the model can be empirically validated as representing the actual causes operating in racial segregation, then the Schelling model is linked to the target phenomenon. Sullivan would argue that the Schelling model could then answer explanatory how-actually questions and would enable understanding of actual causes of racial segregation compared to only possible causes. 

This implies that Sullivan is striving for explanatory understanding that is enabled by establishing a link between model and target. 

Sullivan uses the Schelling model to argue from analogy that what is true for simple models is also true for complex models such as DNN. 
Sullivan argues that the melanoma DNN implements a classifier function that abstracts features of the target phenomenon, in this case melanoma, that represents features that are assumed to be causally relevant for melanoma. A DNN is a complex model as it implements computations that are unintelligible to the modeller. However, the modeller still has background knowledge of the DNN concerning model architecture or the kind of sample data that it uses during training. 
Sullivan argues that even though the melanoma DNN is a complex model, its higher-level functioning is intelligible. It is not a highest-level black box. And even though lower-level implementation details are opaque, understanding them is not relevant for understanding melanoma which the DNN bears on. 
Analogous to the Schelling model, model implementation is irrelevant for understanding the target phenomenon. 

The problem that I see is that there could be a meaningful difference between model implementation being irrelevant versus understanding model implementation being irrelevant. R&B seem to dispute the first in the case of DNN and argue that model implementation of DNN is very much relevant for understanding from DNN. It could be that Sullivan's analogy from simple to complex models fails in this regard, i.e. that it is not so simple to argue that because detail implementations of the Schelling model are irrelevant for its overall model decision as the basis for understanding from it, that the computational implementation of a DNN is irrelevant for its overall classification that is the basis for understanding from DNN.

