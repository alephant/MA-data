ML model as scientific models
- ML algorithms as a class of model
- ML models are predicitively powerful EXAMPLE
- it is not well understood why ML models 
Opacity and the black-box problem



## understanding from models
- what are generally accepted views on preconditions for understanding from models?
- 
## DNN used in scientific practice:

- example for predicting brain function without understanding
- ML model use as scientific models for prediction, explanation and for exploration
- 


## DNN characteristics - capacities and limitations for understanding

- ML complexity not a bug but a feature
- Example illustrating the difference in model characteristics between DNN and mathematical models used for explanation
- As DNNs fulfil different desiderata as other models, they coexist with other useful scientific models in cognitive science. 
- DNNs excel in non-theoretical desiderata. ==What are these?==

## Intro to model opacity in general
why should we care about opacity in the first place?
- we want to use ML models to explain and understand
- phil. of science: even though ML models lack transparency, how come they have high predictive power?

in which cases does opacity matter? in which not?

how is the relation between model intelligibility and model opacity?

what can we learn by investigating opacity in ML models?
can opacity tell us something about unique properties of ML models?

how does opacity differently affect the capacity for a model to explain vs enable understanding?
- difference between explanation and understanding
- a good explanation is one that is linked to scientific evidence
- understanding is linked to a transparent and simple model
- thus, lack of transparency seems to be more related to understanding than to explanation


## necessary introductions for Sullivan vs. T&S
- target of understanding? assigning target?
- understanding model implementation? why do we want or need to understand model impl.? 
- understanding phenomena using the model?
- relation between understanding model implementation and understanding phenomena?
- 

## raising questions for later chapters
opacity
- which factors influence the level of opacity? background knowledge? 
- which factors influence whether or not opacity is a problem? non-epistemic factors?
- which kind of opacity is incompatible with understanding?

Link Uncertainty
- what happens if the empirical validation of the link proves to be false?
- what is the state of the link in the meantime?
- Sullivan speaks of removing the link - can the link be removed completely?
- 

model-centric view/ understanding intra-model inferences
- What if link uncertainty already captures the aspects that misrepresentation link uncertainty wants to capture?




## explainability in AI

explainability in AI covers a wide range of aspects from questions of usefulness of AI models, pragmatic concerns of AI implementation and questions regarding the usefulness of AI models as scientific models for understanding
the problem of opacity itself has various aspects to it
from a philosophy of science perspective one can ask whether AI models can be used as scientific models to explain und enable understanding

field of XAI: how can we build ML models in order to use them for good explanations and understanding?



philosophy of science is a subfield of epistomology
the problem of ML opacity is a subfield of XAI





