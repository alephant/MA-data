---
category: literaturenote
tags: 
citekey: "Sullivan_2022a"
status: unread
dateread:
---

>[!Cite]
>Sullivan, E. (2022). Link Uncertainty, Implementation, and ML Opacity: A Reply to Tamir and Shech. In _Scientific Understanding and Representation_. Routledge.

>[!Synth]
>**Contribution**::
>
>**Related**:: 

>[!Metadata]
> **FirstAuthor**:: Sullivan, Emily
~
>**Title**:: Link Uncertainty, Implementation, and ML Opacity: A Reply to Tamir and Shech
>**Year**:: 2022
>**Citekey**:: Sullivan_2022a
>**itemType**:: bookSection
>**Book**::  
>**Publisher**:: Routledge  
>**ISBN**:: 978-1-00-320290-5

>[!LINK]
>
>[2022_Sullivan_Link_Uncertainty,_Implementation,_and_ML_Opacity.pdf](file://C:\Users\a.niemeier\Zotero\storage\2S7GQNP4\2022_Sullivan_Link_Uncertainty,_Implementation,_and_ML_Opacity.pdf).

>[!Abstract]
>
>This chapter responds to Michael Tamir and Elay Shech’s chapter “Understanding from Deep Learning Models in Context” in this volume.
>.
>
# Notes.


# Annotations%% begin annotations %%  
  
  
  
### Imported: 2024-02-16 4:11 pm  
  
  
  
<mark style="background-color: #ffd400">Quote</mark>  
>How these topological features are expressed (or computed)—either by a visual depiction of nodes and edges or in mathematical notation—does not affect the general goal of representing NYC’s subway system.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>However, the general goal of the algorithm must be known for understanding to be possible.  
  
<mark style="background-color: #e56eee">Quote</mark>  
>The lower-level algorithms become the way of implementing higher-level algorithms. Thus, lower-level algorithm goals are also a matter of implementation.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Which specific learned weights are necessary to know—and their level of description—for understanding phenomena with ML models is the question  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Distinguishing between the features of a model that are in principle irrelevant and those that are intrinsic difference-makers—from a view from nowhere—is untenable. Relevance is related to a target.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>In this case, the LU that would need to be reduced to provide understanding is the LU between the coding language, the model, and the deployed scenario.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>However, feature importance becomes more salient for determining how to reduce LU  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>There is no space to say that there are “varied details [that] matter to the studied target,” but those details are also irrelevant for understanding the target.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Differencemaking is entwined with the target.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>The central question is how to reduce LU for various types of targets, and which aspects of how the model works need to be known to reduce LU for specific cases.  
  
<mark style="background-color: #e56eee">Quote</mark>  
>I sought to show that the target of understanding is important.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>the highlevel decision points of a model  
  
<mark style="background-color: #ffd400">Quote</mark>  
>that the real epistemic problem of opacity becomes an external problem  
  
<mark style="background-color: #ffd400">Quote</mark>  
>However, other aspects of data manipulation or normalization, like RGB color changes, may not be a representational worry or a case of LU in the way Tamir and Shech seem to indicate.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>When models enable understanding of real-world phenomena (or possible real-world phenomena), the target of understanding is not the relationships of features represented by the data.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>It is not the ML model alone that provides understanding; the model induces an explanation paired with external links connecting the model to the phenomena that enables understanding. This is why I argued that most ML models merely provide how-possibly explanations. The ML models do not provide causal support themselves, but rather indicate possible (causal) hypotheses that additional research must justify through reducing LU.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Restricting ML models to the model-centric view of the TML hypothesis goes against the goals that many ML researchers themselves postulate, as well as keeping ML models apart from the rest of model-based science, which unnecessarily constrains our scientific toolbox.  
  
  
%% end annotations %%




%% Import Date: 2024-02-16T16:11:26.730+01:00 %%
