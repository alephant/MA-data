---
category: literaturenote
tags: 
citekey: "Sullivan_2022b"
status: unread
dateread:
---

>[!Cite]
>Sullivan, E. (2022). How Values Shape the Machine Learning Opacity Problem. In K. Khalifa, I. Lawler, & E. Shech, _Scientific Understanding and Representation_ (1st ed., pp. 306–322). Routledge. [https://doi.org/10.4324/9781003202905-27](https://doi.org/10.4324/9781003202905-27)

>[!Synth]
>**Contribution**::
>
>**Related**:: 

>[!Metadata]
> **FirstBookauthor**:: Khalifa, Kareem
> **Bookauthor**:: Lawler, Insa
> **Bookauthor**:: Shech, Elay
~> **FirstAuthor**:: Sullivan, Emily
~
>**Title**:: How Values Shape the Machine Learning Opacity Problem
>**Year**:: 2022
>**Citekey**:: Sullivan_2022b
>**itemType**:: bookSection
>**Book**::  
>**Publisher**:: Routledge  
>**Location**:: New York
>**Pages**:: 306-322  
>**ISBN**:: 978-1-00-320290-5

>[!LINK]
>
>[2022_Sullivan_How_Values_Shape_the_Machine_Learning_Opacity_Problem.pdf](file://C:\Users\a.niemeier\Zotero\storage\UPCYVHQZ\2022_Sullivan_How_Values_Shape_the_Machine_Learning_Opacity_Problem.pdf).

>[!Abstract]
>
>One of the main worries with machine learning model opacity is that we cannot know enough about how the model works to fully understand the decisions they make. But how much is model opacity really a problem? This chapter argues that the problem of machine learning model opacity is entangled with non-epistemic values. The chapter considers three different stages of the machine learning modeling process that corresponds to understanding phenomena: (i) model acceptance and linking the model to the phenomenon, (ii) explanation, and (iii) attributions of understanding. At each of these stages, non-epistemic values can, in part, determine how much machine learning model opacity poses a problem.
>.
>
# Notes
# Annotations  
(09/02/2024, 12:52:33)

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=1&annotation=UKDZRKIJ) “the problem of machine learning model opacity is entangled with non-epistemic values” ([Sullivan, 2022, p. 1](zotero://select/library/items/TDHYGL55)) **main claim**

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=1&annotation=2BGZKL2K) “Many agree that for a model to enable understanding, transparency (Dellsén 2020; Strevens 2013), simplicity (Bokulich 2008; Kuorikoski and Ylikoski 2015; Strevens 2008), and the ability to manipulate the model (De Regt 2017; Kelp 2015; Wilkenfeld 2013) are all necessary” ([Sullivan, 2022, p. 1](zotero://select/library/items/TDHYGL55)) **read on transparency, simplicity, intelligibility** as often cited criteria for model usefulness

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=1&annotation=GPVSLD3S) “Therefore, the important question is what should be transparent.” ([Sullivan, 2022, p. 1](zotero://select/library/items/TDHYGL55))

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=2&annotation=XQ6SKKZ9) “nonepistemic values are inescapable in scientific practice” ([Sullivan, 2022, p. 2](zotero://select/library/items/TDHYGL55)) **Sullivan's view values in scientific practice**  
thus, non-epistemic values impact ML opacity

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=2&annotation=M7NMKNVC) “Biddle (2020) pinpoints several aspects of the model pipeline that involves tradeoffs closer to representational risk that must be resolved in nonepistemic ways, such as identifying the problem to be modeled, training and benchmarking, algorithm design, and model deployment decisions” ([Sullivan, 2022, p. 2](zotero://select/library/items/TDHYGL55)) which would reduce misrepresentation link uncertainty

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=2&annotation=NDWAF9QU) “Specifically, the problem of opacity is a function of how much link uncertainty (LU) the model has (Sullivan 2022a).” ([Sullivan, 2022, p. 2](zotero://select/library/items/TDHYGL55)) **problem of opacity as a function of LU**

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=2&annotation=VP7PZYXZ) “There are various interpretability techniques available for “black-box” ML models that provides us with the necessary details regarding how the model made its decision such that the problem of model opacity becomes an external problem of LU.” ([Sullivan, 2022, p. 2](zotero://select/library/items/TDHYGL55)) **tools to check higher-level goals**:  
tools that give us (just enough) insight into model impl. to determine the higher-level algo. goals which must be known for understanding to be possible

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=2&annotation=EP838V9V) “Thus, one central aspect of accepting whether a model could be used to explain and enable understanding is in linking the model’s inferences to the target phenomenon.” ([Sullivan, 2022, p. 2](zotero://select/library/items/TDHYGL55)) **clarify what do we actually link?** model's inferences  
are these non-epistemic values?

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=3&annotation=FWQPCF6P) “Similarly, on such a view, the link between the model and the specific purpose needs to be established as adequate enough to provide insight into the research question identified” ([Sullivan, 2022, p. 3](zotero://select/library/items/TDHYGL55)) **accepting model adequacy as a precondition for establishing a link**  
on which grounds to be assume model adequacy?  
when is model adequacy enough to enable a link?

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=3&annotation=UNXDE8IN) “All said, one of the central features of accepting whether a model can provide understanding of phenomena is accepting whether the links between the model and the phenomenon are strong enough.” ([Sullivan, 2022, p. 3](zotero://select/library/items/TDHYGL55)) how accepting the strength of the link connecting the model to the target influences model acceptance  
thus, when empirical validation turns out to be lower than thought, the link will not be accepted as strong, thus, the model will not be accepted providing understanding

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=3&annotation=P9X83GS6) “if the proponents of inductive risk are right, then deciding when we have reduced LU enough to understand can involve the consideration of non-epistemic values.” ([Sullivan, 2022, p. 3](zotero://select/library/items/TDHYGL55)) **possible answer to decide when LU is low enough** to enable understanding when inductive risk is minimized which involves considering non-epistemic values

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=3&annotation=2GNJ5DPD) “It is my contention that there is also an epistemic risk in accepting that the model could explain or provide understanding of risk factors for patients with pneumonia.” ([Sullivan, 2022, p. 3](zotero://select/library/items/TDHYGL55)) there is not just a risk in accepting model prediction but also in accepting model understanding

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=3&annotation=G6CAXYCN) “Thus, accepting whether a particular model is adequate to explain or provide understanding involves the (implicit) weighing of these values.” ([Sullivan, 2022, p. 3](zotero://select/library/items/TDHYGL55)) **as non-epistemic values influence whether or not one accepts a model as being adequate to provide understanding, non-epistemic values also influence whether or not the link between model and target is strong enough/ adequate enough/ when LU is low enough/ and what empirical support is necessary to overcome the problem of opacity externalized to LU.**

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=3&annotation=UKZBEB93) “Thus, ML models face link uncertainty risk. Judgments about when there is enough evidence connecting a model to its target, such that model opacity is not an epistemic barrier, can involve epistemic risk entangled with non-epistemic values.” ([Sullivan, 2022, p. 3](zotero://select/library/items/TDHYGL55))

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=3&annotation=LHHKBKX4) “Representational questions regarding what data should be used to represent the target phenomena, specific ML architectures suited for the problem, and even the specific interpretability technique chosen to gain high-level insight into black-box ML models require judgments that reflect values.” ([Sullivan, 2022, p. 3](zotero://select/library/items/TDHYGL55)) **modelling process needs to take non-epistemic values into account**:  
  
so even the methods that T&S discuss to reduce their notion of misrepresentation link uncertainty (sampling methodology, data preparation) need to be judged whether or not they take non-epistemic values into account

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=4&annotation=UML43SP3) “Since the LU between the original neural network model and the target was high, the opacity of the model created a greater epistemic barrier. Moreover, since the LU-risk was high, the need for more research into the external connection between the model and the target increases further. Therefore, if the extent to which model opacity undermines explanation and understanding is based on the degree to which there is an external connection between the model and the target, then the problem of opacity in ML is entangled with non-epistemic values, since the process of accepting whether there is sufficient connection between the model and its target is itself entangled with non-epistemic values.” ([Sullivan, 2022, p. 4](zotero://select/library/items/TDHYGL55))

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=8&annotation=C3FDJVJZ) “pragmatic encroachment theories of knowledge suggest that the stakes of a situation influence attributions of knowledge” ([Sullivan, 2022, p. 8](zotero://select/library/items/TDHYGL55)) the stakes decide when you can say that you know

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=8&annotation=GRBRBXBB) “Specifically, depending on the stakes, someone might need to know more about how a given ML model works to understand.” ([Sullivan, 2022, p. 8](zotero://select/library/items/TDHYGL55)) **argument on the relation between stakes of a situation and need to understand model implementation for understanding phen.**  
high stakes: more need  
low stakes: lower need

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=8&annotation=CWRI89PB) “It is not a settled question as to what constitutes understanding. Some argue that understanding is just a kind of knowledge (Riaz 2015), whereas others argue that understanding is distinct from knowledge (Hills 2016; Lawler 2019).” ([Sullivan, 2022, p. 8](zotero://select/library/items/TDHYGL55)) **read**: what constitutes knowledge

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=9&annotation=FI57LEEC) “hen we can motivate a pragmatic encroachment view for when model opacity becomes a problem for understanding” ([Sullivan, 2022, p. 9](zotero://select/library/items/TDHYGL55)) how many what-if question should I be able to answer on model implementation to be able to attribute understanding of the phen.  
can I answer just a few what-if questions on model impl. due to model opacity, by which no und. can be attributed?

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=9&annotation=893QNKKJ) “There are two broad ways that non-epistemic values impact our attributions of understanding in the context of ML models: (i) the domain requires greater model transparency to attribute understanding, and (ii) the personal stakes in a given context can require greater model transparency to attribute understanding” ([Sullivan, 2022, p. 9](zotero://select/library/items/TDHYGL55)) **non-epistemic values (domain and personal stakes) require model transparency to attribute understanding**  
i.e. whether or not model opacity prevents attribution of understanding depends on non-epistemic values

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=9&annotation=MGSLCB4R) “However, in the case of a doctor recommender platform, given the importance that doctors have in someone’s well-being (it could be a matter of life or death), a user would need to answer more what-if-things-had-been-different questions about how the algorithm works in order to understand.” ([Sullivan, 2022, p. 9](zotero://select/library/items/TDHYGL55)) **critique**: how is this different from personal stakes? how to decide the stakes of a domain if not on a subjective basis? what if in a specific context news have higher stakes than doctor (need to check whether a certain info is true or not)?

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=9&annotation=4WEZG7Y6) “Because of the greater consequences of error, we need to be able to answer more what-if-things-had-been-different-questions regarding how the model works in order to attribute someone with understanding.” ([Sullivan, 2022, p. 9](zotero://select/library/items/TDHYGL55))

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=10&annotation=I59FT5XH) “In other words, given how important it is for Zoe to find an apartment, and the higher risk of potential bias” ([Sullivan, 2022, p. 10](zotero://select/library/items/TDHYGL55)) **critique**: what about the exact mirror case "the higher risk of potential preference" that requires greater model transparency to attribute understanding?  
It seems like every personal stakes case necessarily involves the requirement for model transparency for the mirror personal stakes case.  
how can we then set a threshold for what-if questions that would enable the attribution of understanding?

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=10&annotation=3H54S6UI) “Instead, my aim was to argue that non-epistemic factors contribute to the question as to how much of a problem opacity really is.” ([Sullivan, 2022, p. 10](zotero://select/library/items/TDHYGL55))

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=10&annotation=8WAMV33E) “However, in order to explain and gain understanding with an ML model the problem of opacity greatly depends on features external to the model instead of features internal to it (i.e., link uncertainty and empirical support, explanatory functions, and the social and personal significance of the model and its domain).” ([Sullivan, 2022, p. 10](zotero://select/library/items/TDHYGL55))

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=11&annotation=JQQAQF6U) “Bokulich, Alisa. 2011. “How Scientific Models Can Explain.”” ([Sullivan, 2022, p. 11](zotero://select/library/items/TDHYGL55)) **read**: when does a model explain

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=13&annotation=5CN7JHBA) “See Karaca (2021) for a discussion of type I and type II errors in ML and a proposal for costsensitive error classification to minimize inductive risk during model construction.” ([Sullivan, 2022, p. 13](zotero://select/library/items/TDHYGL55)) **read**: which might be relevant to decide when LU is low enough to enable understanding by minimizing inductive risk

[Go to annotation](zotero://open-pdf/library/items/UPCYVHQZ?page=14&annotation=M2HFMAVR) “See Gilpin (2018) for a review of various ML interpretability methods.” ([Sullivan, 2022, p. 14](zotero://select/library/items/TDHYGL55)) **read**: tools that give us (just enough) insight into model impl. to determine the higher-level algo. goals which must be known for understanding to be possible.


# Annotations%% begin annotations %%  
  
  
  
### Imported: 2024-02-09 12:52 pm  
  
  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>the problem of machine learning model opacity is entangled with non-epistemic values  
  
<mark style="background-color: #5fb236">Quote</mark>  
>Many agree that for a model to enable understanding, transparency (Dellsén 2020; Strevens 2013), simplicity (Bokulich 2008; Kuorikoski and Ylikoski 2015; Strevens 2008), and the ability to manipulate the model (De Regt 2017; Kelp 2015; Wilkenfeld 2013) are all necessary  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Therefore, the important question is what should be transparent.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>nonepistemic values are inescapable in scientific practice  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Biddle (2020) pinpoints several aspects of the model pipeline that involves tradeoffs closer to representational risk that must be resolved in nonepistemic ways, such as identifying the problem to be modeled, training and benchmarking, algorithm design, and model deployment decisions  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Specifically, the problem of opacity is a function of how much link uncertainty (LU) the model has (Sullivan 2022a).  
  
<mark style="background-color: #ffd400">Quote</mark>  
>There are various interpretability techniques available for “black-box” ML models that provides us with the necessary details regarding how the model made its decision such that the problem of model opacity becomes an external problem of LU.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Thus, one central aspect of accepting whether a model could be used to explain and enable understanding is in linking the model’s inferences to the target phenomenon.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Similarly, on such a view, the link between the model and the specific purpose needs to be established as adequate enough to provide insight into the research question identified  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>All said, one of the central features of accepting whether a model can provide understanding of phenomena is accepting whether the links between the model and the phenomenon are strong enough.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>if the proponents of inductive risk are right, then deciding when we have reduced LU enough to understand can involve the consideration of non-epistemic values.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>It is my contention that there is also an epistemic risk in accepting that the model could explain or provide understanding of risk factors for patients with pneumonia.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Thus, accepting whether a particular model is adequate to explain or provide understanding involves the (implicit) weighing of these values.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Thus, ML models face link uncertainty risk. Judgments about when there is enough evidence connecting a model to its target, such that model opacity is not an epistemic barrier, can involve epistemic risk entangled with non-epistemic values.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Representational questions regarding what data should be used to represent the target phenomena, specific ML architectures suited for the problem, and even the specific interpretability technique chosen to gain high-level insight into black-box ML models require judgments that reflect values.  
  
<mark style="background-color: #e56eee">Quote</mark>  
>Since the LU between the original neural network model and the target was high, the opacity of the model created a greater epistemic barrier. Moreover, since the LU-risk was high, the need for more research into the external connection between the model and the target increases further. Therefore, if the extent to which model opacity undermines explanation and understanding is based on the degree to which there is an external connection between the model and the target, then the problem of opacity in ML is entangled with non-epistemic values, since the process of accepting whether there is sufficient connection between the model and its target is itself entangled with non-epistemic values.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>pragmatic encroachment theories of knowledge suggest that the stakes of a situation influence attributions of knowledge  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Specifically, depending on the stakes, someone might need to know more about how a given ML model works to understand.  
  
<mark style="background-color: #5fb236">Quote</mark>  
>It is not a settled question as to what constitutes understanding. Some argue that understanding is just a kind of knowledge (Riaz 2015), whereas others argue that understanding is distinct from knowledge (Hills 2016; Lawler 2019).  
  
<mark style="background-color: #ffd400">Quote</mark>  
>hen we can motivate a pragmatic encroachment view for when model opacity becomes a problem for understanding  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>There are two broad ways that non-epistemic values impact our attributions of understanding in the context of ML models: (i) the domain requires greater model transparency to attribute understanding, and (ii) the personal stakes in a given context can require greater model transparency to attribute understanding  
  
<mark style="background-color: #ff6666">Quote</mark>  
>However, in the case of a doctor recommender platform, given the importance that doctors have in someone’s well-being (it could be a matter of life or death), a user would need to answer more what-if-things-had-been-different questions about how the algorithm works in order to understand.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Because of the greater consequences of error, we need to be able to answer more what-if-things-had-been-different-questions regarding how the model works in order to attribute someone with understanding.  
  
<mark style="background-color: #ff6666">Quote</mark>  
>In other words, given how important it is for Zoe to find an apartment, and the higher risk of potential bias  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Instead, my aim was to argue that non-epistemic factors contribute to the question as to how much of a problem opacity really is.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>However, in order to explain and gain understanding with an ML model the problem of opacity greatly depends on features external to the model instead of features internal to it (i.e., link uncertainty and empirical support, explanatory functions, and the social and personal significance of the model and its domain).  
  
<mark style="background-color: #5fb236">Quote</mark>  
>Bokulich, Alisa. 2011. “How Scientific Models Can Explain.”  
  
<mark style="background-color: #5fb236">Quote</mark>  
>See Karaca (2021) for a discussion of type I and type II errors in ML and a proposal for costsensitive error classification to minimize inductive risk during model construction.  
  
<mark style="background-color: #5fb236">Quote</mark>  
>See Gilpin (2018) for a review of various ML interpretability methods.  
  
  
%% end annotations %%




%% Import Date: 2024-02-16T16:03:29.796+01:00 %%
