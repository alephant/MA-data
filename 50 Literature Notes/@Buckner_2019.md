---
category: literaturenote
tags: 
citekey: "Buckner_2019"
status: unread
dateread:
---

>[!Cite]
>Buckner, C. (2019). Deep learning: A philosophical introduction. _Philosophy Compass_, _14_(10), e12625. [https://doi.org/10.1111/phc3.12625](https://doi.org/10.1111/phc3.12625)

>[!Synth]
>**Contribution**::
>
>**Related**:: 

>[!Metadata]
> **FirstAuthor**:: Buckner, Cameron
~
>**Title**:: Deep learning: A philosophical introduction
>**Year**:: 2019
>**Citekey**:: Buckner_2019
>**itemType**:: journalArticle
>**Journal**:: *Philosophy Compass*
>**Volume**:: 14
>**Issue**:: 10
>**Pages**:: e12625 
>**DOI**:: 10.1111/phc3.12625

>[!LINK]
>
>[2019_Buckner_Deep_learning.pdf](file://C:\Users\a.niemeier\Zotero\storage\2RXEUYJP\2019_Buckner_Deep_learning.pdf).

>[!Abstract]
>
>Deep learning is currently the most prominent and widely successful method in artificial intelligence. Despite having played an active role in earlier artificial intelligence and neural network research, philosophers have been largely silent on this technology so far. This is remarkable, given that deep learning neural networks have blown past predicted upper limits on artificial intelligence performance—recognizing complex objects in natural photographs and defeating world champions in strategy games as complex as Go and chess—yet there remains no universally accepted explanation as to why they work so well. This article provides an introduction to these networks as well as an opinionated guidebook on the philosophical significance of their structure and achievements. It argues that deep learning neural networks differ importantly in their structure and mathematical properties from the shallower neural networks that were the subject of so much philosophical reflection in the 1980s and 1990s. The article then explores several different explanations for their success and ends by proposing three areas of inquiry that would benefit from future engagement by philosophers of mind and science.
>.
>
# Notes
# Annotations  
(20/12/2023, 12:11:51)

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=2&annotation=J8VASFYW) “deep learning networks are currently regarded as the best models of perceptual similarity judgments in primates (Yamins & DiCarlo, 2016)” ([Buckner, 2019, p. 2](zotero://select/library/items/E5DLIBYK))

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=8&annotation=97E9IXXT) “With these basic features of DCNNs in place, we can now ask: Why do these networks tend to work so well? Let us consider three popular explanations for DCNNs' distinctive successes. Though they are sometimes offered as competitors, it is not clear that they are in conflict and may illuminate complementary aspects of the same underlying phenomenon.” ([Buckner, 2019, p. 8](zotero://select/library/items/E5DLIBYK)) As key characteristics mirror biological functioning, DCNN might be good models for human cognition

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=8&annotation=744S96A4) “The division of labor between convolutional and pooling nodes in early DCNNs was inspired by Hubel and Wiesel's (1967) discovery of two different types of neuron in cat visual cortex” ([Buckner, 2019, p. 8](zotero://select/library/items/E5DLIBYK)) biological inspiration of hierarchical feature compostion in DCNN: "division of labour" between simple and complex cells in ventral stream

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=8&annotation=GMXFUY5Z) “This story was later extended with a variety of imaging methods to suggest a whole layered processing cascade of hierarchical abstraction in primate visual cortex (Goodale & Milner, 1992)” ([Buckner, 2019, p. 8](zotero://select/library/items/E5DLIBYK)) hierarchical processing in visual cortex

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=9&annotation=EBIA67K9) “Another interpretation of DCNNs' success holds their characteristic features to implement a set of unrevisable, domain-general assumptions that help networks control for common forms of variation in perceptual input. Consonant with the previous subsection, depth enforces the assumption that complex features are built from simpler ones. Passing the windows of kernels over the whole image at each layer applies the assumption that each feature can be reused many times in the composition of more complex ones and that features can occur anywhere in the image (e.g., as a hexagon may reuse the same angle feature six times in different locations).” ([Buckner, 2019, p. 9](zotero://select/library/items/E5DLIBYK)) DCNN follow domain-general assumptions to adjust for nuisance variation; possibly parallel to biological sensory systems as these must adjust for nuisance as well

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=10&annotation=7JACGT4R) “Though some may scoff at these “mere efficiency gains,” they would be decisive in evolutionary or behavioral competitions which required making complex decisions under limited time and resources.” ([Buckner, 2019, p. 10](zotero://select/library/items/E5DLIBYK)) DCNN learning process = reducing computational complexity by exploiting symmetries in feature space; evolutionary and behaviorally favourable

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=11&annotation=HIQTXRLX) “I have recently argued that current DCNNs already model a crucially important component of mammalian intelligence—a particular kind of abstraction—that until now eluded our grasp (Buckner, 2018)” ([Buckner, 2019, p. 11](zotero://select/library/items/E5DLIBYK))

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=11&annotation=M6FVFDAX) “These claims intersect with conversations from the Golden Age of connectionism, as philosophers appreciated early on that neural networks might provide a proof of concept that certain types of knowledge could be learned directly from sensory experience, without innate knowledge (c.f. Bechtel & Abrahamsen, 2002, pp. 54–57). Connectionist research was thus often associated with the banner of classical empiricism, and classical rule-and-symbol based methods with the banner of rationalism.” ([Buckner, 2019, p. 11](zotero://select/library/items/E5DLIBYK)) neural networks modelling the empiricist vs. rationalist view of learning and knowledge acquisition

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=11&annotation=Z8JD49BI) “neural network research could be rationalist in orientation, in the sense that innate knowledge could be modeled by presetting link weights in a way that biased networks to acquire particular information” ([Buckner, 2019, p. 11](zotero://select/library/items/E5DLIBYK)) neural networks modelling innate knowledge (as a cognitive capacity)

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=12&annotation=YJPVNL8T) “Some prominent deep learning researchers have also called for a return to innate biases in deep networks to help them master tasks like causal reasoning or linguistic processing” ([Buckner, 2019, p. 12](zotero://select/library/items/E5DLIBYK)) research question for AI researchers: how can DNN model cognition, e.g. causal reasoning or linguistic processing?

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=12&annotation=24LI64CB) “DCNNs may also distinctively address some long-standing mysteries in empiricist philosophy of mind regarding the role of abstraction in the acquisition of categorical knowledge, a faculty that has frequently been invoked by empiricists to account for learners' transition from basic sensory associations to the abstract representations deployed in higher cognition but has until recently not been adequately explained (c.f. Buckner, 2018; Laurence & Margolis, 2012).” ([Buckner, 2019, p. 12](zotero://select/library/items/E5DLIBYK))

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=12&annotation=KDBHZAPA) “First, critics have worried that the most successful DCNNs require far more training exemplars—especially supervised learning on large training sets, where the correct answers are labeled—than humans and animals require to learn the same information. Second, they have argued that the phenomenon of “adversarial examples” (a concept which may not be possible to define except by ostension, for reasons to be elaborated below) demonstrates that what is learned by DCNNs differs substantially from what is learned by humans and animals.” ([Buckner, 2019, p. 12](zotero://select/library/items/E5DLIBYK)) Reasons why DNNs are not a good model for human knowledge acquisition

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=13&annotation=SPL4M8HB) “this first kind of adversarial example has come to be called a “perturbed image.”” ([Buckner, 2019, p. 13](zotero://select/library/items/E5DLIBYK))

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=14&annotation=TGWYCHGJ) “Rather than being an easily overcome quirk of particular models or training sets, they appear to highlight a robust property of current DCNN methods.” ([Buckner, 2019, p. 14](zotero://select/library/items/E5DLIBYK))

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=14&annotation=D72T8ASN) “For modeling purposes, however, they might also show that despite categorizing naturally-occurring images as well or better than human adults, DCNNs do not really acquire the same kind of category knowledge that humans do—perhaps instead building “a Potemkin village that works well on naturally occurring data, but is exposed as fake when one visits points in space that do not have a high probability”” ([Buckner, 2019, p. 14](zotero://select/library/items/E5DLIBYK)) What a DNN models as knowledge is not the same as what a human acquires

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=14&annotation=USIWE5JX) “DCNNs may thus succeed in modeling human perceptual similarity judgments but not yet have the resources to draw a distinction between what something looks like and what it really is.” ([Buckner, 2019, p. 14](zotero://select/library/items/E5DLIBYK)) adversarial examples could in fact show how DNNs succeed as models for low-level aspects of human perceptual categorization

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=15&annotation=ZMPSJW9E) “This movement holds (roughly) that explanations in the life sciences work by locating structures and their organization in a target system whose coordinated operations regularly produce the phenomenon of interest.” ([Buckner, 2019, p. 15](zotero://select/library/items/E5DLIBYK)) the "new mechanistic movement" vindicating neural networks as mechnistic models for human cognition

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=15&annotation=8B8849WR) “DCNNs, however, are pitched at a high level of abstraction from perceptual cortex, which could lead one to doubt that they succeed at providing mechanistic explanations for even perceptual processing” ([Buckner, 2019, p. 15](zotero://select/library/items/E5DLIBYK)) DNNs are "too abstract" to be a good model for human cognition

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=15&annotation=HHK6VSSX) “In particular, some key aspects of the Hubel and Wiesel's story which originally inspired DCNNs have been challenged on neuroanatomical grounds (especially regarding the purported dichotomy between simple and complex cells on which the cooperation between convolution and pooling was based” ([Buckner, 2019, p. 15](zotero://select/library/items/E5DLIBYK)) Lack of empirical grounding, weak external link of DNN: their structure doesn't match neuroanatomical findings

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=15&annotation=KVRNL5SF) “there remains significant debate as to whether backpropagation learning is biologically plausible” ([Buckner, 2019, p. 15](zotero://select/library/items/E5DLIBYK)) Lack of biological plausibility of DNN

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=15&annotation=R7LYZSYQ) “These authors are likely to think that the attempt to force DCNN-based explanations into a mechanistic mold is a mistake, emphasizing instead the computational or mathematical principles which could be implemented in a variety of very different mechanisms.” ([Buckner, 2019, p. 15](zotero://select/library/items/E5DLIBYK)) Alternatives to mechanistic views of DNNs see DNNs as computational or mathematical models

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=17&annotation=S749YFXZ) “Buckner, C. (2018). Empiricism without magic: Transformational abstraction in deep convolutional neural networks. Synthese, 195(12), 5339–5372.” ([Buckner, 2019, p. 17](zotero://select/library/items/E5DLIBYK)) DCNN model crucial component of mammalian intelligence

[Go to annotation](zotero://open-pdf/library/items/2RXEUYJP?page=17&annotation=LHF5YDKT) “Glennan, S. (2017). The new mechanical philosophy. London: Oxford University Press.” ([Buckner, 2019, p. 17](zotero://select/library/items/E5DLIBYK)) vindicating neural networks as mechanistic models for human cognition.


# Annotations%% begin annotations %%  
  
  
  
### Imported: 2023-12-19 8:33 pm  
  
  
  
<mark style="background-color: #ffd400">Quote</mark>  
>deep learning networks are currently regarded as the best models of perceptual similarity judgments in primates (Yamins & DiCarlo, 2016)  
  
<mark style="background-color: #ffd400">Quote</mark>  
>I have recently argued that current DCNNs already model a crucially important component of mammalian intelligence—a particular kind of abstraction—that until now eluded our grasp (Buckner, 2018)  
  
<mark style="background-color: #ffd400">Quote</mark>  
>These claims intersect with conversations from the Golden Age of connectionism, as philosophers appreciated early on that neural networks might provide a proof of concept that certain types of knowledge could be learned directly from sensory experience, without innate knowledge (c.f. Bechtel & Abrahamsen, 2002, pp. 54–57). Connectionist research was thus often associated with the banner of classical empiricism, and classical rule-and-symbol based methods with the banner of rationalism.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>neural network research could be rationalist in orientation, in the sense that innate knowledge could be modeled by presetting link weights in a way that biased networks to acquire particular information  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Some prominent deep learning researchers have also called for a return to innate biases in deep networks to help them master tasks like causal reasoning or linguistic processing  
  
<mark style="background-color: #ffd400">Quote</mark>  
>First, critics have worried that the most successful DCNNs require far more training exemplars—especially supervised learning on large training sets, where the correct answers are labeled—than humans and animals require to learn the same information. Second, they have argued that the phenomenon of “adversarial examples” (a concept which may not be possible to define except by ostension, for reasons to be elaborated below) demonstrates that what is learned by DCNNs differs substantially from what is learned by humans and animals.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>this first kind of adversarial example has come to be called a “perturbed image.”  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Rather than being an easily overcome quirk of particular models or training sets, they appear to highlight a robust property of current DCNN methods.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>For modeling purposes, however, they might also show that despite categorizing naturally-occurring images as well or better than human adults, DCNNs do not really acquire the same kind of category knowledge that humans do—perhaps instead building “a Potemkin village that works well on naturally occurring data, but is exposed as fake when one visits points in space that do not have a high probability”  
  
<mark style="background-color: #ffd400">Quote</mark>  
>DCNNs may thus succeed in modeling human perceptual similarity judgments but not yet have the resources to draw a distinction between what something looks like and what it really is.  
  
<mark style="background-color: #5fb236">Quote</mark>  
>Buckner, C. (2018). Empiricism without magic: Transformational abstraction in deep convolutional neural networks. Synthese, 195(12), 5339–5372.  
  
  
### Imported: 2023-12-20 10:35 am  
  
  
  
<mark style="background-color: #ffd400">Quote</mark>  
>This movement holds (roughly) that explanations in the life sciences work by locating structures and their organization in a target system whose coordinated operations regularly produce the phenomenon of interest.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>DCNNs, however, are pitched at a high level of abstraction from perceptual cortex, which could lead one to doubt that they succeed at providing mechanistic explanations for even perceptual processing  
  
<mark style="background-color: #ffd400">Quote</mark>  
>In particular, some key aspects of the Hubel and Wiesel's story which originally inspired DCNNs have been challenged on neuroanatomical grounds (especially regarding the purported dichotomy between simple and complex cells on which the cooperation between convolution and pooling was based  
  
<mark style="background-color: #ffd400">Quote</mark>  
>there remains significant debate as to whether backpropagation learning is biologically plausible  
  
<mark style="background-color: #ffd400">Quote</mark>  
>These authors are likely to think that the attempt to force DCNN-based explanations into a mechanistic mold is a mistake, emphasizing instead the computational or mathematical principles which could be implemented in a variety of very different mechanisms.  
  
<mark style="background-color: #5fb236">Quote</mark>  
>Buckner, C. (2018). Empiricism without magic: Transformational abstraction in deep convolutional neural networks. Synthese, 195(12), 5339–5372.  
  
<mark style="background-color: #5fb236">Quote</mark>  
>Glennan, S. (2017). The new mechanical philosophy. London: Oxford University Press.  
  
  
### Imported: 2023-12-20 11:16 am  
  
  
  
<mark style="background-color: #ffd400">Quote</mark>  
>DCNNs may also distinctively address some long-standing mysteries in empiricist philosophy of mind regarding the role of abstraction in the acquisition of categorical knowledge, a faculty that has frequently been invoked by empiricists to account for learners' transition from basic sensory associations to the abstract representations deployed in higher cognition but has until recently not been adequately explained (c.f. Buckner, 2018; Laurence & Margolis, 2012).  
  
  
### Imported: 2023-12-20 12:12 pm  
  
  
  
<mark style="background-color: #ffd400">Quote</mark>  
>With these basic features of DCNNs in place, we can now ask: Why do these networks tend to work so well? Let us consider three popular explanations for DCNNs' distinctive successes. Though they are sometimes offered as competitors, it is not clear that they are in conflict and may illuminate complementary aspects of the same underlying phenomenon.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>The division of labor between convolutional and pooling nodes in early DCNNs was inspired by Hubel and Wiesel's (1967) discovery of two different types of neuron in cat visual cortex  
  
<mark style="background-color: #ffd400">Quote</mark>  
>This story was later extended with a variety of imaging methods to suggest a whole layered processing cascade of hierarchical abstraction in primate visual cortex (Goodale & Milner, 1992)  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Another interpretation of DCNNs' success holds their characteristic features to implement a set of unrevisable, domain-general assumptions that help networks control for common forms of variation in perceptual input. Consonant with the previous subsection, depth enforces the assumption that complex features are built from simpler ones. Passing the windows of kernels over the whole image at each layer applies the assumption that each feature can be reused many times in the composition of more complex ones and that features can occur anywhere in the image (e.g., as a hexagon may reuse the same angle feature six times in different locations).  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Though some may scoff at these “mere efficiency gains,” they would be decisive in evolutionary or behavioral competitions which required making complex decisions under limited time and resources.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>DCNNs, however, are pitched at a high level of abstraction from perceptual cortex, which could lead one to doubt that they succeed at providing mechanistic explanations for even perceptual processing  
  
  
%% end annotations %%




%% Import Date: 2023-12-20T12:12:10.916+01:00 %%
