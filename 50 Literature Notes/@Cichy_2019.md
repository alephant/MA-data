---
category: literaturenote
tags: 
citekey: "Cichy_2019"
status: unread
dateread:
---
There are several reasons why DNNs should not be dismissed as scientific models. As DNNs fulfil different desiderata as other models, they coexist with other useful scientific models in cognitive science. DNNs excel in non-theoretical desiderata. ==What are these?==
DNNs are useful for predicition, explanation and exploration.
Despite model opacity, DNNs have explanatory power: 
- DNN just provide different kind of explanation (intra-model focus like T&S?)  
- DNN are actually as transparent as simpler models  
- DNN will be made transparent to offer post-hoc explanations
The predictive power of DNNs can be a stepping stone towards explanation and can serve as a criterion for successful explanation. ==i.e. predictive power as a prerequisite for Sullivan's link?==
Example illustrating the difference in model characteristics between DNN and mathematical models used for explanation.
**ML complexity is not a bug but a feature**: complexity improves model adequacy for the target phen. and explorative power of the model.

**“Third, exploring models rather than the world bears the danger of mistaking the model for the world. Investigating DNNs might be mistaken for investigation of the human brain or behaviour. To avoid this, exploration must be accompanied by experimentation on the target phenomenon.”**
==Can model exploration as presented here be understood similar to understanding intra-model inferences by T&S? Can this be used to critique T&S?==


>[!Cite]
>Cichy, R. M., & Kaiser, D. (2019). Deep Neural Networks as Scientific Models. _Trends in Cognitive Sciences_, _23_(4), 305–317. [https://doi.org/10.1016/j.tics.2019.01.009](https://doi.org/10.1016/j.tics.2019.01.009)

>[!Synth]
>**Contribution**::
>
>**Related**:: 

>[!Metadata]
> **FirstAuthor**:: Cichy, Radoslaw M.
> **Author**:: Kaiser, Daniel
~
>**Title**:: Deep Neural Networks as Scientific Models
>**Year**:: 2019
>**Citekey**:: Cichy_2019
>**itemType**:: journalArticle
>**Journal**:: *Trends in Cognitive Sciences*
>**Volume**:: 23
>**Issue**:: 4
>**Pages**:: 305-317 
>**DOI**:: 10.1016/j.tics.2019.01.009

>[!LINK]
>
>[2019_Cichy_Kaiser_Deep_Neural_Networks_as_Scientific_Models.pdf](file://C:\Users\a.niemeier\Zotero\storage\HX9BQA29\2019_Cichy_Kaiser_Deep_Neural_Networks_as_Scientific_Models.pdf).

>[!Abstract]
>.
>
# Notes
# Annotations  
(09/02/2024, 12:20:47)

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=305&annotation=MR8765KR) “DNNs are computational models consisting of many simple processing units (akin to neurons) that work in parallel and are arranged in interconnected layers. Simple neural networks consist of an input layer and an output layer; when more layers are stacked, the networks are called deep [8,9]. A DNN learns to perform particular tasks through training, during which the strength of connections between units is learned. Subsequently, the trained DNN is used to perform the same task on novel inputs.” ([Cichy and Kaiser, 2019, p. 305](zotero://select/library/items/VUIMSKQE)) good working definition for a DNN

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=306&annotation=4UGRSSNY) “We claim that this is unlikely, as favoured properties of models in cognitive science (i.e., desiderata) have to be traded against each other [22], and no single model can fulfil all of them.” ([Cichy and Kaiser, 2019, p. 306](zotero://select/library/items/VUIMSKQE)) **<b>ML model usefulness**:</b>  
**different models fulfil different desiderata, DNN as part of a diverse set of models:**  
“DNNs fit into this as inhabiting one particular niche in the world of models in cognitive science. Assessing model use, we argue that DNNs hold promise for all three functions of models in science: for prediction, for explanation, and for exploration.” (Cichy and Kaiser, 2019, p. 308)

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=308&annotation=ZPDMT74X) “Transferred to the debate on DNNs, critique or praise about a DNN’s achievement can only be appreciated with respect to the desiderata pursued and what was intended to be modelled with what aim.” ([Cichy and Kaiser, 2019, p. 308](zotero://select/library/items/VUIMSKQE)) **ML model usefulness**:  
usefulness of DNN can only be judged in light of which desiderata they were intended to fulfil

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=308&annotation=RH4DYE52) “In scientific practice, non-theoretical desiderata, that is, practical considerations such as speed of computation, ease of manipulation, and ethical considerations, often take precedence over theoretical desiderata.” ([Cichy and Kaiser, 2019, p. 308](zotero://select/library/items/VUIMSKQE)) ?: what kind of desiderata are transparency or intelligibility?

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=308&annotation=I6EKM6FT) “DNNs fit into this as inhabiting one particular niche in the world of models in cognitive science. Assessing model use, we argue that DNNs hold promise for all three functions of models in science: for prediction, for explanation, and for exploration.” ([Cichy and Kaiser, 2019, p. 308](zotero://select/library/items/VUIMSKQE))

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=309&annotation=IPW274MG) “DNNs excel in these non-theoretical desiderata: they compute cheaply and swiftly, and their investigation has fewer ethical limits than many other models.” ([Cichy and Kaiser, 2019, p. 309](zotero://select/library/items/VUIMSKQE)) **DNN characteristics conc. model desiderata**:  
DNNs excel in non-theoretical desiderata

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=309&annotation=ASZ2ZFGY) “Given the need for many different optimal models (relative to a set of desiderata),” ([Cichy and Kaiser, 2019, p. 309](zotero://select/library/items/VUIMSKQE)) **question of model desiderata in relation to model use for explanation and understanding:**  
For a given scientific task, e.g. explaining and understanding a phenomenon, we can ask what set of desiderata does my model need to fulfil

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=309&annotation=T55FJPRA) “Instead, it argues for embracing the plurality and diversity of models” ([Cichy and Kaiser, 2019, p. 309](zotero://select/library/items/VUIMSKQE)) **ML model usefulness**:  
  
It is plausible and practical to use DNNs as an additional model in a diverse set of models

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=309&annotation=NH93HHYF) “However, this is no reason to dismiss the model, as the origin of DNNs plays no role for their scientific value.” ([Cichy and Kaiser, 2019, p. 309](zotero://select/library/items/VUIMSKQE)) **reason not to dismiss ML models**:  
model origin is irrelevant for scientific value  
  
ML models not built to test theories

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=309&annotation=9LVBMA5T) “In sum, putting DNNs as models in cognitive science into the broader context of scientific modelling in general, we made three observations: (i) all models of cognitive phenomena constitute trade-offs between desiderata, and also DNNs offer unique strengths and weaknesses; (ii) the set of models used in cognitive science is diverse, and DNNs are one particular, useful kind; and (iii) the origin of a model is irrelevant for its scientific value, and so are the DNNs’ origins.” ([Cichy and Kaiser, 2019, p. 309](zotero://select/library/items/VUIMSKQE)) **contextualize DNN as useful scientific models in cognitive science**

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=310&annotation=5BKFU745) “A pragmatic reason is that due to its predictive power a DNN could be used akin to a tool to reach practical aims without direct recurrence to explanation.” ([Cichy and Kaiser, 2019, p. 310](zotero://select/library/items/VUIMSKQE)) **example for ML model application** besides explanation

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=310&annotation=45DN43XD) “The scientific benefit is in predicting and thus substituting brain function; explanation and understanding are secondary” ([Cichy and Kaiser, 2019, p. 310](zotero://select/library/items/VUIMSKQE)) **cite** ML model application as "mimicking without understanding" as predicting, thus substituting brain function, not explaining and understanding brain function

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=310&annotation=HWI6K29X) “A second, more theoretical reason is that predictive power can be a stepping stone towards explanation.” ([Cichy and Kaiser, 2019, p. 310](zotero://select/library/items/VUIMSKQE))

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=310&annotation=3S3WD3I9) “Third, predictive power ultimately is a criterion for successful explanation” ([Cichy and Kaiser, 2019, p. 310](zotero://select/library/items/VUIMSKQE))

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=310&annotation=Y2UZGAT7) “The blueprint notion many researchers have in mind is so-called mathematical–theoretical modelling [43,44]” ([Cichy and Kaiser, 2019, p. 310](zotero://select/library/items/VUIMSKQE)) math. models as an example for a different modelling approach in cognitive science compared to DNN  
a priori setting of model variables increases transparency compared to DNN  
  
background knowledge of phen. to determine relevant variables

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=311&annotation=J7NUUJE2) “To make the distinction more concrete, consider research on visual representations in primates” ([Cichy and Kaiser, 2019, p. 311](zotero://select/library/items/VUIMSKQE)) **example for key characteristics of DNN models compared to other models used to explain**

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=311&annotation=D4YHPP88) “The Logical Form and Concept of Explanation” ([Cichy and Kaiser, 2019, p. 311](zotero://select/library/items/VUIMSKQE)) key notions of how models explain

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=312&annotation=I9GUBGTR) “Such an abstraction from fine-grained levels of description into single parameters at higher levels of description is common practice in the modelling of complex phenomena.” ([Cichy and Kaiser, 2019, p. 312](zotero://select/library/items/VUIMSKQE)) “Black box explanations are also commonplace in scientific inquiry. Many explanations in various domains obscure low level details to explain higher level causal mechanisms or non-causal dependencies.” (Sullivan, 2022, p. 7)

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=312&annotation=RZPTGYDQ) “Note also that complex (and thus immediately opaque) models might be inevitable for modelling complex” ([Cichy and Kaiser, 2019, p. 312](zotero://select/library/items/VUIMSKQE)) **ML complexity not a bug but a feature**: adequacy to the target phen

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=312&annotation=GP9WE3UF) “Models Are Not Neutral Tools” ([Cichy and Kaiser, 2019, p. 312](zotero://select/library/items/VUIMSKQE)) as models are not neutral tools DNN opacity is not an in principle problem for understanding  
model opacity is just an expression how different models enable and restrict different "ways of thinking", i.e. understanding?  
  
  
**model usefulness is contigent**:  
  
the criteria that DNN need to fulfil in order to be useful models is contigent upon what current DNN models can fulfil

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=313&annotation=IS3PM8Z6) “The goal must therefore be to reduce opaqueness, but not at the price of predictive power.” ([Cichy and Kaiser, 2019, p. 313](zotero://select/library/items/VUIMSKQE)) ?: reducing opacity as a goal?

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=313&annotation=BAHTFJ69) “Together, we presented three different perspectives from which DNNs have explanatory power: (i) they provide teleological explanations; (ii) despite their deceptive appearance, they provide the same explanations as traditional mathematical theoretical models; and (iii) owing to their complexity, they have strong potential for post hoc explanations.” ([Cichy and Kaiser, 2019, p. 313](zotero://select/library/items/VUIMSKQE)) **reasons why DNN can explain despite their opacity**:  
- DNN just provide different kind of explanation (intra-model focus like T&S?)  
- DNN are actually as transparent as simpler models  
- DNN will be made transparent to offer post-hoc explanations

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=313&annotation=IGA3WD8X) “Second, their complexity makes DNNs particularly well suited for exploration: there is lots to explore.” ([Cichy and Kaiser, 2019, p. 313](zotero://select/library/items/VUIMSKQE)) **ML complexity not a bug but a feature**: exploration

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=313&annotation=BZKGL6XP) “While positive and negative analogies are established relations between model and target, and thus offer no new insight, neutral analogies allow us to learn novel facts about the target.” ([Cichy and Kaiser, 2019, p. 313](zotero://select/library/items/VUIMSKQE)) **interesting connection between Hesse's view on model-target relation based on analogies and Sullivan's model-target link**:  
understanding whether or not model characteristics are shared or not with the target as a way to establish link and thus, allow for further understanding of phen.

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=314&annotation=KN524JLG) “This led to a hypothesis challenging current neural theories of object vision: inferior temporal (IT) cortex in primates (thought to represent object category across viewing conditions) may be representing such category-orthogonal properties, too. Analysis of electrophysiological data confirmed this prediction. Here, model exploration and subsequent empirical investigation led to an important refinement of the classical two-stream hypothesis of the visual brain [35].” ([Cichy and Kaiser, 2019, p. 314](zotero://select/library/items/VUIMSKQE)) example of successful use of DNN as exploratory tool

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=315&annotation=GMSZM2QE) “Third, exploring models rather than the world bears the danger of mistaking the model for the world. Investigating DNNs might be mistaken for investigation of the human brain or behaviour. To avoid this, exploration must be accompanied by experimentation on the target phenomenon.” ([Cichy and Kaiser, 2019, p. 315](zotero://select/library/items/VUIMSKQE)) **citeworthy for** **<b>Sullivan's critique of narrow view**</b>  
“Restricting ML models to the model-centric view of the TML hypothesis goes against the goals that many ML researchers themselves postulate, as well as keeping ML models apart from the rest of model-based science, which unnecessarily constrains our scientific toolbox.” (Sullivan, 2022, p. 4)

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=316&annotation=EMRSIN7X) “Cichy, R.M. et al. (2016) Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence” ([Cichy and Kaiser, 2019, p. 316](zotero://select/library/items/VUIMSKQE)) DNN modelling in cognitive science

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=316&annotation=2Z72VUM5) “22. Gelfert, A. (2016) Strategies and trade-offs in model-building. In How to Do Science with Models: A Philosophical Primer (Gelfert, A., ed.), pp. 43–70, Springer International Publishing” ([Cichy and Kaiser, 2019, p. 316](zotero://select/library/items/VUIMSKQE)) trade-off desiderata during model-building

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=316&annotation=ZAT9J9P9) “24. Morrison, M. and Morgan, M.S. (1999) Models as mediating instruments. In Models as Mediators: Perspectives on Natural and Social Science (Morgan, M.S. and Morrison, M., eds), pp. 10–37, Cambridge University Press” ([Cichy and Kaiser, 2019, p. 316](zotero://select/library/items/VUIMSKQE)) ML opacity is just an expression how different models enable and restrict different "ways of thinking", i.e. understanding?

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=316&annotation=C8XL6E8I) “Boon, M. and Knuuttila, T. (2009) Models as epistemic tools in engineering sciences: a pragmatic approach.” ([Cichy and Kaiser, 2019, p. 316](zotero://select/library/items/VUIMSKQE)) **contrast between model prediction and understanding from models**:  
in particular sciences models are used more as predictive tools than tools for understanding

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=316&annotation=3NWRVL23) “. Kay, K.N. (2017) Principles for models of neural information processing.” ([Cichy and Kaiser, 2019, p. 316](zotero://select/library/items/VUIMSKQE)) "one cannot explain one black box by another"

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=316&annotation=4IYLFTH3) “Riesenhuber, M. and Poggio, T. (1999) Hierarchical models of object recognition in cortex. Nat. Neurosci. 2, 1019–1025 47. Marr, D. (2010) Vision, MIT Press” ([Cichy and Kaiser, 2019, p. 316](zotero://select/library/items/VUIMSKQE)) examples for non-DNN modelling in cognitive science

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=316&annotation=4U5EPTT3) “Marblestone, A.H. et al. (2016) Toward an integration of deep learning and neuroscience.” ([Cichy and Kaiser, 2019, p. 316](zotero://select/library/items/VUIMSKQE)) DNN giving teleological explanations in neuroscience

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=316&annotation=ITSD5IFI) “Kietzmann, T.C. et al. (2017) Deep neural networks in computational neuroscience. bioRxiv” ([Cichy and Kaiser, 2019, p. 316](zotero://select/library/items/VUIMSKQE)) DNN modelling in cognitive science

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=316&annotation=2MCISZ8P) “51. Samek, W. et al. (2017) Explainable artificial intelligence: understanding, visualizing and interpreting deep learning models. arxiv. org/abs/1708.08296 52. Zhou, B. et al. (2015) Object detectors emerge in deep scene CNNs. Int. Conf. Learn. Represent. 2015 53. van der, Maaten L. and Hinton, G. (2008) Visualizing data using t-SNE. J. Mach. Learn. Res. 9, 2579–2605 54. Mahendran, A. and Vedaldi, A. (2014) Understanding deep image representations by inverting them. arxiv.org/abs/1412.0035 55. Zeiler, M.D. and Fergus, R. (2013) Visualizing and understanding convolutional networks. arxiv.org/abs/1311.2901 56. Yosinski, J. et al. (2015) Understanding neural networks through deep visualization. arxiv.org/abs/1506.06579” ([Cichy and Kaiser, 2019, p. 316](zotero://select/library/items/VUIMSKQE)) how to make DNN more transparent to enable explanation

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=317&annotation=SGXRXZHT) “57. Simonyan, K. et al. (2013) Deep inside convolutional networks: visualising image classification models and saliency maps. arxiv. org/abs/1312.6034 58. Mordvintsev, A. et al. (2015) Inceptionism: going deeper into neural networks. Google Res. Blog 59. Zhou, B. et al. Interpreting deep visual representations via network dissection. arxiv.org/abs/1711.05611 60. Girshick, R. et al. (2016) Region-based convolutional networks for accurate object detection and segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 38, 142–158 61. Xu, T. et al. (2018) Deeper interpretability of deep networks. arxiv. org/abs/1811.07807” ([Cichy and Kaiser, 2019, p. 317](zotero://select/library/items/VUIMSKQE)) how to make DNN more transparent to enable explanation

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=317&annotation=APZSBA33) “Kriegeskorte, N. (2015) Deep neural networks: a new framework for modeling biological vision and brain information processing. Annu. Rev. Vis. Sci. 1, 417–446” ([Cichy and Kaiser, 2019, p. 317](zotero://select/library/items/VUIMSKQE)) DNN modelling in cognitive science

[Go to annotation](zotero://open-pdf/library/items/HX9BQA29?page=317&annotation=5B3J2I8K) “Kriegeskorte, N. and Douglas, P.K. (2018) Cognitive computational neuroscience. Nat. Neurosci. 21, 1148–1160” ([Cichy and Kaiser, 2019, p. 317](zotero://select/library/items/VUIMSKQE)) DNN modelling in cognitive science.


# Annotations%% begin annotations %%  
  
  
  
### Imported: 2024-02-09 12:21 pm  
  
  
  
<mark style="background-color: #ffd400">Quote</mark>  
>DNNs are computational models consisting of many simple processing units (akin to neurons) that work in parallel and are arranged in interconnected layers. Simple neural networks consist of an input layer and an output layer; when more layers are stacked, the networks are called deep [8,9]. A DNN learns to perform particular tasks through training, during which the strength of connections between units is learned. Subsequently, the trained DNN is used to perform the same task on novel inputs.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>We claim that this is unlikely, as favoured properties of models in cognitive science (i.e., desiderata) have to be traded against each other [22], and no single model can fulfil all of them.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Transferred to the debate on DNNs, critique or praise about a DNN’s achievement can only be appreciated with respect to the desiderata pursued and what was intended to be modelled with what aim.  
  
<mark style="background-color: #e56eee">Quote</mark>  
>In scientific practice, non-theoretical desiderata, that is, practical considerations such as speed of computation, ease of manipulation, and ethical considerations, often take precedence over theoretical desiderata.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>DNNs fit into this as inhabiting one particular niche in the world of models in cognitive science. Assessing model use, we argue that DNNs hold promise for all three functions of models in science: for prediction, for explanation, and for exploration.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>DNNs excel in these non-theoretical desiderata: they compute cheaply and swiftly, and their investigation has fewer ethical limits than many other models.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Given the need for many different optimal models (relative to a set of desiderata),  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Instead, it argues for embracing the plurality and diversity of models  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>However, this is no reason to dismiss the model, as the origin of DNNs plays no role for their scientific value.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>In sum, putting DNNs as models in cognitive science into the broader context of scientific modelling in general, we made three observations: (i) all models of cognitive phenomena constitute trade-offs between desiderata, and also DNNs offer unique strengths and weaknesses; (ii) the set of models used in cognitive science is diverse, and DNNs are one particular, useful kind; and (iii) the origin of a model is irrelevant for its scientific value, and so are the DNNs’ origins.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>A pragmatic reason is that due to its predictive power a DNN could be used akin to a tool to reach practical aims without direct recurrence to explanation.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>The scientific benefit is in predicting and thus substituting brain function; explanation and understanding are secondary  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>A second, more theoretical reason is that predictive power can be a stepping stone towards explanation.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Third, predictive power ultimately is a criterion for successful explanation  
  
<mark style="background-color: #ffd400">Quote</mark>  
>The blueprint notion many researchers have in mind is so-called mathematical–theoretical modelling [43,44]  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>To make the distinction more concrete, consider research on visual representations in primates  
  
<mark style="background-color: #ffd400">Quote</mark>  
>The Logical Form and Concept of Explanation  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Such an abstraction from fine-grained levels of description into single parameters at higher levels of description is common practice in the modelling of complex phenomena.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Note also that complex (and thus immediately opaque) models might be inevitable for modelling complex  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Models Are Not Neutral Tools  
  
<mark style="background-color: #e56eee">Quote</mark>  
>The goal must therefore be to reduce opaqueness, but not at the price of predictive power.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Together, we presented three different perspectives from which DNNs have explanatory power: (i) they provide teleological explanations; (ii) despite their deceptive appearance, they provide the same explanations as traditional mathematical theoretical models; and (iii) owing to their complexity, they have strong potential for post hoc explanations.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Second, their complexity makes DNNs particularly well suited for exploration: there is lots to explore.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>While positive and negative analogies are established relations between model and target, and thus offer no new insight, neutral analogies allow us to learn novel facts about the target.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>This led to a hypothesis challenging current neural theories of object vision: inferior temporal (IT) cortex in primates (thought to represent object category across viewing conditions) may be representing such category-orthogonal properties, too. Analysis of electrophysiological data confirmed this prediction. Here, model exploration and subsequent empirical investigation led to an important refinement of the classical two-stream hypothesis of the visual brain [35].  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Third, exploring models rather than the world bears the danger of mistaking the model for the world. Investigating DNNs might be mistaken for investigation of the human brain or behaviour. To avoid this, exploration must be accompanied by experimentation on the target phenomenon.  
  
<mark style="background-color: #5fb236">Quote</mark>  
>Cichy, R.M. et al. (2016) Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence  
  
<mark style="background-color: #5fb236">Quote</mark>  
>22. Gelfert, A. (2016) Strategies and trade-offs in model-building. In How to Do Science with Models: A Philosophical Primer (Gelfert, A., ed.), pp. 43–70, Springer International Publishing  
  
<mark style="background-color: #5fb236">Quote</mark>  
>24. Morrison, M. and Morgan, M.S. (1999) Models as mediating instruments. In Models as Mediators: Perspectives on Natural and Social Science (Morgan, M.S. and Morrison, M., eds), pp. 10–37, Cambridge University Press  
  
<mark style="background-color: #5fb236">Quote</mark>  
>Boon, M. and Knuuttila, T. (2009) Models as epistemic tools in engineering sciences: a pragmatic approach.  
  
<mark style="background-color: #5fb236">Quote</mark>  
>. Kay, K.N. (2017) Principles for models of neural information processing.  
  
<mark style="background-color: #5fb236">Quote</mark>  
>Riesenhuber, M. and Poggio, T. (1999) Hierarchical models of object recognition in cortex. Nat. Neurosci. 2, 1019–1025 47. Marr, D. (2010) Vision, MIT Press  
  
<mark style="background-color: #5fb236">Quote</mark>  
>Marblestone, A.H. et al. (2016) Toward an integration of deep learning and neuroscience.  
  
<mark style="background-color: #5fb236">Quote</mark>  
>Kietzmann, T.C. et al. (2017) Deep neural networks in computational neuroscience. bioRxiv  
  
<mark style="background-color: #5fb236">Quote</mark>  
>51. Samek, W. et al. (2017) Explainable artificial intelligence: understanding, visualizing and interpreting deep learning models. arxiv. org/abs/1708.08296 52. Zhou, B. et al. (2015) Object detectors emerge in deep scene CNNs. Int. Conf. Learn. Represent. 2015 53. van der, Maaten L. and Hinton, G. (2008) Visualizing data using t-SNE. J. Mach. Learn. Res. 9, 2579–2605 54. Mahendran, A. and Vedaldi, A. (2014) Understanding deep image representations by inverting them. arxiv.org/abs/1412.0035 55. Zeiler, M.D. and Fergus, R. (2013) Visualizing and understanding convolutional networks. arxiv.org/abs/1311.2901 56. Yosinski, J. et al. (2015) Understanding neural networks through deep visualization. arxiv.org/abs/1506.06579  
  
<mark style="background-color: #5fb236">Quote</mark>  
>57. Simonyan, K. et al. (2013) Deep inside convolutional networks: visualising image classification models and saliency maps. arxiv. org/abs/1312.6034 58. Mordvintsev, A. et al. (2015) Inceptionism: going deeper into neural networks. Google Res. Blog 59. Zhou, B. et al. Interpreting deep visual representations via network dissection. arxiv.org/abs/1711.05611 60. Girshick, R. et al. (2016) Region-based convolutional networks for accurate object detection and segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 38, 142–158 61. Xu, T. et al. (2018) Deeper interpretability of deep networks. arxiv. org/abs/1811.07807  
  
<mark style="background-color: #5fb236">Quote</mark>  
>Kriegeskorte, N. (2015) Deep neural networks: a new framework for modeling biological vision and brain information processing. Annu. Rev. Vis. Sci. 1, 417–446  
  
<mark style="background-color: #5fb236">Quote</mark>  
>Kriegeskorte, N. and Douglas, P.K. (2018) Cognitive computational neuroscience. Nat. Neurosci. 21, 1148–1160  
  
  
%% end annotations %%




%% Import Date: 2024-02-09T12:21:10.703+01:00 %%
