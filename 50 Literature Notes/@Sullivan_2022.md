---
category: literaturenote
tags: 
citekey: "Sullivan_2022"
status: unread
dateread:
---

>[!Cite]
>Sullivan, E. (2022). Understanding from Machine Learning Models. _The British Journal for the Philosophy of Science_, _73_(1), 109–133. [https://doi.org/10.1093/bjps/axz035](https://doi.org/10.1093/bjps/axz035)

>[!Synth]
>**Contribution**::
>
>**Related**:: 

>[!Metadata]
> **FirstAuthor**:: Sullivan, Emily
~
>**Title**:: Understanding from Machine Learning Models
>**Year**:: 2022
>**Citekey**:: Sullivan_2022
>**itemType**:: journalArticle
>**Journal**:: *The British Journal for the Philosophy of Science*
>**Volume**:: 73
>**Issue**:: 1
>**Pages**:: 109-133 
>**DOI**:: 10.1093/bjps/axz035

>[!LINK]
>
>[2022_Sullivan_Understanding_from_Machine_Learning_Models.pdf](file://C:\Users\a.niemeier\Zotero\storage\JF9MCQ3U\2022_Sullivan_Understanding_from_Machine_Learning_Models.pdf).

>[!Abstract]
>
>Simple idealized models seem to provide more understanding than opaque, complex, and hyper-realistic models. However, an increasing number of scientists are going in the opposite direction by utilizing opaque machine learning models to make predictions and draw inferences, suggesting that scientists are opting for models that have less potential for understanding. Are scientists trading understanding for some other epistemic or pragmatic good when they choose a machine learning model? Or are the assumptions behind why minimal models provide understanding misguided? In this article, using the case of deep neural networks, I argue that it is not the complexity or black box nature of a model that limits how much understanding the model provides. Instead, it is a lack of scientific and empirical evidence supporting the link that connects a model to the target phenomenon that primarily prohibits understanding.
>.
>
# Notes
# Annotations  
(19/01/2024, 18:50:18)

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=1&annotation=LEZ9CYN5) “Are scientists trading understanding for some other epistemic or pragmatic good when they choose a machine learning model? Or are the assumptions behind why minimal models provide understanding misguided?” ([Sullivan, 2022, p. 1](zotero://select/library/items/GQGEYW5N)) Sullivan's research question: scientist use models to gain understanding; complex models seem to provide less understanding; model complexity and understanding seem to be connected; is this true? is there less potential for understanding when using a ML model? if it is not model complexity that decides potential for understanding, what is it then?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=2&annotation=D5AQ478Q) “It is able to predict a wide array of medical problems, such as schizophrenia, attention-deficit disorder, and severe diabetes, with a higher degree of accuracy than competing predictive models. However, with this increased accuracy comes increased opacity.” ([Sullivan, 2022, p. 2](zotero://select/library/items/GQGEYW5N)) introduce usefulness of ML models "ML models have more predictive power than other models"  
introduce source of opacity "with increased complexity comes increased opacity"

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=2&annotation=YWIYYNVV) “DNNs are opaque to modelers, they are increasingly complex and have less modeler control, and the amount of w-questions they address are seemingly limited. Are scientists trading understanding for some other epistemic or pragmatic good when they choose an opaque and complex machine learning model?” ([Sullivan, 2022, p. 2](zotero://select/library/items/GQGEYW5N)) introduce ithat limited transparency and simplicity of ML models limit understanding

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=2&annotation=JE6FNEW8) “I argue that model simplicity and transparency are not needed for understanding phenomena.” ([Sullivan, 2022, p. 2](zotero://select/library/items/GQGEYW5N)) introduce argument part 1 "transparency and simplicity are not relevant for understanding"

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=2&annotation=FGXRX2I3) “Instead, it is the level of link uncertainty present—that is, the extent to which the model fails to be empirically supported and adequately linked to the target phenomena—that prohibits understanding.” ([Sullivan, 2022, p. 2](zotero://select/library/items/GQGEYW5N)) introduce argument part 2 "link uncertainty is relevant for understanding"

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=3&annotation=HZ6K92KZ) “principle way” ([Sullivan, 2022, p. 3](zotero://select/library/items/GQGEYW5N)) ?: is there another way for black boxing?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=3&annotation=KC6YBJ8B) “Algorithms on their own are not explanations. It is only when algorithmic models are used to answer a question about some event or phenomenon that they explain.” ([Sullivan, 2022, p. 3](zotero://select/library/items/GQGEYW5N)) introduce difference between algorithmic models that do and do not explain "algorithmic models do explaining only when they are used to answer questions about real-world phenomena"  
point of attack "lack of clarification between explanation and understanding" ??

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=3&annotation=LK4RPEG8) “I will adopt the increasingly common view that explanation aims at understanding (De Regt [2017]; Grimm [2010]; Khalifa [2017]; Potochnik [2011], [2015]; Strevens [2008]). In a slogan: explaining why helps us to understand why.” ([Sullivan, 2022, p. 3](zotero://select/library/items/GQGEYW5N)) common view "explanation aims at understanding"  
connecting algorithmic model explanation with understanding "algorithmic models that are used to answer questions about real-world phenomena, ie. that do explaining are able to provide understanding"

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=3&annotation=U9NIF42U) “explanations that utilize complex and opaque models are unable to provide understanding of phenomena” ([Sullivan, 2022, p. 3](zotero://select/library/items/GQGEYW5N)) clarify research question in light of relation between explanation and understanding "are opaque and complex ML models that are used to do explaining unable to provide understanding, because they are opaque and complex - or because of something else?"  
It seems like for Sullivan "understanding" means "understanding real-world phenomena" and "ML models that do explaining" means "answering questions about real-world phenomena" Thus, explaining and understanding both directly refer to a real-world phenomenon  
**"ML models used to explain" = "understanding from ML models"**

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=4&annotation=L4435529) “My arguments do not so much trade on any positive notion of what understanding or explanation is, but on whether, given a lack of information, it is still possible to gain insight about a phenomenon.” ([Sullivan, 2022, p. 4](zotero://select/library/items/GQGEYW5N)) clarify argumentation "I do not say anything about what model explanation and understanding from ML models is - all I say is whether understanding is possible even though ML models are opaque"  
**point of attack** "is it possible to argue for the possibility of understanding from ML models without saying anything about what this understanding is?"

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=4&annotation=HP52D2IB) “I am concerned with what Humphreys ([2004], [2009]) calls epistemic opacity: the extent to which the process of the model and its derived output are inaccessible to scientists and modelers.” ([Sullivan, 2022, p. 4](zotero://select/library/items/GQGEYW5N)) clarify definition "epistemic opacity"

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=4&annotation=772DPIHH) “Before considering the complex case of DNNs, it is worthwhile to first consider a simple case to illustrate the way that models explain and provide understanding of phenomenon” ([Sullivan, 2022, p. 4](zotero://select/library/items/GQGEYW5N)) introduce simple models to better understand complex models

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=4&annotation=CFK36BAR) “He created a model that aims to simulate a neighbourhood where individuals act on simple preferences in order to see the conditions under which segregation occurs.” ([Sullivan, 2022, p. 4](zotero://select/library/items/GQGEYW5N)) exemplify simple model

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=4&annotation=MPP7TM3L) “The use of explanation in machine learning often corresponds to justification, such as explaining how a model operates in order to justify a decision. In this paper, I am using explanation in a broader sense, in terms of explaining a target phenomenon.” ([Sullivan, 2022, p. 4](zotero://select/library/items/GQGEYW5N)) clarify what she means by "explanation" by introducing difference between "explanation in a narrow sense related to explaining intra-model operations used to justify model use" (intra-model sense) and "explanation in a broader sense related to models that do explaining aimed at understanding the target phenomenon" (extra-model sense)

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=5&annotation=76P2KLW6) “There is a distinction between understanding and explaining how the model works and using that model to understand a phenomenon of interest. If one is chiefly concerned about explaining or understanding how a given model is implemented, it is not necessary to know how the model maps on to some real-world phenomenon.” ([Sullivan, 2022, p. 5](zotero://select/library/items/GQGEYW5N)) clarify her case by introducing a distinction between "narrow and broad sense of understanding":  
if you are mainly interested to understand how the model works, you don't need to understand how the model maps onto the phenomenon   
  
however, from that it does not follow for Sullivan that if you are mainly interested to understand how the model maps onto a phenomenon, you do not need understanding of model implementation  
  
She asks the question how much understanding of model implementation is needed to understand the phenomenon  
  
  
**point of attack**: is it true that understanding how the model maps onto the phenomenon is irrelevant for understanding model implementation?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=5&annotation=XTZMXI4S) “The question of this paper, on the other hand, is to what extent understanding the model is necessary for gaining understanding of the phenomenon that the model explains.” ([Sullivan, 2022, p. 5](zotero://select/library/items/GQGEYW5N)) research question in light of "narrow vs broad sense of understanding": "how much do I need to understand the model in order to understand the phenomenon?" (degree of model transparency needed)  
  
"are opaque and complex ML models that do explaining unable to provide understanding, because they are opaque and complex - or because of something else?" (model transparency in principle needed)  
  
**point of attack**: Sullivan claims that "narrow vs broad sense of understanding" in relation to ML models is distinct - but how distinct are they really?  
  
  
**all below Sullivan argues that model opacity should not be the sole focus when deciding whether understanding from ML models is prevented or not and she introduces link uncertainty as a critical factor whether model opacity prevents understanding or not**

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=5&annotation=YRI9HBMJ) “understanding possibilities surrounding a real-world phenomenon” ([Sullivan, 2022, p. 5](zotero://select/library/items/GQGEYW5N)) ?: what is " understanding possibilities surrounding real-world phenomenon" vs "understanding the real-world phenomenon" as stated above?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=5&annotation=YDFJ57KZ) “In order to explain how it is possible segregation could occur, the explanation must include how the algorithmic model simulates a possible population that could be affected by segregation by identifying the key mechanism behind the algorithm and how it maps onto a possible population.” ([Sullivan, 2022, p. 5](zotero://select/library/items/GQGEYW5N)) clarify premise(?) "understanding the phenomenon A (segregation) needs understanding of the model and how it maps onto real-world phenomenon B (population)" based on simple model example  
  
? confusion: the phenomena are not the same? what is the difference between the phenomenon that wants to be understood (segregation) vs the phenomenon the model maps onto (population)?

([Sullivan, 2022, p. 5](zotero://select/library/items/GQGEYW5N)) In the context of the philosophy of science and the discussion on models, as presented in the text, "mapping" refers to the process of correlating elements of a model with elements of a real-world phenomenon. This process is crucial for the effectiveness of the model in explaining or understanding the phenomenon in question.  
  
To break it down further:  
  
1. **Model Elements and Real-World Equivalents**: In the text, the model in question uses coins to represent people of different races, and empty spaces to represent move-in ready houses. This is a direct example of mapping, where components of the model (coins, spaces) are assigned to correspond to specific elements of the real world (people, houses).  
  
2. **Capturing Key Dynamics**: The model includes a mechanism where the catalyst for movement is a preference for neighbors of the same race. This part of the model maps onto real-world social behaviors and preferences, attempting to simulate how these preferences might play out in the context of neighborhood segregation.  
  
3. **Purpose of Mapping**: The ultimate goal of this mapping is to offer a "how-possibly explanation" for a phenomenon—in this case, neighborhood segregation. By establishing a clear correspondence between the model and real-world elements, the model aims to identify possible causal mechanisms that could lead to segregation.  
  
4. **Understanding and Explaining Phenomena**: Through this process, the model does not just represent the phenomenon superficially, but aims to provide insights into how it could possibly occur in the real world. This understanding is essential for developing explanations that are not just descriptive but also predictive and explanatory in nature.  
  
5. **Philosophical Context**: In the philosophy of science, such models are considered valuable for their ability to isolate and test key variables and mechanisms in a controlled, simplified environment. This helps in understanding complex real-world phenomena by focusing on essential aspects and testing hypotheses about causal relationships.  
  
In summary, "mapping" in this context is about creating a systematic correspondence between elements of a model and aspects of a real-world phenomenon, with the aim of gaining a deeper understanding of the underlying mechanisms and causal relationships of that phenomenon.

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=5&annotation=JEM33F2B) “This mapping allows us to interpret the results of the simulation as identifying a possible causal mechanism of segregation.” ([Sullivan, 2022, p. 5](zotero://select/library/items/GQGEYW5N)) critique and ?: what does "mapping" mean?  
what does it mean that "it maps onto a possible population"?  
to what does the algorithmic model (its "key features") map onto?  
  
to the modelled "causes and dependencies operating in the target phenomenon"?  
to the target itself (segregation)?  
  
is the mapping getting empirically validated?  
is the connection between identified causes and actual causes empirically validated?  
  
"empirically validated" means "connecting identified causes to actual causes  
"validating the model-target mapping" means empirically validate the possible causes identified

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=5&annotation=53VSZWRX) “how the algorithm simulates a real population, that is, how the key feature” ([Sullivan, 2022, p. 5](zotero://select/library/items/GQGEYW5N)) ?: "how the model works" and "how the model maps" are conflated? or is it just the same?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=6&annotation=2ICU2GTZ) “Furthermore, the explanation needs to include empirical justification of the claim that individual racial preferences is a salient mechanism that drives real world population moving patterns (Mäki [2009]; Sullivan and Khalifa [2019]).” ([Sullivan, 2022, p. 6](zotero://select/library/items/GQGEYW5N)) clarify empirical justification: as **additional** criteria that how-actually explanations need to satisfy over how-possibly explanations  
  
**however, the explanation employing the model still needs to include details on model implementation, i.e. algorithmic mapping ??**

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=6&annotation=RVXPAS9I) “Without empirical evidence validating that the possible causes identified by Shelling’s model are actual causes, there is no link connecting the model to the phenomenon.” ([Sullivan, 2022, p. 6](zotero://select/library/items/GQGEYW5N)) introduce idea of link connecting the model to the phenomenon  
  
introduce distinction link vs no link

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=6&annotation=8TUPVVFZ) “There is a high level of link uncertainty, that is, a lack of scientific and empirical evidence supporting the link that connects the model to the target phenomenon.” ([Sullivan, 2022, p. 6](zotero://select/library/items/GQGEYW5N)) **point of attack** lack of selectivity: right before lack of validating that the modeled causes are the actual causes means "no link", now there is a link, but it is not supported by evidence, i.e. the exiting link has a high level of uncertainty

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=6&annotation=ACK3W27I) “Indeed, if we suppose instead that the empirical evidence suggested that all segregation is the result of institutional racism or individual racial prejudice” ([Sullivan, 2022, p. 6](zotero://select/library/items/GQGEYW5N)) ?: how can we know what empirical evidence suggests without looking at it from a theory or model?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=6&annotation=UVCJ3KQB) “so that Schelling’s mechanisms for segregation were never realized in any real-world system, then we would have no reason to think that Schelling’s model uncovers any actual causes of segregation, and thus would not be able to explain segregation or enable any real understanding” ([Sullivan, 2022, p. 6](zotero://select/library/items/GQGEYW5N)) ?: because there would be nothing to map to for Schelling's model as the possible causes identified by the model (individual preference) would be non-existent causes operating in the target phenomenon (segregation)?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=6&annotation=IZTDYDS4) “Thus, Schelling’s model only provides understanding in so far as there is there is a link connecting the model to the phenomenon.” ([Sullivan, 2022, p. 6](zotero://select/library/items/GQGEYW5N)) clarify the idea "link": link is when the possible causes identified by the model are empirically validated as the actual causes  
  
?: what is the state of the link before the possible causes get validated as the actual ones then?  
when possible causes are not validated, no real understanding is possible  
when possible causes are validated, real understanding is possible  
  
?: is that true, that a model whose identified causes are not validated, cannot provide understanding?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=6&annotation=SFMRZE7J) “The way of establishing the necessary link” ([Sullivan, 2022, p. 6](zotero://select/library/items/GQGEYW5N)) critique: it needs to say "the way of reducing link uncertainty" or "the way of supporting the established, but weak link"

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=6&annotation=45LMEYQW) “supports the connection between the causes or dependencies that the model uncovers to those causes or dependencies operating in the target phenomenon.” ([Sullivan, 2022, p. 6](zotero://select/library/items/GQGEYW5N)) ?: link is between model-causes and real causes?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=6&annotation=ZJG8TLUH) “to reduce link uncertainty” ([Sullivan, 2022, p. 6](zotero://select/library/items/GQGEYW5N)) critique: here, she switches back to degrees of uncertainty of an established link

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=6&annotation=5BF6832E) “empirical evidence connecting individual preferences to causes of segregation” ([Sullivan, 2022, p. 6](zotero://select/library/items/GQGEYW5N)) clarify Schelling "individual preferences" = possible cause surrounding segregation whose connection to actual causes surrounding segregation are validated by empirical evidence

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=6&annotation=PC2UD95U) “Importantly, establishing the necessary link connecting the phenomenon to the model does not thereby replace the need for or the epistemic value of the model.” ([Sullivan, 2022, p. 6](zotero://select/library/items/GQGEYW5N)) ?: why does she bring up epistemic value of model? relation to her model-induced explanation?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=6&annotation=2FU8KZRW) “The model still explains even once the link between the model and the phenomenon is no longer uncertain.” ([Sullivan, 2022, p. 6](zotero://select/library/items/GQGEYW5N)) ?: what happens when new evidence contradicts previous evidence?  
  
when a possible cause was already identified as an actual cause through empirical evidence, thus, us gaining "real" understanding the real-world phenomenon and its surrounding causes - does this not imply that our understanding being actual understanding is always relative to the evidence that is being produced?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=7&annotation=U25HLPAR) “when the link uncertainty is removed” ([Sullivan, 2022, p. 7](zotero://select/library/items/GQGEYW5N)) critique "link uncertainty" smells of degrees in uncertainty; now uncertainty is being "removed" and not reduced  
?: is it even possible within Sullivan's own framework for link uncertainty to be completely "removed"? (tying to the question of what happens when new evidence contradicts previously empirically validating evidence)

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=7&annotation=46YTEGS2) “The focus should not be unduly placed on how the model works, but instead consider the explanatory question we ask of the model, the role that the algorithm or model plays in the explanation, and the amount, quality, and kind of scientific evidence needed in order to connect the model to the target phenomenon.” ([Sullivan, 2022, p. 7](zotero://select/library/items/GQGEYW5N)) clarify why focusing solely at model opacity, i.e. understanding how the model works, is unfit, when we look at ML models and how to use them to understand real-world phenomena

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=7&annotation=NCPA7RZ7) “Black box explanation” ([Sullivan, 2022, p. 7](zotero://select/library/items/GQGEYW5N)) introduce the general idea of "black box explanation", ie. explanations through which one gains understanding of phenomena without knowing all the details  
  
?: details of what? the model that gives the explanation? or the phenomenon that needs explaining?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=8&annotation=UV8FWDAV) “the implementation is either unknown or illegible to the modeler, the explainer, or the understander.” ([Sullivan, 2022, p. 8](zotero://select/library/items/GQGEYW5N)) introduce idea of "black boxed implementation" relating it to lack of intelligibility

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=8&annotation=3N2F8Y5Y) “There are different ways one could implement factorials in a computer system” ([Sullivan, 2022, p. 8](zotero://select/library/items/GQGEYW5N)) introduce idea of "implementation irrelevance" by examples on algorithmic implementation:  
  
the exact implementation is irrelevant as long as the input-output relation (ie. higher-level emergent properties?) remains the same  
insight into the exact implementation is irrelevant for understanding the phenomenon, as long as the higher-level algorithmic patterns identifying possible causes stay the same

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=8&annotation=FN6TFCC8) “If a climate model involves computing factorials, it is unnecessary for the modeler to know, or make explicit in explaining climate patterns, how exactly the factorials were implemented. Details regarding the implementation are unnecessary for explaining and understanding why a particular climate pattern emerged.” ([Sullivan, 2022, p. 8](zotero://select/library/items/GQGEYW5N)) conditions for understanding higher-level causal mechanisms from models does not include detailed knowledge of implementation

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=9&annotation=P3CP3PEF) “There are countless implementations of Schelling’s model that follow the same basic higher-level algorithm regarding satisfying neighbourhood preferences.” ([Sullivan, 2022, p. 9](zotero://select/library/items/GQGEYW5N)) "The explanation does not rely on the specific movements of, say, coin-267, but on the macrolevel emergence of a segregated pattern. This is true even if the algorithm is indeterminate, such that it is not even possible in theory to predict the resulting segregated pattern because the algorithm involves random choice of which actors to move next."  
Whether or not the model is able to explain or not depends on the "macrolevel emergence of a segregated pattern", ie. the model needs to follow "the same basic higher-level algorithm regarding satisfying neighbourhood preferences", ie. the high-level algorithm needs to stay intact, ie. segregation needs to occur in whichever unpredictable form whatsoever

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=9&annotation=DQY4EP3N) “even though these implementation differences make a difference to how the computer system operates and executes the algorithm” ([Sullivan, 2022, p. 9](zotero://select/library/items/GQGEYW5N)) as far as I understand, it is not necessary to know the implementation in the specific case of the Schelling model and what it was specifically asked to explain (segregation); here, implementation is irrelevant for understanding segregation  
but for other phenomena and for other explanatory questions that are being asked the model, implementation is relevant for understanding  
  
  
?: is knowing "how the computer system operates and executes the algorithm" not the same as knowing "how the algorithmic model simulates a possible population that could be affected by segregation by identifying the key mechanism behind the algorithm and how it maps onto a possible population" ??  
?: doesn't this mean that the model mapping stays the same even though the model implementation changes?  
?: but how can we be sure that the "the key features of the algorithm that map on to real-world populations" stay the same? or don't they stay the same when the implementation changes?  
?: how can we be sure that the mapping by another implementation method in fact leads to the exact same possible causes identified by the model that are then validated as the exact same higher-level causes?  
?: but how can insight into implementation be irrelevant when it is necessary for a model to explain a phenomenon that the explanation includes how the model works and how it maps onto the real-world?  
  
?: or does insight into implementation concerns something else than knowing "how the algorithmic model simulates a possible population that could be affected by segregation by identifying the key mechanism behind the algorithm and how it maps onto a possible population"??

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=9&annotation=IC4YSR9Y) “Thus, implementation black boxing in itself does not undermine our ability to explain or understand phenomena.” ([Sullivan, 2022, p. 9](zotero://select/library/items/GQGEYW5N)) intermediate conclusion: while it is possible for implementation black boxing to impede understanding a phenomenon, it is not in principle relevant for understanding a phenomenon  
  
i.e. model opacity is not in principle relevant  
i.e. intelligibility of model implementation is not in principle relevant

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=9&annotation=SN4QQHEF) “If the low-level implementation details made a difference to the high level results of Schelling’s model, then the implementation would matter for explaining and understanding segregation.” ([Sullivan, 2022, p. 9](zotero://select/library/items/GQGEYW5N)) ?: what are the low-level implementation details of Schelling's model? what are the high-level results?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=9&annotation=7A87ITYS) “if the explanatory question concerns implementation or why building a model in a particular way is preferable” ([Sullivan, 2022, p. 9](zotero://select/library/items/GQGEYW5N)) expand model-focused explanatory question:  
  
if the model becomes the phenomenon to be explained as model-as-target then implementation becomes relevant

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=10&annotation=8AAFL4LF) “3.2 Levels of implementation black boxes” ([Sullivan, 2022, p. 10](zotero://select/library/items/GQGEYW5N)) investigate the impact of model opacity on the possibility for understanding by introducing concepts of "depth of black box" and "different types of implementation black boxes" and looking at how they impact the possibility for understanding for algorithmic and simple models:  
  
"different types of implementation black boxes":  
1. basic implementation black boxing in function calls (e.g. sig() )  
2. impl. black boxing by in principle unpredictable algorithms  
3. impl. black boxing by regularization methods to make model generalizable (see below)  
  
  
"depth of black box":  
whether or not a model is a highest-level black box depends on various factors  
if it is not a highest-level black box it is compatible with understanding  
  
if it is a highest-level black box it is very plausible that it is not compatible with understanding

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=10&annotation=3GBF7TT4) “In the iterative code example in figure 1, the fact-iter algorithm is not black boxed, but the multiplication and addition algorithms are.” ([Sullivan, 2022, p. 10](zotero://select/library/items/GQGEYW5N)) example of algorithmic black boxing vs no black boxing (here, the black boxed algo would be intelligible if one would look "under its hood")

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=10&annotation=48SIG7S3) “One of the goals in designing computer systems is to build modular systems” ([Sullivan, 2022, p. 10](zotero://select/library/items/GQGEYW5N)) introduce function of algo black box "modularity" "black box as a feature not a bug"

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=10&annotation=2QX4Y3RQ) “For some algorithms, not all sub-steps are fixed or comprehensible to the modeler. It can be that in the course of running the simulation, some aspect of the algorithm itself changes or updates.” ([Sullivan, 2022, p. 10](zotero://select/library/items/GQGEYW5N)) Sullivan makes a case for the position she argues against "black boxing is a problem for understanding"

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=10&annotation=4ICG9WL4) “Applying this to Schelling’s model” ([Sullivan, 2022, p. 10](zotero://select/library/items/GQGEYW5N)) apply idea of levels of implementation black box on a simple model to show that understanding is not impeded whether or not the mid-level implementation black box is removed or not  
  
as understanding here relies on high-level emergent properties of the model which do not change whether or not the lower level implementation changes  
"that algo that produces the 'end result' (a segregated board) which enables our understanding stays the same, even when lower-level algos produce different lower-level results (different coin movement etc.)"

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=11&annotation=SQ65MMVS) “In order to gain understanding of segregation using Schelling’s model, you do not need to peer inside and see the implementation here. We have the same level of understanding of the mechanisms of segregation whether or not the implementation black box is removed.” ([Sullivan, 2022, p. 11](zotero://select/library/items/GQGEYW5N)) prepare "what is true for simple models is also true for complex models"  
Sullivan makes her case starting with simple models and than applies what is true for simple models to complex models

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=11&annotation=5R54CCVZ) “The explanation does not rely on the specific movements of, say, coin-267, but on the macrolevel emergence of a segregated pattern” ([Sullivan, 2022, p. 11](zotero://select/library/items/GQGEYW5N)) "Simply having a highly predictive model, and knowing the high-level emerging properties of the model, uncovers that it is possible to use a machine learning representation for disease prediction.  
  
Importantly, it is not necessary to look inside the implementation black box to answer these types of howpossibly questions. All that is needed is the higher-level understanding of how the system is able to identify high-level patterns within data."

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=11&annotation=XIVP57QC) “However, if we already know what factorials are, then this highest-level black box, for factorials, turns into a simple implementation black box that is compatible with understanding.” ([Sullivan, 2022, p. 11](zotero://select/library/items/GQGEYW5N)) ?: how does background knowledge impact the level of black box and enable understanding even with a highest-level implementation black box?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=12&annotation=IHVE47FJ) “we need to ask whether we have the highest level of implementation black boxing” ([Sullivan, 2022, p. 12](zotero://select/library/items/GQGEYW5N)) relate "depth of black box" with DNN:  
as it seems relevant for understanding whether we have a highest-level implementation black box or not, let's see if DNN are highest-level black boxes

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=12&annotation=3TVUGIVV) “As I have been arguing, in order to consider how opacity limits understanding, we need to isolate the explanatory question of interest, the level of link uncertainty present, and determine the depth of the black boxes” ([Sullivan, 2022, p. 12](zotero://select/library/items/GQGEYW5N)) argument layout of how to show whether or not opacity is the limiting factor for understanding from DNN:  
  
- "determine depth of the black boxes":DNN are not black boxed at the highest level  
  
- "isolate expl. question": deep patient DNN provides us with how-possibly explanations, ie. it allows for understanding of possibilities; it requires empirical evidence to establish link between DNN and its target phenomenon to allow for understanding actual causes  
  
- "isolate level of link uncertainty": DNN's level of link uncertainty reduced/ resolved by empirical evidence  
  
thus, whether or not opacity is relevant for understanding needs to be considered in relation to the depth of the implementation black boxes, the expl. questions asked and the degree of link uncertainty  
  
and this consideration leads us to conclude that opacity is not relevant for understanding phen.  
it is relevant for understanding how the model works though  
  
  
?: aren't all these aspects (type of expl. question, degree of link uncertainty, background knowledge of model and phenomenon) contained in the question how deep the black box is?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=13&annotation=84P2RUUU) “5 See Goodfellow et al. ([2016], Chapter 1) for a discussion on how DNNs used in computer science are inspired by the brain and the limits of this analogy. For a philosophical treatment of these limits, see BailerJones and Bailer-Jones ([2002]). Alternatively, the field of computational neuroscience does seek to model the brain and draw inferences using DNNs (Glorot et al. [2011]). See Buckner ([2018]) for a philosophical argument that DNNs (CNNs in particular) capture processes of the brain. Since this paper is not about brain processing explanations in computational neuroscience, I leave these latter considerations aside.” ([Sullivan, 2022, p. 13](zotero://select/library/items/GQGEYW5N)) read on DNN as models for the brain

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=14&annotation=RVHFRKEQ) “The activation functions, the number of layers, the input data (and how it is represented), and the number of nodes in each layer are all determined by the modeler.” ([Sullivan, 2022, p. 14](zotero://select/library/items/GQGEYW5N)) expand DNN intelligibility  
  
which model features need to in principle be intelligible to the modeler (+ backpropagation process)

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=15&annotation=7MW7W39R) “The exact representational relationship between the layers, and the exact functioning of the DNN, will differ depending on the phenomena modelled, type of DNN, and the choice of activation and learning functions” ([Sullivan, 2022, p. 15](zotero://select/library/items/GQGEYW5N)) add context to intra-model inferences?  
  
variance in intra-model representations/ inferences also depend on the phenomena modeled, type of DNN etc.

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=17&annotation=RUCXXG2T) “Importantly, modelers are not working completely in the dark. They rely on fundamental statistical assumptions and theories to ensure that the resulting model is generalizable” ([Sullivan, 2022, p. 17](zotero://select/library/items/GQGEYW5N)) ?: in what way does statistical knowledge become relevant as background knowledge reducing the depth of the black box in DNN?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=17&annotation=D28E6CKZ) “The modelling process is what determines the set of steps or rules that the resulting model will follow with any new input data it receives.” ([Sullivan, 2022, p. 17](zotero://select/library/items/GQGEYW5N)) difference between modelling process and parameterization?  
modelling process = determine model parameterization = determine intra-model inferences

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=18&annotation=BM6KQ375) “So, while the modeler does not have direct control over the modelling process, a contrast case with Schelling’s model, still the process is not black boxed at the highest level, such that it would prevent understanding of the phenomenon the resulting model aims to capture.” ([Sullivan, 2022, p. 18](zotero://select/library/items/GQGEYW5N)) premise: models that are black boxed at the highest level impede all understanding that can be gained from them  
  
premise: background knowledge of a model reduces level of black box  
premise: modelers have background knowledge of DNN  
  
  
conclusion: black boxed implementation of DNN does not prevent understanding as they are not black boxed at the highest level as modelers have background knowledge of the modelling process (training, validation and testing)

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=18&annotation=AIHR87HF) “However, different types of saliency testing are enough to satisfy our need to know the high level details of how the model works to open the door to understanding the phenomenon the model bears on” ([Sullivan, 2022, p. 18](zotero://select/library/items/GQGEYW5N)) support intermediate claim "black boxed implementation of DNN does not prevent understanding as they are not black boxed at the highest level" by adding indirect means (saliency maps) that enable us to validate whether the high-level goals of the model (determined by the modeler) are satisfied or not, thus, enabling understanding

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=19&annotation=I4GWFRXD) “On the other hand, the fact that there still remains low-level ‘reasoning’ and implementation black boxes in the resulting deep patient model—even after indirect saliency testing—undermines understanding” ([Sullivan, 2022, p. 19](zotero://select/library/items/GQGEYW5N)) ?: why does low-level implementation black boxing in the deep patient DNN still undermine understanding? later, she states the opposite that it does provide understanding of possibilities surrounding the phenomenon

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=19&annotation=FQK262I3) “Thus, there is a lingering concern that even though black boxes of DNN models are not at the highest level, still the understanding we gain from these models is limited because of the lower level opacity in the DNN model itself.” ([Sullivan, 2022, p. 19](zotero://select/library/items/GQGEYW5N)) building up "even DNN that are not black boxed at the highest level, but on a lower level still impede understanding" as counter-argument to own "DNN are not black boxed at the highest level" in order to dismiss it and introduce link uncertainty as the relevant criteria for understanding

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=19&annotation=2XDIGI92) “how-possibly questions about a target phenomenon, and why or howactually questions concerning a real-world phenomenon or target system” ([Sullivan, 2022, p. 19](zotero://select/library/items/GQGEYW5N)) ?: does she differentiate between "target phenomenon" and "real-world phenomenon/target system"?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=19&annotation=QILLQBI8) “We saw with Schelling’s model that implementation black boxes are compatible with understanding possibilities surrounding target systems and compatible with how-actually and why questions surrounding target systems. On the other hand, not surprisingly, implementation black boxes can inhibit understanding of how the model works.” ([Sullivan, 2022, p. 19](zotero://select/library/items/GQGEYW5N)) clarify how models with implementation black boxes allow or do not allow understanding depending on the questions asked:  
  
different types of implementation black boxes do not in principle limit understanding of phenomena either by way of giving how-possibly or how-actually explanations or by answering why-questions,  
  
implementation black boxes do limit understanding of how the model works  
  
IF however, models with implementation black boxes do not allow understanding of phenomena, it is not due to implementation black box, but due to link uncertainty

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=19&annotation=WPMSYCPB) “each with varying degrees of answerability” ([Sullivan, 2022, p. 19](zotero://select/library/items/GQGEYW5N)) define answerability (ie. the possibility for understanding) depends on the type of explanatory question asked and the type/ level of implementation black box

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=19&annotation=ZTS8VRPS) “We could also ask explanatory questions of the model, like the one the researchers were after: how is it possible to use a single model to predict a variety of diseases instead of relying on several models each designed for one disease?” ([Sullivan, 2022, p. 19](zotero://select/library/items/GQGEYW5N)) introduce the kind of explanatory question that Sullivan is going to ask later on:  
"how is it possible schizo can be predicted and correlated with features found in medical records?"  
  
?: but is that a how-possibly question asked to understand possibilities surrounding target systems OR to understand how the model works?  
RELEVANT to reinstall opacity as relevant factor for understanding  
  
if understanding how a phenomenon is possibly predicted using the model necessitates insight into the model workings, then model opacity impedes  understanding.  
  
if understanding how a phenomenon is possibly predicted using the model does not necessitate insight into the model workings, then model opacity  does not impede understanding.

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=19&annotation=XKX9TER9) “The main challenge” ([Sullivan, 2022, p. 19](zotero://select/library/items/GQGEYW5N)) research question: do implementation black boxes that prevent understanding of how the model works, also prevent understanding of the phenomenon  
How are the explanatory questions of the model (model-focused) related to the questions about the phenomenon (phenomenon-focused)?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=20&annotation=J8VGALDE) “It is my contention that the same is true with DNNs” ([Sullivan, 2022, p. 20](zotero://select/library/items/GQGEYW5N)) showing what is true for simple models is also true for complex models:  
initial question: how much understanding of how the model works is necessary for gaining understanding of the phenomenon the model bears on?  
recap example of simple model: The mid-level updating algo is black boxed; the high-level algo stays intact; revealing the mid-level implementation black box around the updating algo does not change the level of understanding on the phenomenon gained from the model  
  
ie. it is true for simple models that understanding more how the model works does not change the amount of understanding gained about the phenomenon

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=20&annotation=NVSQS9L4) “answering how it is possible to predict disease development for a range of diseases” ([Sullivan, 2022, p. 20](zotero://select/library/items/GQGEYW5N)) give explanatory question for deep patient model  
  
I guess, the question is aimed at understanding how a phenomenon is possibly predicted using the model and NOT how the model works  
ie. how is schizo possibly predicted using the model  
if understanding how a phenomenon is possibly predicted using the model necessitates insight into the model workings, then model opacity impedes  understanding.  
  
if understanding how a phenomenon is possibly predicted using the model does not necessitate insight into the model workings, then model opacity  does not impede understanding.

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=20&annotation=93KMJ3GR) “Simply having a highly predictive model, and knowing the high-level emerging properties of the model, uncovers that it is possible to use a machine learning representation for disease prediction.” ([Sullivan, 2022, p. 20](zotero://select/library/items/GQGEYW5N)) give basic conditions to use ML model to answer how-possibly questions:  
  
having knowledge of the high-level emerging properties of the model (that stay unchanged even when lower-level implementation is unpredictable or changes)  
having a model with high predictive power  
NOT necessary to have model transparency, ie. implementation is irrelevant

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=21&annotation=B6PEJWP3) “Moreover, it also seems that we are unable to get a satisfying explanation for why it is actually the case that certain indicators reliably go hand in hand with a given disease using the deep patient model” ([Sullivan, 2022, p. 21](zotero://select/library/items/GQGEYW5N)) give reasons why the deep patient model cannot answer why-questions, e.g. How is schizo actually predicted using the model?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=21&annotation=XCL97Z9M) “While it is tempting to attribute this gap in understanding to the implementation black box of the deep patient model, we should not take the bait.” ([Sullivan, 2022, p. 21](zotero://select/library/items/GQGEYW5N)) relate what is true for simple models is also true for complex models, i.e.  
"implementation black boxes that prevent understanding how the simple OR complex model works DO NOT prevent understanding of the phenomenon"  
  
it is true for simple models that understanding more how the model works being prevented by implementation black box does not change the amount of understanding gained about the phenomenon  
it is true for simple models that understanding more how the model works (ie. revealing the mid-level implementation black box around the updating algo) does not change the amount of understanding gained about the phenomenon  
thus, not being able to answer "How is schizo actually predicted using the model?" is NOT due to implementation black box, but due to link uncertainty

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=21&annotation=FI8W7B3U) “DNNs and link uncertainty” ([Sullivan, 2022, p. 21](zotero://select/library/items/GQGEYW5N)) give reasons why there is link uncertainty in DNN, what explanatory questions it can still answer, how to reduce link uncertainty in DNN by empirical and scientific evidence, model is still useful even after link uncertainty is resolved

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=21&annotation=MBJSL8SP) “In the deep patient case, the model is greatly informed by existing empirical evidence concerning diseases” ([Sullivan, 2022, p. 21](zotero://select/library/items/GQGEYW5N)) give reasons why deep patient DNN can provide understanding of possibilities (how-possible explanations)  
ie. the identification by the model (model establishes a systematic correspondence between possible and actual causes) is supported by empirical evidence

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=22&annotation=AGWALXWP) “Link uncertainty is prevalent. This is highlighted by the ways the model did not meet expectations.” ([Sullivan, 2022, p. 22](zotero://select/library/items/GQGEYW5N)) give reasons why deep patient DNN cannot provide understanding of actual causes surrounding the phenomonen  
  
give conditions for link uncertainty to occur in DNN: when the model does not work as expected due to lack of background knowledge (not necessarily empirical evidence as knowing what the model unknowing tracks is not an empirical finding, right?)

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=22&annotation=CXEK6GLK) “Exploring these hypotheses further would reduce the link uncertainty and increase the level of understanding the deep patient model could provide on disease development. The type of scientific evidence needed to reduce the link uncertainty, in this case, could consist of building additional statistical models that makes the deep patient results more robust, conducting clinical trials, or conducting various longitudinal studies” ([Sullivan, 2022, p. 22](zotero://select/library/items/GQGEYW5N)) expand link uncertainty: how to reduce link uncertainty

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=22&annotation=U22BALC7) “The stronger the link, the greater possible understanding the model can provide.” ([Sullivan, 2022, p. 22](zotero://select/library/items/GQGEYW5N)) again, here degree of link is about degree of possible understanding

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=22&annotation=9MIKYKHZ) “once the link uncertainty is resolved” ([Sullivan, 2022, p. 22](zotero://select/library/items/GQGEYW5N)) and again, can link uncertainty ever be resolved?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=22&annotation=DM5J63FP) “the deep patient model is able to explain and enable understanding of disease development” ([Sullivan, 2022, p. 22](zotero://select/library/items/GQGEYW5N)) ?: why does she reiterate that the empirical value remains after link uncertainty is resolved?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=22&annotation=NEU8I55Q) “because of reducing link uncertainty” ([Sullivan, 2022, p. 22](zotero://select/library/items/GQGEYW5N)) I thought it was about "resolving link uncertainty"?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=23&annotation=6TSS22RY) “the melanoma DNN model has the same level of implementation black boxing as the deep patient model” ([Sullivan, 2022, p. 23](zotero://select/library/items/GQGEYW5N)) ?: why has the melanoma DNN and deep patient DNN the same level of implementation black boxing?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=23&annotation=GJNIVA46) “and some of these questions are more pointed why- and how-actually questions” ([Sullivan, 2022, p. 23](zotero://select/library/items/GQGEYW5N)) ?: why is it possible to ask the melanoma DNN why and how-actually questions compared to the sexual orientation DNN?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=23&annotation=EDC3TT5J) “The level of scientific justification and background knowledge linking the appearance of moles to instances of melanoma is extensive” ([Sullivan, 2022, p. 23](zotero://select/library/items/GQGEYW5N)) expand on what counts as evidence: empirical, scientific and **background knowledge**  
**point of attack:** Sullivan does not clearly define what she understands as empirical and scientific evidence

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=23&annotation=YFI2AWMY) “Visual appearance serves as the leading deciding factor for initial medical intervention” ([Sullivan, 2022, p. 23](zotero://select/library/items/GQGEYW5N)) ?: establishing a factor as a deciding factor is mostly done by empirical means, so is "scientific justification and background knowledge" here just another way to say "empirical evidence"?

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=24&annotation=J25S5X6V) “This can further understanding, especially once these newly discovered patterns undergo further investigation” ([Sullivan, 2022, p. 24](zotero://select/library/items/GQGEYW5N)) expand usefulness of DNN "exploratory function of DNN with high predictive power and low level of link uncertainty": expand understanding by discovering new possible causes surrounding the phenomenon (melanoma) which then need empirical investigation

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=24&annotation=HL4ZCY46) “Understanding is narrowed to a population subset, which is common in medical sciences. Just like many other scientific models, the usefulness of the model depends on the target system and the explanandum. If certain parameters change, the given model ceases to be the right model for explaining.” ([Sullivan, 2022, p. 24](zotero://select/library/items/GQGEYW5N)) RELEVANT concerning context-dependency of model usefulness and "no question from nowhere"

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=25&annotation=4HGQNFH8) “However, the researchers take this how-possibly evidence and argue further that the model serves as supporting evidence for existing scientific theories” ([Sullivan, 2022, p. 25](zotero://select/library/items/GQGEYW5N)) give reason why sexual orientation DNN cannot provide understanding of phenomenon as it cannot answer how-actually questions due to its high level of link uncertainty  
  
the authors try to use a how-possibly explanation given by the model to answer a how-actually question  
ie. use the understanding gained from a how-possibly question to infer understanding of actual causes surrounding a phenomenon

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=26&annotation=8EMYBWIC) “The level of implementation black boxing is the same in the deep patient case, the melanoma case, and sexual orientation case” ([Sullivan, 2022, p. 26](zotero://select/library/items/GQGEYW5N)) argumentation from different amount of understanding gained from DNN with same level of implementation black boxing but different level of link uncertainty:  
level of impl. black boxing influences understanding  
level of link uncertainty influences understanding  
level of impl. black boxing is the same  
level of link uncertainty is different  
thus, difference in understanding can only be due to difference in level of link uncertainty

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=27&annotation=9MGENTDA) “This general claim about the importance of removing link uncertainty in order to gain understanding stretches beyond the cases of minimal and complex models.” ([Sullivan, 2022, p. 27](zotero://select/library/items/GQGEYW5N)) example why link uncertainty and the question of how it can be removed is relevant for other scientific questions too

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=27&annotation=TLCXCIKG) “Moreover, the cases discussed here require an empirical connection that involves going outside of the model in question. On the other hand, model-based explanations that aim to explain mathematical or structural dependences may require something other than empirical evidence to connect the model to the phenomenon of interest. I leave this question for future work.” ([Sullivan, 2022, p. 27](zotero://select/library/items/GQGEYW5N)) future outlook on what evidence is needed when special phenomena are involved

[Go to annotation](zotero://open-pdf/library/items/JF9MCQ3U?page=28&annotation=U6CK2I5Q) “There may be other reasons that many DNN models cannot provide understanding of phenomena” ([Sullivan, 2022, p. 28](zotero://select/library/items/GQGEYW5N)) addressing other unrelated reasons why DNN cannot provide understanding besides link uncertainty:  
DNN cannot answer explanatory question, DNN is mere predictive tool, DNN is a temporary tool, the level of understanding gained from DNN is influenced by other non-epistemic values.


# Annotations%% begin annotations %%  
  
  
  
### Imported: 2024-01-19 6:50 pm  
  
  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Are scientists trading understanding for some other epistemic or pragmatic good when they choose a machine learning model? Or are the assumptions behind why minimal models provide understanding misguided?  
  
<mark style="background-color: #ffd400">Quote</mark>  
>It is able to predict a wide array of medical problems, such as schizophrenia, attention-deficit disorder, and severe diabetes, with a higher degree of accuracy than competing predictive models. However, with this increased accuracy comes increased opacity.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>DNNs are opaque to modelers, they are increasingly complex and have less modeler control, and the amount of w-questions they address are seemingly limited. Are scientists trading understanding for some other epistemic or pragmatic good when they choose an opaque and complex machine learning model?  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>I argue that model simplicity and transparency are not needed for understanding phenomena.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Instead, it is the level of link uncertainty present—that is, the extent to which the model fails to be empirically supported and adequately linked to the target phenomena—that prohibits understanding.  
  
<mark style="background-color: #e56eee">Quote</mark>  
>principle way  
  
<mark style="background-color: #ff6666">Quote</mark>  
>Algorithms on their own are not explanations. It is only when algorithmic models are used to answer a question about some event or phenomenon that they explain.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>I will adopt the increasingly common view that explanation aims at understanding (De Regt [2017]; Grimm [2010]; Khalifa [2017]; Potochnik [2011], [2015]; Strevens [2008]). In a slogan: explaining why helps us to understand why.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>explanations that utilize complex and opaque models are unable to provide understanding of phenomena  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>My arguments do not so much trade on any positive notion of what understanding or explanation is, but on whether, given a lack of information, it is still possible to gain insight about a phenomenon.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>I am concerned with what Humphreys ([2004], [2009]) calls epistemic opacity: the extent to which the process of the model and its derived output are inaccessible to scientists and modelers.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Before considering the complex case of DNNs, it is worthwhile to first consider a simple case to illustrate the way that models explain and provide understanding of phenomenon  
  
<mark style="background-color: #ffd400">Quote</mark>  
>He created a model that aims to simulate a neighbourhood where individuals act on simple preferences in order to see the conditions under which segregation occurs.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>The use of explanation in machine learning often corresponds to justification, such as explaining how a model operates in order to justify a decision. In this paper, I am using explanation in a broader sense, in terms of explaining a target phenomenon.  
  
<mark style="background-color: #ff6666">Quote</mark>  
>There is a distinction between understanding and explaining how the model works and using that model to understand a phenomenon of interest. If one is chiefly concerned about explaining or understanding how a given model is implemented, it is not necessary to know how the model maps on to some real-world phenomenon.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>The question of this paper, on the other hand, is to what extent understanding the model is necessary for gaining understanding of the phenomenon that the model explains.  
  
<mark style="background-color: #ff6666">Quote</mark>  
>understanding possibilities surrounding a real-world phenomenon  
  
<mark style="background-color: #ffd400">Quote</mark>  
>In order to explain how it is possible segregation could occur, the explanation must include how the algorithmic model simulates a possible population that could be affected by segregation by identifying the key mechanism behind the algorithm and how it maps onto a possible population.  
  
  
>  
  
<mark style="background-color: #ff6666">Quote</mark>  
>This mapping allows us to interpret the results of the simulation as identifying a possible causal mechanism of segregation.  
  
<mark style="background-color: #e56eee">Quote</mark>  
>how the algorithm simulates a real population, that is, how the key feature  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Furthermore, the explanation needs to include empirical justification of the claim that individual racial preferences is a salient mechanism that drives real world population moving patterns (Mäki [2009]; Sullivan and Khalifa [2019]).  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Without empirical evidence validating that the possible causes identified by Shelling’s model are actual causes, there is no link connecting the model to the phenomenon.  
  
<mark style="background-color: #ff6666">Quote</mark>  
>There is a high level of link uncertainty, that is, a lack of scientific and empirical evidence supporting the link that connects the model to the target phenomenon.  
  
<mark style="background-color: #e56eee">Quote</mark>  
>Indeed, if we suppose instead that the empirical evidence suggested that all segregation is the result of institutional racism or individual racial prejudice  
  
<mark style="background-color: #e56eee">Quote</mark>  
>so that Schelling’s mechanisms for segregation were never realized in any real-world system, then we would have no reason to think that Schelling’s model uncovers any actual causes of segregation, and thus would not be able to explain segregation or enable any real understanding  
  
<mark style="background-color: #e56eee">Quote</mark>  
>Thus, Schelling’s model only provides understanding in so far as there is there is a link connecting the model to the phenomenon.  
  
<mark style="background-color: #ff6666">Quote</mark>  
>The way of establishing the necessary link  
  
<mark style="background-color: #e56eee">Quote</mark>  
>supports the connection between the causes or dependencies that the model uncovers to those causes or dependencies operating in the target phenomenon.  
  
<mark style="background-color: #ff6666">Quote</mark>  
>to reduce link uncertainty  
  
<mark style="background-color: #ffd400">Quote</mark>  
>empirical evidence connecting individual preferences to causes of segregation  
  
<mark style="background-color: #e56eee">Quote</mark>  
>Importantly, establishing the necessary link connecting the phenomenon to the model does not thereby replace the need for or the epistemic value of the model.  
  
<mark style="background-color: #e56eee">Quote</mark>  
>The model still explains even once the link between the model and the phenomenon is no longer uncertain.  
  
<mark style="background-color: #ff6666">Quote</mark>  
>when the link uncertainty is removed  
  
<mark style="background-color: #ffd400">Quote</mark>  
>The focus should not be unduly placed on how the model works, but instead consider the explanatory question we ask of the model, the role that the algorithm or model plays in the explanation, and the amount, quality, and kind of scientific evidence needed in order to connect the model to the target phenomenon.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Black box explanation  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>the implementation is either unknown or illegible to the modeler, the explainer, or the understander.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>There are different ways one could implement factorials in a computer system  
  
<mark style="background-color: #ffd400">Quote</mark>  
>If a climate model involves computing factorials, it is unnecessary for the modeler to know, or make explicit in explaining climate patterns, how exactly the factorials were implemented. Details regarding the implementation are unnecessary for explaining and understanding why a particular climate pattern emerged.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>There are countless implementations of Schelling’s model that follow the same basic higher-level algorithm regarding satisfying neighbourhood preferences.  
  
<mark style="background-color: #e56eee">Quote</mark>  
>even though these implementation differences make a difference to how the computer system operates and executes the algorithm  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Thus, implementation black boxing in itself does not undermine our ability to explain or understand phenomena.  
  
<mark style="background-color: #e56eee">Quote</mark>  
>If the low-level implementation details made a difference to the high level results of Schelling’s model, then the implementation would matter for explaining and understanding segregation.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>if the explanatory question concerns implementation or why building a model in a particular way is preferable  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>3.2 Levels of implementation black boxes  
  
<mark style="background-color: #ffd400">Quote</mark>  
>In the iterative code example in figure 1, the fact-iter algorithm is not black boxed, but the multiplication and addition algorithms are.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>One of the goals in designing computer systems is to build modular systems  
  
<mark style="background-color: #ffd400">Quote</mark>  
>For some algorithms, not all sub-steps are fixed or comprehensible to the modeler. It can be that in the course of running the simulation, some aspect of the algorithm itself changes or updates.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Applying this to Schelling’s model  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>In order to gain understanding of segregation using Schelling’s model, you do not need to peer inside and see the implementation here. We have the same level of understanding of the mechanisms of segregation whether or not the implementation black box is removed.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>The explanation does not rely on the specific movements of, say, coin-267, but on the macrolevel emergence of a segregated pattern  
  
<mark style="background-color: #e56eee">Quote</mark>  
>However, if we already know what factorials are, then this highest-level black box, for factorials, turns into a simple implementation black box that is compatible with understanding.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>we need to ask whether we have the highest level of implementation black boxing  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>As I have been arguing, in order to consider how opacity limits understanding, we need to isolate the explanatory question of interest, the level of link uncertainty present, and determine the depth of the black boxes  
  
<mark style="background-color: #5fb236">Quote</mark>  
>5 See Goodfellow et al. ([2016], Chapter 1) for a discussion on how DNNs used in computer science are inspired by the brain and the limits of this analogy. For a philosophical treatment of these limits, see BailerJones and Bailer-Jones ([2002]). Alternatively, the field of computational neuroscience does seek to model the brain and draw inferences using DNNs (Glorot et al. [2011]). See Buckner ([2018]) for a philosophical argument that DNNs (CNNs in particular) capture processes of the brain. Since this paper is not about brain processing explanations in computational neuroscience, I leave these latter considerations aside.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>The activation functions, the number of layers, the input data (and how it is represented), and the number of nodes in each layer are all determined by the modeler.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>The exact representational relationship between the layers, and the exact functioning of the DNN, will differ depending on the phenomena modelled, type of DNN, and the choice of activation and learning functions  
  
<mark style="background-color: #e56eee">Quote</mark>  
>Importantly, modelers are not working completely in the dark. They rely on fundamental statistical assumptions and theories to ensure that the resulting model is generalizable  
  
<mark style="background-color: #ffd400">Quote</mark>  
>The modelling process is what determines the set of steps or rules that the resulting model will follow with any new input data it receives.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>So, while the modeler does not have direct control over the modelling process, a contrast case with Schelling’s model, still the process is not black boxed at the highest level, such that it would prevent understanding of the phenomenon the resulting model aims to capture.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>However, different types of saliency testing are enough to satisfy our need to know the high level details of how the model works to open the door to understanding the phenomenon the model bears on  
  
<mark style="background-color: #e56eee">Quote</mark>  
>On the other hand, the fact that there still remains low-level ‘reasoning’ and implementation black boxes in the resulting deep patient model—even after indirect saliency testing—undermines understanding  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Thus, there is a lingering concern that even though black boxes of DNN models are not at the highest level, still the understanding we gain from these models is limited because of the lower level opacity in the DNN model itself.  
  
<mark style="background-color: #e56eee">Quote</mark>  
>how-possibly questions about a target phenomenon, and why or howactually questions concerning a real-world phenomenon or target system  
  
<mark style="background-color: #ffd400">Quote</mark>  
>We saw with Schelling’s model that implementation black boxes are compatible with understanding possibilities surrounding target systems and compatible with how-actually and why questions surrounding target systems. On the other hand, not surprisingly, implementation black boxes can inhibit understanding of how the model works.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>each with varying degrees of answerability  
  
<mark style="background-color: #ff6666">Quote</mark>  
>We could also ask explanatory questions of the model, like the one the researchers were after: how is it possible to use a single model to predict a variety of diseases instead of relying on several models each designed for one disease?  
  
<mark style="background-color: #ffd400">Quote</mark>  
>The main challenge  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>It is my contention that the same is true with DNNs  
  
<mark style="background-color: #e56eee">Quote</mark>  
>answering how it is possible to predict disease development for a range of diseases  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Simply having a highly predictive model, and knowing the high-level emerging properties of the model, uncovers that it is possible to use a machine learning representation for disease prediction.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Moreover, it also seems that we are unable to get a satisfying explanation for why it is actually the case that certain indicators reliably go hand in hand with a given disease using the deep patient model  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>While it is tempting to attribute this gap in understanding to the implementation black box of the deep patient model, we should not take the bait.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>DNNs and link uncertainty  
  
<mark style="background-color: #ffd400">Quote</mark>  
>In the deep patient case, the model is greatly informed by existing empirical evidence concerning diseases  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Link uncertainty is prevalent. This is highlighted by the ways the model did not meet expectations.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Exploring these hypotheses further would reduce the link uncertainty and increase the level of understanding the deep patient model could provide on disease development. The type of scientific evidence needed to reduce the link uncertainty, in this case, could consist of building additional statistical models that makes the deep patient results more robust, conducting clinical trials, or conducting various longitudinal studies  
  
<mark style="background-color: #ff6666">Quote</mark>  
>The stronger the link, the greater possible understanding the model can provide.  
  
<mark style="background-color: #ff6666">Quote</mark>  
>once the link uncertainty is resolved  
  
<mark style="background-color: #e56eee">Quote</mark>  
>the deep patient model is able to explain and enable understanding of disease development  
  
<mark style="background-color: #ff6666">Quote</mark>  
>because of reducing link uncertainty  
  
<mark style="background-color: #e56eee">Quote</mark>  
>the melanoma DNN model has the same level of implementation black boxing as the deep patient model  
  
<mark style="background-color: #e56eee">Quote</mark>  
>and some of these questions are more pointed why- and how-actually questions  
  
<mark style="background-color: #ffd400">Quote</mark>  
>The level of scientific justification and background knowledge linking the appearance of moles to instances of melanoma is extensive  
  
<mark style="background-color: #e56eee">Quote</mark>  
>Visual appearance serves as the leading deciding factor for initial medical intervention  
  
<mark style="background-color: #ffd400">Quote</mark>  
>This can further understanding, especially once these newly discovered patterns undergo further investigation  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Understanding is narrowed to a population subset, which is common in medical sciences. Just like many other scientific models, the usefulness of the model depends on the target system and the explanandum. If certain parameters change, the given model ceases to be the right model for explaining.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>However, the researchers take this how-possibly evidence and argue further that the model serves as supporting evidence for existing scientific theories  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>The level of implementation black boxing is the same in the deep patient case, the melanoma case, and sexual orientation case  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>This general claim about the importance of removing link uncertainty in order to gain understanding stretches beyond the cases of minimal and complex models.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Moreover, the cases discussed here require an empirical connection that involves going outside of the model in question. On the other hand, model-based explanations that aim to explain mathematical or structural dependences may require something other than empirical evidence to connect the model to the phenomenon of interest. I leave this question for future work.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>There may be other reasons that many DNN models cannot provide understanding of phenomena  
  
  
%% end annotations %%




%% Import Date: 2024-01-19T18:50:39.812+01:00 %%
