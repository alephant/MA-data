---
category: literaturenote
tags: 
citekey: "Shech_2022"
status: unread
dateread:
---

>[!Cite]
>Shech, M., & Tamir, E. (2022). Understanding from Deep Learning Models in Context. In _Scientific Understanding and Representation_. Routledge.

>[!Synth]
>**Contribution**::
>
>**Related**:: 

>[!Metadata]
> **FirstAuthor**:: Shech, Michael
> **Author**:: Tamir, Elay
~
>**Title**:: Understanding from Deep Learning Models in Context
>**Year**:: 2022
>**Citekey**:: Shech_2022
>**itemType**:: bookSection
>**Book**::  
>**Publisher**:: Routledge  
>**ISBN**:: 978-1-00-320290-5

>[!LINK]
>
>[2022_Shech_Tamir_Understanding_from_Deep_Learning_Models_in_Context.pdf](file://C:\Users\a.niemeier\Zotero\storage\ZTJ7YW79\2022_Shech_Tamir_Understanding_from_Deep_Learning_Models_in_Context.pdf).

>[!Abstract]
>
>This chapter places into context how the term model in machine learning (ML) contrasts with traditional usages of scientific models for understanding and we show how the direct analysis of an estimator’s learned transformations (specifically, the hidden layers of a deep learning model) can improve understanding of the target phenomenon and reveal how the model organizes relevant information. Specifically, three modes of understanding will be identified: the difference between implementation irrelevance and functionally approximate irrelevance will be disambiguated, and how this distinction impacts potential understanding with these models will be explored. In addition, by distinguishing between empirical link failures and representational ones, an ambiguity in the concept of link uncertainty will be addressed, thus clarifying the role played by scientific background knowledge in enabling understanding with ML.
>.
>
# Notes
# Annotations  
(27/01/2024, 15:08:10)

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=1&annotation=N62AIISY) “we show how direct analysis of an estimator’s learned transformations (specifically, the hidden layers of a deep learning model) can improve understanding of the target phenomenon” ([Shech and Tamir, 2022, p. 1](zotero://select/library/items/3X99RVG8)) **goal / conclusion**

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=1&annotation=LZ85R597) “ML-trained algorithms are often called models, encouraging questions about how such automated effective estimation techniques” ([Shech and Tamir, 2022, p. 1](zotero://select/library/items/3X99RVG8))

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=2&annotation=47466JML) “despite the ostensible opaqueness or “blackbox” nature of how particular estimations are generated” ([Shech and Tamir, 2022, p. 2](zotero://select/library/items/3X99RVG8))

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=2&annotation=NXFL3RQM) “In this paper, we place into context how the term model in ML contrasts with traditional usages of scientific models for understanding, resolving core ambiguities involving representational links to the target phenomenon” ([Shech and Tamir, 2022, p. 2](zotero://select/library/items/3X99RVG8)) **not differentiating between the meaning of scientific model vs ML model creates misunderstanding about what is meant by "links to the target phenomenon" - T&S want to resolve these ambiguities**

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=3&annotation=7LKHVS7P) “paradigmatic non-ML model like Schelling’s model” ([Shech and Tamir, 2022, p. 3](zotero://select/library/items/3X99RVG8))

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=4&annotation=9RLZ7QP7) “On the vertical side of the C-schema, one interacts with M directly to draw inferences” ([Shech and Tamir, 2022, p. 4](zotero://select/library/items/3X99RVG8)) draw inferences directly about M, not about T

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=4&annotation=FKLEXB32) “Last, on the horizontal bottom side, M is used to relevantly draw inferences and answer questions about, or impute properties to, T.” ([Shech and Tamir, 2022, p. 4](zotero://select/library/items/3X99RVG8)) use M to draw inferences about T, i.e. understanding the external target phenomenon

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=5&annotation=BUYT7JPR) “afford understanding in a number of ways” ([Shech and Tamir, 2022, p. 5](zotero://select/library/items/3X99RVG8)) so more "ways of understanding" than "understanding the external target phen." (see right before "models tend to facilitate inferences about phenomena of interest" = sounds obviously like understanding of external target phen.)

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=5&annotation=Y7ISHKUH) “For instance, in a typical map (or map-like) model, we can not only use M to draw inferences about T but we can also see how such inferences are drawn, for example, because of relevant similarities or isomorphic relations associated with M and T (say, a map and a city)” ([Shech and Tamir, 2022, p. 5](zotero://select/library/items/3X99RVG8)) **understanding gained from maps vs ML models:**  
by looking at map representations one can gain understanding of how a map model draws inferences about the target system  
  
  
by looking at learned representation layers in DL one can gain understanding of how intra-model inferences are drawn (i.e. how the ML model learns to organize raw input data to optimally estimate target features)  
  
and by understanding how intra-model inferences are drawn one can gain understanding of the external target phenomenon

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=5&annotation=Z53VDSL3) “In one prominent family of C-schema accounts, to which we return, understanding is powered by some relevant structural similarity or isomorphism between M and an abstraction of features of T.” ([Shech and Tamir, 2022, p. 5](zotero://select/library/items/3X99RVG8))

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=5&annotation=A2SURS46) “We agree with Sullivan on this point—implementation opacity does not matter anymore than drawing a map with red or blue ink matters—but we think that there is an important overlooked distinction between this type of implementation irrelevance and (what we call) functionally approximate irrelevance.” ([Shech and Tamir, 2022, p. 5](zotero://select/library/items/3X99RVG8)) **Sullivan's view on implementation irrelevance is incomplete**

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=6&annotation=WGIF5XM2) “Generally, we are in agreement but we will highlight an important contrast between empirical link uncertainty (e.g., found in Schelling’s model due to an inaccurate similarity preference parameter, and other potential empirical questions concerning actual populations) and what we describe as data-misrepresentation link uncertainty.” ([Shech and Tamir, 2022, p. 6](zotero://select/library/items/3X99RVG8)) **Sullivan's view on link uncertainty is incomplete**

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=7&annotation=B82JGIRN) “We argue below that while DL models do not necessarily provide understanding in the same manner as (say) map representations, learned representation layers may be leveraged to improve understanding by providing insight into how a DL model learns to organize raw input data to optimally estimate y-targets.” ([Shech and Tamir, 2022, p. 7](zotero://select/library/items/3X99RVG8)) **T&S key approach:**  
  
by looking at learned representation layers in DL one can gain understanding of how intra-model inferences are drawn (i.e. how the ML model learns to organize raw input data to optimally estimate target features)  
  
and by understanding how intra-model inferences are drawn one can gain understanding of the external target phenomenon

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=8&annotation=2IRSQNAX) “Inferences about how well the model can estimate y-targets given x-data, exploration of what particular features of x-data tend to play important roles in (correct) estimations of y-targets, and the study of how network parameters and hidden layer transformations organize and restructure x-data to effectively estimate y-targets are all examples of “vertical” inferences in a C-schema.” ([Shech and Tamir, 2022, p. 8](zotero://select/library/items/3X99RVG8)) **modes of understanding targeting how intra-model inferences are drawn:**  
IR understanding: understanding how well the model can estimate target features given input data  
IF: understanding of what particular features of input data are important for correctly estimating target features  
LR: understanding how input data is transformed in learned representations to estimate target features

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=8&annotation=GWRRYFNG) “we must be specific about the target” ([Shech and Tamir, 2022, p. 8](zotero://select/library/items/3X99RVG8)) and it is not the external target!!

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=8&annotation=V6VP4375) “how a learned distribution estimates the “actual” distribution describing a phenomenon’s studied features” ([Shech and Tamir, 2022, p. 8](zotero://select/library/items/3X99RVG8)) this is what the intra-model inferences are about: through the intra-model inferences the actual distribution describing the external phenomenon's feature relationship is estimated  
  
i.e. the "appropriate target of understanding" should be that which estimates the actual distribution, i.e. the intra-model inferences

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=8&annotation=V6LYZJWL) “(TML) Target of ML Hypothesis: The target phenomenon of understanding with ML models is the relationship(s) of features represented by the data.” ([Shech and Tamir, 2022, p. 8](zotero://select/library/items/3X99RVG8)) **T&S hypothesis**:  
  
the appropriate target phenomenon of understanding for ML models are **how intra-model inferences are drawn**  
  
  
**and these intra-model inferences are targeted by modes of understanding**  
**and this understanding is enabled through FAI**  
  
  
to say it differently: by understanding the "relationships of features represented by the data" (TML target intrinsic to the model) the "real-world relationships of features found in the external phenomenon" can be understood  
  
there are different modes of understanding depending which TML target one chooses; these enable different kinds of understanding of the external phenomenon  
Relation to link uncertainty:  
  
Based on this TML target-oriented understanding one is in the right position to evaluate link uncertainty between the ML model and the external target  
  
to say it differently, to properly evaluate link uncertainty and thus, make sure that the model enables understanding of the external target phenomenon, one needs to look at how intra-model inferences are drawn

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=8&annotation=XXL8QWI4) “real world relationships between features implied by the description that are the target phenomenon not itself.” ([Shech and Tamir, 2022, p. 8](zotero://select/library/items/3X99RVG8)) **really irritating: they mean to say "real world relationships between features described by the actual distribution estimated by the model based on relationships of features represented by the data"**

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=9&annotation=UQZF7YK5) “We argue that focusing on precisely what features sampled data do and do not represent is paramount to evaluating ML model link uncertainty.” ([Shech and Tamir, 2022, p. 9](zotero://select/library/items/3X99RVG8)) **argue to focus on horizontal top side of C-schema to avoid data-misrpresentation link uncertainty**

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=9&annotation=RKQFRA82) “In order to study trained ML models, especially complex DL models, for insight into external targets, we must first clarify how a link from ML models to TML targets (horizontal bottom side of a C-schema) may work.” ([Shech and Tamir, 2022, p. 9](zotero://select/library/items/3X99RVG8))

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=9&annotation=X8CAZFPY) “understanding from subway maps and their targets” ([Shech and Tamir, 2022, p. 9](zotero://select/library/items/3X99RVG8)) For instance, in a typical map (or map-like) model, we can not only use M to draw inferences about T but we can also see how such inferences are drawn, for example, because of relevant similarities or isomorphic relations associated with M and T (say, a map and a city)  
"use M to draw inferences about T" = infer paths from point A to B  
"relevant similarities or isomorphic relations associated with M and T" = based on structural similarities data represents the subway system in a way that enables certain kinds of understanding about the subway system  
"we can also see how such inferences are drawn" = **insight into map's topology can help us understand which inferences we can draw about T using M**

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=9&annotation=7GQBT5C9) “The map can then be used to understand how to navigate the represented subway system so long as these navigation insights are circumscribed by what is faithfully supported through the captured facts” ([Shech and Tamir, 2022, p. 9](zotero://select/library/items/3X99RVG8)) a map enables understanding as long as the map representations support that kind of understanding  
<b>  
a model enables understanding of external phenomenon as long as the relationships of features represented by the data support that kind of understanding</b>

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=10&annotation=MQWQPI55) “After all, an ML model’s estimations of depend directly on the parameter values and network architecture” ([Shech and Tamir, 2022, p. 10](zotero://select/library/items/3X99RVG8)) like a map's specific topology is relevant for which understanding the map enables, a ML model's parameterization (the optimized parameter values to best fit the data), sampling data and network architecture is relevant for which understanding the ML model enables

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=10&annotation=W7SAPBK5) “We have a dilemma: To understand feature relationships with an ML model despite detail opacity with respect to learned parameter instantiations, said details ostensibly cannot be used for insight into the relationship, but if estimations of directly depend on the instantiation of these parameter values how can they be used to understand the relationship between and without such details?” ([Shech and Tamir, 2022, p. 10](zotero://select/library/items/3X99RVG8)) **Sullivan's implementation irrelevance causes dilemma in the case of ML models**: If implementation black boxes around parameterization (i.e. learned parameter instantiation) are irrelevant for understanding from ML models (Sullivan's take), how can we gain understanding of intra-model inferences that depend on parameterization (T&S TML)?   
Which in turn would be relevant for understanding the external phenomenon (T&S take)?

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=10&annotation=5MEJKYZC) “What distinguishes functionally approximate irrelevance from implementation irrelevance is that in the former varied details matter to the studied target” ([Shech and Tamir, 2022, p. 10](zotero://select/library/items/3X99RVG8)) **Dilemma gets resolved (FAI)**  
impl. irrelevance: variation in implementation does not matter for understanding  
  
FAI: variation in implementation matters for the **TML target** (how the intra-model inferences are drawn), but not in ways relevant for understanding the external phenomenon as the variation in implementation is approx. irrelevant for modes of understanding using ML models such as IR understanding etc.

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=10&annotation=HPS3JIXM) “but they are varied only in ways that approximately preserve the relevant aspects of the phenomenon to be understood” ([Shech and Tamir, 2022, p. 10](zotero://select/library/items/3X99RVG8)) **attack**: what does it mean "but only in ways that approx. preserve the relevant aspects of the external phenomenon"? who decides what is relevant or not?  
preserving the aspects of the (external) phenomenon means that the relevant "real-world relationships of features found in the external phenomenon" are preserved in the ML model representations

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=10&annotation=KXTVWLTH) “DL may help us understand the following aspects and relationships of features represented by the data” ([Shech and Tamir, 2022, p. 10](zotero://select/library/items/3X99RVG8)) **T&S identify multiple modes of understanding that target different aspects of how intra-model inferences are drawn (TML targets):**  
IR: understanding how well the model can estimate target features given input data  
IF: understanding of what particular features of input data are important for correctly estimating target features  
LR: understanding how input data is transformed in learned representations to estimate target features  
  
**clarification between the connection between TML targets and external target phenomenon:**  
as these modes of understanding target "relationships of features represented by the data" they give indication of "real-world relationships of features found in the external phenomenon"

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=10&annotation=ZGWN7UP5) “mutual information” ([Shech and Tamir, 2022, p. 10](zotero://select/library/items/3X99RVG8)) characteristics of the vector space (?): "Mutual information calculates the statistical dependence between two variables and is the name given to information gain when applied to variable selection."

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=10&annotation=U6WICYWK) “features of a target phenomenon” ([Shech and Tamir, 2022, p. 10](zotero://select/library/items/3X99RVG8))

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=10&annotation=RFRY5ZHW) “Our account may be interpreted as a refinement of Sullivan, making explicit that accounting for role and degree of approximation matters to understanding.” ([Shech and Tamir, 2022, p. 10](zotero://select/library/items/3X99RVG8)) **cite:** T&S say their take is a "refinement"

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=11&annotation=IPRTIK9Z) “Random differences in how the parameters are initialized before training, order differences in how the models see the training data, or differences in hyper-parameters used to define how the models are trained can result in two substantively different learned parameterizations and , even though the probability estimates and generated by the respective DL models approximate the feature relationships described by equivalently” ([Shech and Tamir, 2022, p. 11](zotero://select/library/items/3X99RVG8)) **example how FAI enables understanding**  
  
  
the actual distribution describing the external phenomenon's feature relationship is estimated equivalently, even though the underlying parameterization is different and changes how the equivalent estimates (i.e. model performance) are enabled  
  
i.e. if the **<b>variation in implementation is functionally approx. irrelevant for understanding the TML target,** one can still gain understanding of "real-world relationships of features found in the external phenomenon"</b>  
**functionally approx. irrelevant variations in implementation imply that relevant feature relationships of the external phenomenon are approx. preserved in their model representation**

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=11&annotation=PC7CGEI7) “IR is compatible with Sullivan’s highest level opacity.” ([Shech and Tamir, 2022, p. 11](zotero://select/library/items/3X99RVG8)) **cite**

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=11&annotation=P6XJFZG5) “features of the target phenomenon” ([Shech and Tamir, 2022, p. 11](zotero://select/library/items/3X99RVG8)) **attack**: lack of selectivity: here they mean external target phen!

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=11&annotation=X3Z5MIDX) “significant parameter detail differences (i.e. ) are irrelevant” ([Shech and Tamir, 2022, p. 11](zotero://select/library/items/3X99RVG8)) **attack T&S do not in fact operate on highest-level opacity:**  
  
T&S need the comparison between two models to be able to say that a difference in implementation between models is irrelevant to gain IR understanding of relationships between features represented by the data.  
Only through the comparison between ML models that "perform sufficiently and equivalently across different strata of the data", i.e. generalize the same way across new data, i.e. enable to draw the same inferences about the external target, T&S can say that even highest-level opacity is compatible with understanding.  
  
However, Sullivan's claim that highest-level opacity is not compatible with understanding the external target is derived only from observations of a singular ML model and implies that the modeler has no background knowledge about the model or the target, i.e. there is no way for the modeler to judge whether or not the model performs sufficiently and equivalently across different strata of the data AND the modeler can not gain any knowledge through comparison of performance between two models. i.e. the modeler is not able to state that he can gain the same inferences about the external target, nor has the modeler any knowledge that the models are implemented differently to make derive from the same performance of the models any meaning.  
  
**So there are 4 points that are different between how T&S and Sullivan understand highest-level opacity in relation to knowledge and implementation irrelevance:**  
1. no background knowledge of the model implies no knowledge of different parameterization  
2. no background knowledge of the target implies no way to judge whether or not the model performs sufficiently and equivalently across different strata of the data  
3. no possibility to gain any knowledge through the comparison of performance between two models  
4. Sullivan and T&S do operate with the same notion of implementation irrelevance  
  
It seems that T&S are not in fact considering models with highest-level opacity as knowledge is gained through the comparison of model performance between the models. This makes it possible for them to say that difference in implementation between both models are aprrox. irrelevant. Thus, the level of opacity T&S are considering is reduced for them to be able to say that understanding of the target phen. is possible through their postulated modes of understanding.  
  
"At minimum, the reliable generalizability of an ML model to similarly sampled data increases our understanding that there is some signal in the x-data useful for y-target estimation."  
Dont we need some knowledge into the external target to be able to judge whether or not the generalizability of a ML model is reliable or not?  
Doesnt this imply that we do not in fact on the basis of a highest level black box?  
  
  
Things I need to be sure:  
- are T&S postulating that the models have different parameterization? Sullivan says highest-level opacity means also no background knowledge of the model implementation - thus, do T&S  already NOT operate on the basis of highest-level opacity or is it okay for them to postulate knowledge of different parameterization while at the same time claim that they operate on the basis of a highest-level black box?  
- is it really a problem that T&S utlilize a comparison between models vs Sullivan only focuses on highest-level opacity in relation to a singular model? does this really imply gain of knowledge?  
  
  
- is it even relevant when T&S claim of highest-level opacity falls? how would this impact their argumentation?

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=11&annotation=BMAII7VS) “With the ML model’s target clearly defined as the relationships of features represented by and , the fact that has approximate irrelevance becomes clear: approximates well enough, so while the details matter to how the estimates are made, they can still be (approximately) irrelevant for the target of understanding. With the functionally approximate irrelevance of particular parameter details ensured, IR understanding even with “highest level” opacity is possible.” ([Shech and Tamir, 2022, p. 11](zotero://select/library/items/3X99RVG8)) **why highest-level black boxes are compatible with understanding TML targets:**  
  
if the variation in implementation is functionally approx. irrelevant for understanding certain aspects of how intra-model inferences are drawn e.g. how well the model can estimate target features given input data (IR), one can gain still understanding of the external phen., even when the model implementation is opaque.  
  
What is meant by that?  
**While the variation of implementation matters for how intra-model inferences are drawn, the variation does not matter for IR understanding.**  
The exact implementation of the intra-model inferences can be black boxed, but as they are "sufficiently reliable" they enable IR understanding.  
  
  
**Conclusion**: Highest-level implementation black boxes are compatible with understanding if variation of implementation is approx. irrelevant for understanding the TML target.

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=11&annotation=4LK33UBP) “understanding that there is some signal in the x-data useful for y-target estimation” ([Shech and Tamir, 2022, p. 11](zotero://select/library/items/3X99RVG8)) **example** for IR understanding the TML target

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=12&annotation=GNFWLKAU) “that there is mutual information between the features represented by and .” ([Shech and Tamir, 2022, p. 12](zotero://select/library/items/3X99RVG8)) and by IR understanding the TML target understanding the external phenomenon is enabled

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=12&annotation=UJKH3CNX) “approximate irrelevance comes into question and the researcher should doubt whether such FI attribution yields very much genuine understanding” ([Shech and Tamir, 2022, p. 12](zotero://select/library/items/3X99RVG8)) **what happens when variations in implementation between models is NOT aprroximately irrelevant:**  
then one cannot gain understanding of "relationships of features represented by the data" which enables correct **predictions** of the external target

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=12&annotation=5KMW3FQ8) “are predictive of the target” ([Shech and Tamir, 2022, p. 12](zotero://select/library/items/3X99RVG8)) **"understanding external targets using ML models" or "predicting external targets using ML models? Isnt this a big difference?**

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=12&annotation=97PEK2NQ) “FI understanding of the phenomenon” ([Shech and Tamir, 2022, p. 12](zotero://select/library/items/3X99RVG8)) **attack lack of selectivity**: still unclear!! FI understanding of TML target to enable understanding the external phenomenon?  
  
OR FI understanding IS understanding the external phenomenon by means of understanding the TML target? (yes?...)

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=12&annotation=2VC4J9MH) “by establishing that certain detail variations approximately preserve resulting insights into the target phenomenon’s feature relationships (viz. FI relationships), understanding is possible” ([Shech and Tamir, 2022, p. 12](zotero://select/library/items/3X99RVG8)) functionally approx. irrelevant variations in implementation imply that relevant feature relationships of the external phenomenon are approx. preserved in their model representation  
  
**??: FI relationships = target phen. feature relationships, i.e. that FI understanding is understanding of target phen. feature relationships**

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=12&annotation=I3R7965X) “possibility of training a DL model to detect these relationships successfully (IR)” ([Shech and Tamir, 2022, p. 12](zotero://select/library/items/3X99RVG8)) training ML models to detect relationships of features represented in the data??!!

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=13&annotation=EML645EW) “the individual parameterizations of the learned representations are opaque” ([Shech and Tamir, 2022, p. 13](zotero://select/library/items/3X99RVG8)) the detail word-embedding, i.e. the specific parameters of the vector representing the word are opaque

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=14&annotation=WNTBG48G) “However, if these representations can be used for LR understanding of the represented features, properties such as the relative positions of word-embeddings used to complete analogies should be evident in the respective representations despite these differences” ([Shech and Tamir, 2022, p. 14](zotero://select/library/items/3X99RVG8)) **how invariant intrinsic properties of learned representations (e.g. relative angle or position among embeddings in embeddings space) allow for "direct engagement with hidden layer representations to gain LR understanding"**  
  
  
there is information intrinsic to the learned representations, not intrinsic to their detail parameterization

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=14&annotation=LTE74U7D) “there is nothing extrinsic about any of these properties” ([Shech and Tamir, 2022, p. 14](zotero://select/library/items/3X99RVG8)) there are certain intrinsic properties to the embeddings space, i.e. the learned representations, e.g. relative angle or position among embeddings  
  
**invariant intrinsic properties allow for "direct engagement with hidden layer representations to gain LR understanding"**

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=14&annotation=N5JAXMYX) “extrinsically there is very little information carried over” ([Shech and Tamir, 2022, p. 14](zotero://select/library/items/3X99RVG8)) there is information intrinsic to the learned representations, not intrinsic to their detail parameterization

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=14&annotation=U4QLQIPX) “However, when the learned rotations were used to align the embeddings first, near equivalent performance was recovered” ([Shech and Tamir, 2022, p. 14](zotero://select/library/items/3X99RVG8)) model performance, i.e. understanding gained from the models was comparable when intrinsic properties of learned representations were aligned between both models

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=15&annotation=SDY8UW3A) “to reconciling Sullivan’s contrast of DL models with idealized models” ([Shech and Tamir, 2022, p. 15](zotero://select/library/items/3X99RVG8)) **I dont know how Sullivan needs reconciling?** she didnt contrast ML and idealized models in a way that conclude that ML models are limited in the understanding they can provide compared to idealized models?? in fact, Sullivan sees ML and idealized models more as the same when looking at implementation black box and concludes for both that model opacity is not an in principle problem for understanding!

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=15&annotation=UJ8U8IGS) “leverage these models for LR (FI and MI) understanding” ([Shech and Tamir, 2022, p. 15](zotero://select/library/items/3X99RVG8)) **can the difference in "target of ML model" between Sullivan and T&S be reconciled?** arent they arguing about something different??

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=15&annotation=TARTIZ7B) “Our above discussion explored the TML hypothesis that understanding with ML models targets the relationships of features represented by the data and described by some underlying distribution directly or indirectly estimated by the model” ([Shech and Tamir, 2022, p. 15](zotero://select/library/items/3X99RVG8)) **First time T&S clearly and completely state what the TML states**  
or is it even wrong as it must say "understanding with ML models targets the relationships of features represented by the data and described by some underlying learned distribution by which the model estimates the actual distribution describing the real-world relationships of features in the external target" ??

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=15&annotation=HF6FSENG) “when we “open up the black box” to acquire this understanding” ([Shech and Tamir, 2022, p. 15](zotero://select/library/items/3X99RVG8)) WHAT? why would you open the black box, when you previously said that even a highest-level black box is compatible with understanding??  
dont you mean "when we do not open the black box"???

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=15&annotation=SEMUGZUA) “In this section, we consider examples discussed by Sullivan to disambiguate two kinds of link uncertainty that we submit are conflated in Sullivan (2022) so that the understanding gained from FI and LR can potentially be leveraged to prevent such uncertainty.” ([Shech and Tamir, 2022, p. 15](zotero://select/library/items/3X99RVG8)) **Because Sullivan conflates different kinds of link uncertainty, FI and LR understanding cannot be gained to remove/reduce such link uncertainty**

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=15&annotation=JUN3H3TF) “What links justify the conclusion that their model was able to outperform expert dermatologists tested on “the same data?” ([Shech and Tamir, 2022, p. 15](zotero://select/library/items/3X99RVG8)) their angle to attack "Sullivans conflated notion of link uncertainty"

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=16&annotation=LY6N85TX) “Since ML models primarily 16 “learn” from the provided data, ruling out data leakage and confounding bias in sampling methodology, data preparation, and y-target labeling are fundamental to preventing link uncertainty with ML models.” ([Shech and Tamir, 2022, p. 16](zotero://select/library/items/3X99RVG8)) Do confounds and empirically validated link concern the same aspects of relationship between model and target?

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=16&annotation=TV4QQQAU) “the actual features represented by the data were not just the intended features” ([Shech and Tamir, 2022, p. 16](zotero://select/library/items/3X99RVG8)) **attack misrepresentation uncertainty**: I dont think Sullivans link is concerned with which features of the data are intended or not, but with which features are empirically validated as being actual features of the external target?  
but arent confounding variables in the data just features that could not be empirically validated as being actual features of the external target, and thus, the differentiation in data-misrepresentation and link uncertainty is irrelevant? as link uncertainty already captures the aspects that misrepresentation wants to capture?

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=16&annotation=52YF5695) “rendering the link with intended features, namely, unmarked skin and their dermatological state uncertain” ([Shech and Tamir, 2022, p. 16](zotero://select/library/items/3X99RVG8)) **attack misrepresentation uncertainty**: this has nothing to do with the confounds being part of the actually sampled data, but with the confounds not being empirically validated as being actual features of the target.  
when a ML model captures something that is not there, i.e. a confound cannot be empirically or scientifically validated as being an actual feature of the external target; it can in fact be empirically and scientifically validated as being a confound, i.e. not actually being present in the target  
well, the confound can in fact be actually present in the represented data, but the confound being present is not relevant for the explanatory question that is asked to the model

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=16&annotation=5EWBAVPA) “in both directions” ([Shech and Tamir, 2022, p. 16](zotero://select/library/items/3X99RVG8)) why in both??

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=17&annotation=XX3CKVKC) “In misrepresentation uncertainty such as leakage cases, however, the data still veridically correspond to some feature actually measured and encoded in the data, but substantively not the intended features to be understood.” ([Shech and Tamir, 2022, p. 17](zotero://select/library/items/3X99RVG8)) Schelling model: uncertainty due to mismatch (unclear if veridical correspondence!) between intended model parameters and the target phenomenon (not due to unintended mismatch as with the confounded model)  
The model intentionally hits something which does not exist  
  
confounded model: uncertainty due to **unintended** match (veridical correspondence!) between unintended model parameters and target phenomenon  
The model unintentionally hits something real which is not an intended target

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=17&annotation=3UXBD6S2) “but in order to establish a link between the model and the target, more is needed” ([Shech and Tamir, 2022, p. 17](zotero://select/library/items/3X99RVG8)) **attack misrepresentation uncertainty:** here it sounds as if a link can only then be established if empirical as well as representational link uncertainty is prevented!  
below they talk that complementary to an empirical link a representational link needs to be established to establish "clear horizontal links for understanding"

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=17&annotation=SN8EYIAW) “The data used by an ML model must represent the targeted features of the phenomenon as intended. If not, a background of scientific evidence does not prevent misrepresentation uncertainty.” ([Shech and Tamir, 2022, p. 17](zotero://select/library/items/3X99RVG8)) **Representation of the target phenomenon as intended must be established as well in order for there to be a link connecting model and target.**  
  
To remove empirical link uncertainty is not enough, to establish a link between model and target.  
Data-misrepresentation link uncertainty must be removed as well, i.e. a representational link must be established.  
Establishing an empirical link means that target features identified by the model are empirically validated as being some actual target features  
  
(in Sullivan's words that the possible causes that are being identified by the model are empirically validated as being the actual causes)  
Establishing a representational link means that target features must represent the actual target features and not some other unintended features.

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=17&annotation=GXB6BBXL) “misrepresentation uncertainty also highlights that appropriate sampling methodology and data preparation are necessary to establish a representation link between the ML model’s data and the intended features of the target phenomenon.” ([Shech and Tamir, 2022, p. 17](zotero://select/library/items/3X99RVG8)) **In addition to what Sullivan claims is needed for empirical link (background knowledge etc.) T&S claim additional criteria are needed to be truly able to establish a link**: appropriate sampling methodology and data preparation is necessary to make sure that all data that is used to train the model represent the target phenomenon as intended and do not include unintended features that cause unintended although veridical matches

[Go to annotation](zotero://open-pdf/library/items/ZTJ7YW79?page=19&annotation=GKUJ9IYR) “does not necessarily render the level of questions answered by MI, FI, and LR opaque” ([Shech and Tamir, 2022, p. 19](zotero://select/library/items/3X99RVG8)) ??.


# Annotations%% begin annotations %%  
  
  
  
### Imported: 2024-01-27 3:08 pm  
  
  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>we show how direct analysis of an estimator’s learned transformations (specifically, the hidden layers of a deep learning model) can improve understanding of the target phenomenon  
  
<mark style="background-color: #ffd400">Quote</mark>  
>ML-trained algorithms are often called models, encouraging questions about how such automated effective estimation techniques  
  
<mark style="background-color: #ffd400">Quote</mark>  
>despite the ostensible opaqueness or “blackbox” nature of how particular estimations are generated  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>In this paper, we place into context how the term model in ML contrasts with traditional usages of scientific models for understanding, resolving core ambiguities involving representational links to the target phenomenon  
  
<mark style="background-color: #ffd400">Quote</mark>  
>paradigmatic non-ML model like Schelling’s model  
  
<mark style="background-color: #ffd400">Quote</mark>  
>On the vertical side of the C-schema, one interacts with M directly to draw inferences  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Last, on the horizontal bottom side, M is used to relevantly draw inferences and answer questions about, or impute properties to, T.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>afford understanding in a number of ways  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>For instance, in a typical map (or map-like) model, we can not only use M to draw inferences about T but we can also see how such inferences are drawn, for example, because of relevant similarities or isomorphic relations associated with M and T (say, a map and a city)  
  
<mark style="background-color: #ffd400">Quote</mark>  
>In one prominent family of C-schema accounts, to which we return, understanding is powered by some relevant structural similarity or isomorphism between M and an abstraction of features of T.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>We agree with Sullivan on this point—implementation opacity does not matter anymore than drawing a map with red or blue ink matters—but we think that there is an important overlooked distinction between this type of implementation irrelevance and (what we call) functionally approximate irrelevance.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Generally, we are in agreement but we will highlight an important contrast between empirical link uncertainty (e.g., found in Schelling’s model due to an inaccurate similarity preference parameter, and other potential empirical questions concerning actual populations) and what we describe as data-misrepresentation link uncertainty.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>We argue below that while DL models do not necessarily provide understanding in the same manner as (say) map representations, learned representation layers may be leveraged to improve understanding by providing insight into how a DL model learns to organize raw input data to optimally estimate y-targets.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Inferences about how well the model can estimate y-targets given x-data, exploration of what particular features of x-data tend to play important roles in (correct) estimations of y-targets, and the study of how network parameters and hidden layer transformations organize and restructure x-data to effectively estimate y-targets are all examples of “vertical” inferences in a C-schema.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>we must be specific about the target  
  
<mark style="background-color: #ffd400">Quote</mark>  
>how a learned distribution estimates the “actual” distribution describing a phenomenon’s studied features  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>(TML) Target of ML Hypothesis: The target phenomenon of understanding with ML models is the relationship(s) of features represented by the data.  
  
<mark style="background-color: #ff6666">Quote</mark>  
>real world relationships between features implied by the description that are the target phenomenon not itself.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>We argue that focusing on precisely what features sampled data do and do not represent is paramount to evaluating ML model link uncertainty.  
  
<mark style="background-color: #e56eee">Quote</mark>  
>In order to study trained ML models, especially complex DL models, for insight into external targets, we must first clarify how a link from ML models to TML targets (horizontal bottom side of a C-schema) may work.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>understanding from subway maps and their targets  
  
<mark style="background-color: #ffd400">Quote</mark>  
>The map can then be used to understand how to navigate the represented subway system so long as these navigation insights are circumscribed by what is faithfully supported through the captured facts  
  
<mark style="background-color: #ffd400">Quote</mark>  
>After all, an ML model’s estimations of depend directly on the parameter values and network architecture  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>We have a dilemma: To understand feature relationships with an ML model despite detail opacity with respect to learned parameter instantiations, said details ostensibly cannot be used for insight into the relationship, but if estimations of directly depend on the instantiation of these parameter values how can they be used to understand the relationship between and without such details?  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>What distinguishes functionally approximate irrelevance from implementation irrelevance is that in the former varied details matter to the studied target  
  
<mark style="background-color: #ff6666">Quote</mark>  
>but they are varied only in ways that approximately preserve the relevant aspects of the phenomenon to be understood  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>DL may help us understand the following aspects and relationships of features represented by the data  
  
<mark style="background-color: #ffd400">Quote</mark>  
>mutual information  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>features of a target phenomenon  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Our account may be interpreted as a refinement of Sullivan, making explicit that accounting for role and degree of approximation matters to understanding.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>Random differences in how the parameters are initialized before training, order differences in how the models see the training data, or differences in hyper-parameters used to define how the models are trained can result in two substantively different learned parameterizations and , even though the probability estimates and generated by the respective DL models approximate the feature relationships described by equivalently  
  
<mark style="background-color: #ffd400">Quote</mark>  
>IR is compatible with Sullivan’s highest level opacity.  
  
<mark style="background-color: #ff6666">Quote</mark>  
>features of the target phenomenon  
  
<mark style="background-color: #ff6666">Quote</mark>  
>significant parameter detail differences (i.e. ) are irrelevant  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>With the ML model’s target clearly defined as the relationships of features represented by and , the fact that has approximate irrelevance becomes clear: approximates well enough, so while the details matter to how the estimates are made, they can still be (approximately) irrelevant for the target of understanding. With the functionally approximate irrelevance of particular parameter details ensured, IR understanding even with “highest level” opacity is possible.  
  
<mark style="background-color: #e56eee">Quote</mark>  
>understanding that there is some signal in the x-data useful for y-target estimation  
  
<mark style="background-color: #e56eee">Quote</mark>  
>that there is mutual information between the features represented by and .  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>approximate irrelevance comes into question and the researcher should doubt whether such FI attribution yields very much genuine understanding  
  
<mark style="background-color: #ff6666">Quote</mark>  
>are predictive of the target  
  
<mark style="background-color: #ff6666">Quote</mark>  
>FI understanding of the phenomenon  
  
<mark style="background-color: #ffd400">Quote</mark>  
>by establishing that certain detail variations approximately preserve resulting insights into the target phenomenon’s feature relationships (viz. FI relationships), understanding is possible  
  
<mark style="background-color: #ffd400">Quote</mark>  
>possibility of training a DL model to detect these relationships successfully (IR)  
  
<mark style="background-color: #ffd400">Quote</mark>  
>the individual parameterizations of the learned representations are opaque  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>However, if these representations can be used for LR understanding of the represented features, properties such as the relative positions of word-embeddings used to complete analogies should be evident in the respective representations despite these differences  
  
<mark style="background-color: #ffd400">Quote</mark>  
>there is nothing extrinsic about any of these properties  
  
<mark style="background-color: #ffd400">Quote</mark>  
>extrinsically there is very little information carried over  
  
<mark style="background-color: #ffd400">Quote</mark>  
>However, when the learned rotations were used to align the embeddings first, near equivalent performance was recovered  
  
<mark style="background-color: #ff6666">Quote</mark>  
>to reconciling Sullivan’s contrast of DL models with idealized models  
  
<mark style="background-color: #ff6666">Quote</mark>  
>leverage these models for LR (FI and MI) understanding  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Our above discussion explored the TML hypothesis that understanding with ML models targets the relationships of features represented by the data and described by some underlying distribution directly or indirectly estimated by the model  
  
<mark style="background-color: #ff6666">Quote</mark>  
>when we “open up the black box” to acquire this understanding  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>In this section, we consider examples discussed by Sullivan to disambiguate two kinds of link uncertainty that we submit are conflated in Sullivan (2022) so that the understanding gained from FI and LR can potentially be leveraged to prevent such uncertainty.  
  
<mark style="background-color: #ffd400">Quote</mark>  
>What links justify the conclusion that their model was able to outperform expert dermatologists tested on “the same data?  
  
<mark style="background-color: #ffd400">Quote</mark>  
>Since ML models primarily 16 “learn” from the provided data, ruling out data leakage and confounding bias in sampling methodology, data preparation, and y-target labeling are fundamental to preventing link uncertainty with ML models.  
  
<mark style="background-color: #ff6666">Quote</mark>  
>the actual features represented by the data were not just the intended features  
  
<mark style="background-color: #ff6666">Quote</mark>  
>rendering the link with intended features, namely, unmarked skin and their dermatological state uncertain  
  
<mark style="background-color: #ff6666">Quote</mark>  
>in both directions  
  
<mark style="background-color: #ffd400">Quote</mark>  
>In misrepresentation uncertainty such as leakage cases, however, the data still veridically correspond to some feature actually measured and encoded in the data, but substantively not the intended features to be understood.  
  
<mark style="background-color: #ff6666">Quote</mark>  
>but in order to establish a link between the model and the target, more is needed  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>The data used by an ML model must represent the targeted features of the phenomenon as intended. If not, a background of scientific evidence does not prevent misrepresentation uncertainty.  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
>misrepresentation uncertainty also highlights that appropriate sampling methodology and data preparation are necessary to establish a representation link between the ML model’s data and the intended features of the target phenomenon.  
  
<mark style="background-color: #ff6666">Quote</mark>  
>does not necessarily render the level of questions answered by MI, FI, and LR opaque  
  
  
%% end annotations %%




%% Import Date: 2024-01-27T15:08:27.158+01:00 %%
