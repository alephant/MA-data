- do I agree with Sullivan that model opacity is not an in-principle problem for understanding?
- do I agree with Sullivan that higher-level properties of a ML models are causally relevant for the model decision?
- do I agree with Sullivan that high-level properties of a ML model are intelligible?

I agree with Sullivan's position that model opacity is not an in-principle problem for understanding.
But I qualify her position based on the objections raised by Räz & Beisbart without completely following their conclusion. I follow their conclusion insofar that opaque ML models do not enable explanatory understanding. If it is explanatory understanding that Sullivan is arguing for, then I would not agree with her in this sense. But I do not follow Räz & Beisbart in the sense that a weaker notion of understanding is trivial and provides little value to scientific inquiry. Concerning an non-explanatory notion of understanding I might adopt the view shared by Boge. A weaker notion than explanatory understanding need not be trivial. Exploration, objectual or heurisitic understanding can aid to develop better models and find better explanations that enable understanding in a stronger sense. 

Even if opaque ML models can only enable non-explanatory understanding, the overlay of model decision by multiple opaque ML models can turn out to develop a "robust theorem" that corresponds to the actual features of the target phenomenon (see: robustness analysis by Weisberg, Levin etc.)

T&S approach could potentially serve to investigate which higher-level properties actually drive the model decision. The weaknesses of Sullivan's argumentation for ML models not being a highest-level black box (see R&B objections on saliency maps) might be alleviated by T&S model-centric approach.
