research question: 
under which conditions does opacity of ML models matter for them to provide scientific understanding?
when does opacity of ML models matter for them to provide scientific understanding?

# Relevant






# Open Questions

Understanding from models vs model-based explanations?

Does the problem of opacity specific to ML models follow from the general model criteria of intelligibility?

**point of attack** "is it possible to argue for the possibility of understanding from ML models without saying anything about what this understanding is?"

"narrow and broad sense of understanding" using the model to understand how the model works vs using the model to understand a phenomenon - **point of attack**: Sullivan claims that "narrow vs broad sense of understanding" in relation to ML models is distinct - but how distinct are they really?

"There is a distinction between understanding and explaining how the model works and using that model to understand a phenomenon of interest. If one is chiefly concerned about explaining or understanding how a given model is implemented, it is not necessary to know how the model maps on to some real-world phenomenon. The question of this paper, on the other hand, is to what extent understanding the model is necessary for gaining understanding of the phenomenon that the model explains." **point of attack**: is it true that understanding how the model maps onto the phenomenon is irrelevant for understanding model implementation?